\subsection{Programs}

Most of the computer programs used for this work were written in the C
programming language. Contact networks were generated using the
\software{igraph} library~\autocite{csardi2006igraph}, which was also used to
store phylogenetic trees. Hash tables, and the dynamic programming matrices for
computing the tree kernel, were stored using Judy
arrays~\autocite{baskins2004judy}. The GNU Scientific Library
(GSL)~\autocite{gough2009gnu} was used to generate random draws from
probability distributions, and to perform the bisection step in the ABC-SMC
algorithm.

\subsubsection{Epidemic simulation over a network}

I implemented method of~\autocite{gillespie1977exact} for simulating epidemics,
and the corresponding transmission trees, over static contact networks. This
method has been applied previously for the same
purpose~\autocite{robinson2013dynamics, leventhal2012inferring}, however no
existing open source implementations were available.

Let $G = (V, E)$ be a directed contact network. Each directed edge $e = (u, v)$
in the network is associated with a transmission rate $\beta_e$, which indicates
that, once $u$ becomes infected, $u$ will infect $v$ with rate $\beta_e$. In
other words, the time between $u$ becoming infected and the infection passing
from $u$ to $v$ is a random variable with distribution $\Exponential(\beta_e)$.
Note that $v$ may become infected before this time has elapsed, if $v$ has
other incoming edges. Each vertex $v$ also has a removal rate $\gamma_v$, which
is the rate at which $v$ is removed from the population after becoming
infected. Removal may correspond to death or recovery with immunity, or a
combination of both, but in our implementation recovered nodes never re-enter
the susceptible population. Borrowing a term from HIV epidemiology, a
\defn{discordant edge} in $G$ is an edge $(u, v)$ where $u$ is infected and $v$
is uninfected. 

To describe the algorithm, we introduce some notation and variables. Let
$\inc(v)$ be the set of incoming edges to $v$, and $\out(v)$ be the set of
outgoing edges from $v$. Let $I$ be the set of infected nodes in the network,
$R$ be the set of removed nodes, and $S$ be the remaining susceptible nodes.
Let $D$ be the set of discordant edges in the network, $\beta$ be the 
total transmission rate over all discordant edges, and $\gamma$ be the total
removal rate of all infected nodes. That is,
\[
  \beta = \sum_{e \in D} \beta_e, \quad
  \gamma = \sum_{v \in I} \gamma_v.
\]
The variables $S$, $I$, $R$, $D$, $\beta$, and $\gamma$ are all updated as the
simulation progresses, but since these updates are very straightforward we do
not write them explicitly in the algorithm. When a new node $v$ is infected, it
is deleted from $S$ and added to $I$, any formerly discordant edges in $\in(v)$
are deleted from $D$, and edges in $\out(v)$ to nodes in $S$ are added to $D$.
If $v$ is later removed, it is deleted from $I$ and added to $R$, and any
discordant edges in $\out(v)$ are deleted from $D$. In both cases, the
variables $\beta$ and $\gamma$ are updated to reflect the changes. 

\newcommand{\tip}{\mathrm{tip}}

The full algorithm is shown in Algorithm \ref{alg:nettree}. The transmission
tree $T$ is simulated along with the epidemic. We keep a map called $\tip$,
which maps infected nodes in $I$ to the tips of $T$. The variable $n$ counts
the number of nodes, both extant and internal, in $T$. The simulation continues
until either there are no discordant edges left in the network, or we reach a
user-defined cutoff of time ($t_{\max}$) or number of infections ($I_{\max}$). 

\begin{algorithm}
  \label{alg:nettree}
  \caption{Simulation of an epidemic and transmission tree over a contact network}
  \begin{algorithmic}
    \State infect one node $v$ at random
    \State let $T$ be a single node with index $1$
    \State $\tip[v] \gets 1$
    \State $n \gets 1$
    \State $t \gets 0$
    \While{$D \neq \emptyset$ and $|I| + |R| < I_{\max}$ and $t < t_{\max}$}
      \State $s \gets \min(t_{\max} - t, \Exponential(\beta + \gamma))$
      \For{$v \in \tip$}
        \State{extend the branch length of $\tip[v]$ by $s$}
      \EndFor
      \State $t \gets t + s$
      \If{$t < t_{\max}$}
        \If{$\Uniform(0, 1) < \nicefrac{\beta}{\beta + \gamma}$}
          \State choose an edge $e = (u, v)$ from $D$ with probability $\beta_e / \beta$
                 and infect $v$
          \State add nodes with labels $n+1$ and $n+2$ to $T$
          \State connect $n+1$ and $n+2$ to $\tip[v]$ in $T$, with branch lengths $0$
          \State $\tip[v] \gets n + 1$
          \State $\tip[u] \gets n + 2$
          \State $n \gets n + 2$
        \Else
          \State choose a node $v$ from $I$ with probability $\gamma_v / \gamma$
                 and remove $v$
          \State remove $v$ from $\tip$
        \EndIf
        \State update $S$, $I$, $R$, $D$, $\beta$, and $\gamma$
      \EndIf
    \EndWhile
  \end{algorithmic}
\end{algorithm}

\subsubsection{Phylogenetic kernel and normalized lineages-through-time}

The tree kernel developed in~\autocite{poon2013mapping} provides a
comprehensive similarity score between two phylogenetic trees. The kernel
computes the dot-product of two feature vectors, corresponding to the two
trees, in the infinite-dimensional feature space of all possible subset trees
with branch lengths. I implemented the fast algorithm developed
in~\autocite{moschitti2006making}, which first enumerates all pairs of subtrees
with the same number of leaf children, and then computes the kernel by dynamic
programming.

In addition, we implemented a modified version of the normalized
lineages-through-time statistic developed in~\autocite{janzen2015approximate},
which uses piecewise linear functions instead of step functions for the
lineages-through-time plots. This modification is to address a potential
inconsistency for trees of different sizes, illustrated in
Figure~\ref{fig:nltt}.

\begin{figure}[ht]
  \centering
  \caption[Comparison of original and modified normalised lineages-through-time]
    {Comparison of original formulation of normalized lineages-through-time,
     developed in~\autocite{janzen2015approximate}, with our modified version
     using linear interpolation. Here, the red and blue trees both have
     uniformly spaced branching times. Using step functions (left), the nLTT
     of the two trees is non-zero due to the differing numbers of internal
     branches. Using linear interpolation, the nLTT is zero (right). The lines
     on the right graph have been offset for visibility.}
  \includegraphics[scale=0.75]{nltt}
  \label{fig:nltt}
\end{figure}

\subsubsection{Adaptive sequential Monte-Carlo approximate Bayseian computation}

I implemented the adaptive sequential Monte-Carlo algorithm
developed in~\autocite{del2012adaptive} for approximate Bayesian computation.
In contrast to most existing sequential Monte-Carlo methods, this algorithm
does not require the user to specify a sequence of decreasing tolerances to
approach the target posterior distribution. Rather, the tolerances are computed
adaptively at each step, starting from infinity at the first iteration. The
algorithm may be stopped when the tolerance reaches a user-defined final value,
or when the rate of acceptance of the MCMC kernel reaches a user-defined
threshold. Following a heuristic applied by the
authors~\autocite{del2012adaptive}, we used the latter stopping criterion,
accepting the SMC approximation to the posterior when the MCMC acceptance rate
dropped below 1.5\%.

\subsection{Simulation experiments}

\subsubsection{Identification of separable parameters in kernel space}
\label{subsubsec:kernel}

Recall that our approximate Bayesian computation approach to fitting contact
network models involves simulating transmission trees under a wide variety of
parameter values, and then comparing these simulated trees to the true
transmission tree. Values which produce trees similar to the observed
transmission tree are distinguished as more likely than values which produce
trees very different from the truth. In order for this type of analysis to
succeed, it is critical that different parameter values produce different
looking trees. Otherwise, if many different values produce trees which are too
similar to each other, it will be impossible to distinguish which value is most
consistent with the real tree. Just as importantly, trees simulated with
similar parameter values must be similar to each other. In mathematical terms,
we require the trees simulated from distinct parameter values to be
\defn{separable} in tree space. The concept of separability is illustrated in
Figure~\ref{fig:separable}.

\begin{figure}[ht]
  \centering
  \includegraphics{separable}
  \caption[Separable versus non-separable pararameters in kernel space]{
    Separable versus non-separable parameters in kernel space. Trees have been
    simulated under three sets of parameters, represented as blue, red, and
    green. An observed tree is shown in black. Trees are layed out such that
    the distance between two trees corresponds to their similarity. In panel A,
    trees from the same parameter set are similar to each other, but different
    from other trees. The true tree is most consistent with the red parameters.
    In panels B and C, trees from the same parameter set are not similar to
    each other (B), or trees from different parameter sets are similar to each
    other (C). It's difficult to say which parameters the true tree is most
    consistent with.
  }
  \label{fig:separable}
\end{figure}

Before undertaking a complete ABC analysis, I analysed four simple contact
network models to determine whether their parameters could be separated in tree
kernel space. The four models are described in detail in
subsection~\ref{subsubsec:generative} of the introduction. Briefly, they are:
random networks, where each possible contact has a fixed probability of
occuring; preferential attachment networks, where highly connected nodes tend
to attract more contacts; small world networks, where nodes are connected to
their immediate neighbours and the occasional far-flung contact; and full
networks, where every possible connection is present. I will describe here only
the procedure and results for the preferential attachment networks. The details
of the other three types of graph can be found in the supplemental materials.

The method of testing for separability was described previously
in~\autocite{poon2015phylodynamic}, but I will reiterate it here for
completeness. As a concrete example, consider the attachment power parameter
$\alpha$ of the preferential attachment networks. This parameter describes the
strength of attraction to highly connected nodes and is bounded below by zero
indicating no extra attraction. By qualitative observation, I determined that a
power of 2.0, which produced networks with very few ``hub'' nodes with
extremely high degree, was a suitable upper bound (see
Figure~\ref{fig:pabounds}). Therefore, I chose to test the values 0.5, 1.0, and
1.5 for separability. The other parameters were fixed: the number of nodes in
the network was 5000, and the mean degree of each node in the network was four.
As discussed in subsection~\ref{subsubsec:generative}, this is the smallest
mean degree value for preferential attachment networks which produces networks
which are more than trees. 

\begin{figure}[ht]
  \centering
  \label{fig:pabounds}
  \includegraphics{pa_power_bounds}
  \caption[Upper and lower bounds on preferential attachment power]{
    Qualitative justification for choice of zero and two as lower and upper
    bounds on preferential attachment power $\alpha$. (A) Preferential
    attachment network on 50 nodes with $\alpha = 0$, the lower bound enforced
    by the model. (B) Preferential attachment network on 50 nodes with $\alpha
    = 2$, where a few nodes have very high degree but the majority have very
    low degree. (C) Density plot of node degrees in a 5000-node network with
    $\alpha = 0$. The maximum degree of a node in this network was 24. (D)
    Density plot of node degrees in a 5000-node network with $\alpha = 2$. The
    maximum degree of a node in this network was 4942. 
  }
\end{figure}

For each of the values of $\alpha$, I generated 100 networks on 5000 nodes. An
epidemic was simulated over each network (see subsection X) until 1000 nodes
were infected, and 500 of those infected nodes were sampled to form a
transmission tree. This resulted in 300 total simulated transmission trees -
100 for each of the three values of $\alpha$. Next, I computed the tree
kernel~\autocite{poon2013mapping} (see subsection~\ref{subsubsec:treeshape})
for each pair of trees. These values were placed into a 300 $\times$ 300 kernel
matrix, where the value at the $(i, j)$th position was the tree kernel of the 
$i$th and $j$th trees. 

The tree kernel provides a pairwise similarity score between two trees. The
higher the kernel score, the more similar the trees are to each other.
Therefore, to have separability, we need trees simulated with the same value of
$\alpha$ to have high kernel scores with each other, but low kernel scores with
trees from different $\alpha$ values. We can visually check whether or not this
is true by laying out the trees as points on a graph, in such a way that trees
with high scores are close to each other, but trees with low scores are far
apart. This is accomplished by performing a kernel principal components
analysis (kPCA)~\autocite{scholkopf1998nonlinear} on the kernel matrix.
Briefly, ordinary principal components analysis (PCA) finds a lower dimensional
representation of points in a high dimensional space which preserves as much of
the variation in the data as possible. kPCA performs the same task, but using
the dot products of each pair of points as input, instead of the data points
themselves. A two-dimensional kPCA projection of the simulated trees is shown
in Figure~\ref{fig:pakpca}. The scenario just described - 500 samples from 1000
infected nodes - is the central panel. To ensure that the method could be used
in a variety of contexts, the same analysis was performed with 500 and 2000
infected nodes, as well as 100 and 1000 sampled tips. The projections show that
all three values are well separated from each other in many of the scenarios.
With smaller trees, it becomes harder to disinguish $\alpha = 0.5$ from $\alpha
= 1.0$ in two dimensions.

\begin{figure}[ht]
  \centering
  \label{fig:pakpca}
  \includegraphics{kernel_pa_kpca}
  \caption[Projection of kernel matrix for different attachment power values
  onto its first two principal components]{
    Projection of the kernel matrix for different preferential attachment power
    values onto its first two principal components, for eight simulation
    scenarios. Each point corresponds to a simulated transmission tree, and is
    coloured by preferential attachment power. Facets are number of infected
    nodes (horizontal), and number of sampled tips (vertical). The parameters
    to the tree kernel were $\lambda = 0.3$ and $\sigma = 5$, and the nLTT was
    not used. Qualitatively, trees with a larger number of tips are easier to
    separate in kernel space, regardless of what sampling proportion they
    represent. In all cases, the highest attachment power can be separated from
    the other two, but the two lowest values become hand to distinguish with in
    the 100-tip trees.
  }
\end{figure}

\begin{figure}[ht]
  \centering
  \label{fig:pacrossv}
  \includegraphics{kernel_pa_crossv}
  \caption[Cross-validation performance of kernel support vector machine
  classifier for preferential attachment power]{
    Cross-validation performance of kernel support vector machine classifier
    for preferential attachment power.
  }
\end{figure}

\subsubsection{Grid search}

\subsubsection{Approximate Bayesian computation}

\subsection{Applications}

\subsubsection{HIV in British Columubia}
