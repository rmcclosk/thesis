

\add{In this chapter, we will address the three research aims
of this thesis introduced in \cref{sec:obj}. First, in \cref{sec:netabc}, we
describe \software{netabc}, a computer program that implements an
\acrlong{ABC}-based algorithm to fit contact network models to phylogenetic
data. We also provide a justification for the use of \gls{ABC} for this problem
by arguing that the likelihood functions required to fit these models by more
conventional means are likely to be computationally intractable.  Second, in
\cref{sec:ba}, we perform a simulation study to investigate the \acrlong{BA}
network model, which uses a preferential attachment mechanism to generate
networks with the power law degree distributions observed in real world social
and sexual networks. We progress through two exploratory analyses testing the
identifiability of the model's parameters, and conclude by testing
\software{netabc}'s ability to recover the parameters from simulated
transmission trees. Third, in \cref{sec:hiv}, we apply \software{netabc} to fit
the \gls{BA} model to six real world \gls{HIV} datasets, with the understanding
of the model parameters' identifiability gained through the simulation
experiments. We conclude the chapter with a unified discussion of the three
research aims, including interpretation of the results of both the simulated
and real data experiments, as well as an examination of the limitations of our
approach and opportunities for future investigation.}

\section{\software{Netabc}: a computer program for estimation of contact
network parameters with ABC}
\label{sec:netabc}

\software{Netabc} is a computer program to perform statistical inference of
contact network parameters from an estimated transmission tree using
\gls{ABC}. \add{As discussed in \cref{sec:obj}, the principal statistical
algorithm used by \software{netabc} is adaptive
\gls{ABC}-\gls{SMC}~\autocite{del2012adaptive}. In addition, there are two
supplementary components which are specific to the domain of phylogenetics and
contact networks: Gillespie simulation~\autocite{gillespie1976general}, to
simulate transmission trees on contact networks; and the tree
kernel~\autocite{poon2013mapping}, which is used as the distance function in
\gls{ABC} to compare transmission trees~\autocite{poon2015phylodynamic} (see
\cref{sec:abc}).} We give a high-level overview of the program here, before
describing these components in detail. \software{Netabc} takes as input an
estimated transmission tree, which can be derived from a viral phylogeny by
rooting and time-scaling as described in \cref{subsec:phylodynamics} or
estimated by other methods~\autocite{cottam2008integrating,
jombart2011reconstructing, ypma2012unravelling, morelli2012bayesian,
didelot2014bayesian, hall2015epidemic}. We variously refer to this estimated
transmission tree as the observed tree, input tree, or true tree.

As described in \cref{sec:smc}, \software{netabc} keeps track of a population
of particles $x^{(k)}$ indexed by an integer $k$, each of which contains
particular parameter values $\theta^{(k)}$ for the \add{contact network} model
we are trying to fit \add{to the input tree}. A small number \gls{M} of contact
networks $z^{(k,i)}$, $1 \leq i \leq \gls{M}$, are generated under the model
for each particle in accordance with that particle's parameters. An epidemic is
simulated over each of these networks using Gillespie simulation, and by
keeping track of its progress, a transmission tree is obtained. Thus, each
particle becomes associated with several simulated transmission trees. These
trees are compared to the input tree using the tree kernel. Particles are
weighted according to the similarity of their associated simulated trees with
the true tree, with more similar trees receiving higher weights. The particles
are iteratively perturbed to explore the parameter space, and particles with
simulated trees too distant from the true tree are periodically dropped and
resampled. Once a convergence criterion is attained, the final set of particles
is used as a Monte Carlo approximation to the target distribution of \gls{ABC},
which is assumed to resemble the posterior distribution on model parameters
(see \cref{sec:abc}). A graphical schematic of this algorithm is shown in
\cref{fig:abcsmc}.

\begin{figure}
    \includegraphics{abc-smc.pdf}
    \caption[
        Graphical schematic of the ABC-SMC algorithm implemented in \software{netabc}.
    ]{
        Graphical schematic of the \gls{ABC}-\gls{SMC} algorithm implemented in
        \software{netabc}. Particles are initially drawn from their prior
        distributions, making the initial population a Monte Carlo
        approximation to the prior. At each iteration, particles are perturbed,
        and a distance threshold around the true tree contracts. Particles are
        rejected, and eventually resampled, when all their associated simulated
        trees lie outside the threshold. As the algorithm progresses, the
        population smoothly approaches a Monte Carlo approximation of the
        \gls{ABC} target distribution, which is assumed to resemble the
        posterior.
    }
    \label{fig:abcsmc}
\end{figure}

\software{Netabc} is written in the \software{C} programming language. The
\software{igraph} library~\autocite{csardi2006igraph} is used to generate and
store contact networks and phylogenies. Judy arrays~\autocite{baskins2004judy}
are used for hash tables and dynamic programming matrices. The
\gls{GSL}~\autocite{gough2009gnu} is used to generate random draws from
probability distributions, and to \del{perform the bisection step} \add{solve
for the next $\varepsilon$ by bisection} in the adaptive \gls{ABC}-\gls{SMC}
algorithm. Parallelization is implemented with \gls{POSIX}
threads~\autocite{barney2009posix}. In addition to the \software{netabc} binary
to perform \gls{ABC}, we provide three additional stand-alone utilities:
\software{treekernel}, to calculate the tree kernel; \software{nettree}, to
simulate a transmission tree over a contact network; and \software{treestat},
to compute various summary statistics of phylogenies. The programs are freely
available at \url{https://github.com/rmcclosk/netabc}.

To check that our implementation of Gillespie simulation was correct, we
reproduced Figure 1A of \textcite{leventhal2012inferring} (our
\cref{fig:leventhal}), which plots the imbalance of transmission trees
simulated over four network models at various levels of pathogen
transmissibility. Our implementation of adaptive \gls{ABC}-\gls{SMC} was tested
by applying it to the same mixture of Gaussians used by
\textcite{del2012adaptive} to demonstrate their method (originally used
by~\textcite{sisson2007sequential}). We were able to obtain a close
approximation to the function (see \cref{fig:smctest}), and attained the
stopping condition used by the authors in a comparable number of steps. To
check that the algorithm would converge to a bimodal distribution, we also
applied it to a mixture of two Gaussians with means $\pm4$ and variances 1. The
algorithm was able to recover both peaks (\cref{fig:smctest2}).

\subsection{Simulation of transmission trees over contact networks}
\label{subsec:nettree}

The simulation of epidemics, and the corresponding transmission trees, over
contact networks is performed in \software{netabc} using the Gillespie
simulation algorithm~\autocite{gillespie1976general}. This method has been
independently implemented and applied by several
authors~\autocite[\textit{e.g.}][]{o2011contact, robinson2013dynamics,
leventhal2012inferring, groendyke2011bayesian, villandre2016assessment}.
\textcite{groendyke2011bayesian} published their implementation as an
\software{R} package, but since the \gls{SMC} algorithm is quite
computationally intensive, we chose to implement our own version in
\software{C} \add{as part of \software{netabc}}.

Let $G = (V, E)$ be a directed contact network. We assume the individual nodes
and edges of $G$ follow the dynamics of the \gls{SIR}
model~\autocite{kermack1927contribution}. Each directed edge $e = (u, v)$ in
the network is associated with a transmission rate $\beta_e$, which indicates
that, once $u$ becomes infected, the waiting time until $u$ infects $v$ is
distributed as $\Exponential(\gls{beta}_e)$. Note that $v$ may become infected
before this time has elapsed, if $v$ has other incoming edges. $v$ also has a
removal rate $\gls{nu}_v$, so that the waiting time until removal of $v$ from
the population is $\Exponential(\gls{nu}_v)$. Removal may correspond to death
or recovery with immunity, or a combination of both, but in our implementation
recovered nodes never re-enter the susceptible population. We define a
\defn{discordant edge} as an edge $(u, v)$ where $u$ is infected and $v$ has
never been infected. \add{In the epidemiology literature, the symbol $\gamma$
is usually used in place of \gls{nu}; we use \gls{nu} here to distinguish the
recovery rate from the power law exponent of scale free networks (see
\cref{subsec:pa}).}

To describe the algorithm, we introduce some notation and variables. Let
$\inc(v)$ be the set of incoming edges to $v$, and $\out(v)$ be the set of
outgoing edges from $v$. Let \gls{gI} be the set of infected nodes in the
network, \gls{gR} be the set of removed nodes, \gls{gS} be the set of
susceptible nodes, and \gls{gD} be the set of discordant edges in the network.
Let \gls{beta} be the total transmission rate over all discordant edges, and
\gls{nu} be the total removal rate of all infected nodes,
\[
    \gls{beta} = \sum_{e \in \gls{gD}} \gls{beta}_e, \quad
    \gls{nu} = \sum_{v \in \gls{gI}} \gls{nu}_v.
\]
The variables \gls{gS}, \gls{gI}, \gls{gR}, \gls{gD}, \gls{beta}, and \gls{nu}
are all updated as the simulation progresses. When a node $v$ becomes infected,
it is deleted from \gls{gS} and added to \gls{gI}. Any formerly discordant
edges in $\inc(v)$ are deleted from \gls{gD}, and edges in $\out(v)$ to nodes
in \gls{gS} are added to \gls{gD}. If $v$ is later removed, it is deleted from
\gls{gI} and added to \gls{gR}, and any discordant edges in $\out(v)$ are
deleted from \gls{gD}. At the time of either infection or removal, the
variables \gls{beta} and \gls{nu} are updated to reflect the changes in the
network. \del{The updates to \gls{gS}, \gls{gI}, \gls{gR}, \gls{gD},
\gls{beta}, and \gls{nu} are straightforward and are not written explicitly in
the algorithm.}

\newcommand{\tip}{\mathit{tip}}

The Gillespie simulation algorithm is given as \cref{alg:nettree}. The
transmission tree \gls{T} is simulated along with the epidemic. We keep a map
called ``$\tip$'', which maps infected nodes in \gls{gI} to the tips of
\gls{T}. The simulation continues until either there are no discordant edges
left in the network, or we reach a user-defined cutoff of time (\gls{tmax}) or
number of infections (\gls{I}). We use the notation $\Uniform(0, 1)$ to
indicate a number drawn from a uniform distribution on $(0, 1)$, and
$\Exponential(\lambda)$ to indicate a number drawn from an exponential
distribution with rate $\lambda$. The combined number of internal nodes and
tips in \gls{T} is denoted $|T|$. \add{The updates to \gls{gS}, \gls{gI},
\gls{gR}, \gls{gD}, \gls{beta}, and \gls{nu} described in the previous
paragraph are not written explicitly in \cref{alg:nettree}, as they are quite
straightforward and would only obfuscate the pseudocode.}

\begin{algorithm}
    \label{alg:nettree}
    \caption{Simulation of an epidemic and transmission tree over a contact network}
    \begin{algorithmic}
        \State infect a node $v$ at random, updating \gls{gS}, \gls{gI},
                 \gls{gD}, \gls{beta}, and \gls{nu}
        \State $\gls{T} \gets$ a single node with label $1$
        \State $\tip[v] \gets 1$
        \State $\gls{t} \gets 0$
        \While{$\gls{gD} \neq \emptyset$ and $|\gls{gI}| + |\gls{gR}| < \gls{I}$
                and $\gls{t} < \gls{tmax}$}
            \State $s \gets \min(\gls{tmax} - \gls{t},\,
                \Exponential(\gls{beta} + \gls{nu}))$
            \For{$v \in \tip$}
                \State{extend the branch length of $\tip[v]$ by $s$}
            \EndFor
            \State $\gls{t} \gets \gls{t} + s$
            \If{$\gls{t} < \gls{tmax}$}
                \If{$\Uniform(0, \gls{beta} + \gls{nu}) < \gls{beta}$}
                    \State choose an edge $e = (u, v)$ from \gls{gD} with
                        probability $\gls{beta}_e / \gls{beta}$ and infect $v$
                    \State $\tip[v] \gets |\gls{T}|+1$
                    \Comment add new tips to tree and tip array
                    \State $\tip[u] \gets |\gls{T}|+2$
                    \Comment corresponding to $u$ and $v$ 
                    \State add tips with labels $(|\gls{T}|+1)$ and
                        $(|\gls{T}|+2)$ to \gls{T}
                    \State connect the new nodes to $\tip[v]$ in $\gls{T}$,
                        with branch lengths $0$ 
                \Else
                    \State choose a node $v$ from \gls{gI} with probability
                        $\gls{nu}_v / \gls{nu}$ and remove $v$
                    \State delete $v$ from $\tip$
                \EndIf
                \State update \gls{gS}, \gls{gI}, \gls{gR}, \gls{gD},
                    \gls{beta}, and \gls{nu}
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\subsection{Phylogenetic kernel}

The tree kernel developed by \textcite{poon2013mapping} provides a
comprehensive similarity score between two phylogenetic trees, via the
dot-product of the two trees' feature vectors in the space of all possible
subset trees with branch lengths (see \cref{subsec:treeshape}). \add{Because
the branch lengths are continuous, there are infinitely many possible subset
trees; hence, the feature space is infinite-dimensional.} The kernel was
implemented using the fast algorithm developed by
\textcite{moschitti2006making}. First, the production rule of each node, which
is the total number of children and the number of leaf children, is recorded.
The nodes of both trees are ordered by production rule, and a list of pairs of
nodes sharing the same production rule is created. These are the nodes for
which the value of the tree kernel must be computed - all other pairs have a
value of zero. The pairs to be compared are then re-ordered so that the child
nodes are always evaluated before their parents. Due to its recursive
definition, ordering the pairs in this way allows the tree kernel to be
computed by dynamic programming. The complexity of this implementation is
$O(|\gls{T}_1||\gls{T}_2|)$ for the two trees $\gls{T}_1$ and $\gls{T}_2$ being
compared.

The tree kernel cannot be used directly as a distance measure for \gls{ABC},
since it is maximized, not minimized, when the two trees being compared are the
same. Therefore, we defined the distance between two trees as
\[
    \gls{rho}(\gls{T}_1, \gls{T}_2) = 1 - \frac{K(\gls{T}_1, \gls{T}_2)}{\sqrt{K(\gls{T}_1, \gls{T}_1) K(\gls{T}_2, \gls{T}_2)}},
\]
which is a number between 0 and 1 that is minimized when $\gls{T}_1 = \gls{T}_2$. This is
similar to the normalization used by \textcite{collins2002new,
poon2013mapping}.

\subsection{Adaptive sequential Monte Carlo for Approximate Bayesian computation}
\label{subsec:adaptsmc}

We implemented the adaptive \gls{SMC} algorithm for \gls{ABC} developed by
\textcite{del2012adaptive}. This algorithm is similar to the reference
\gls{ABC}-\gls{SMC} algorithm described in \cref{subsec:abcalg}, except that
the sequence of tolerances $\gls{epsilon}_i$ is automatically determined rather
than specified in advance. The tolerances are chosen such that the \gls{ESS} of
the particle population, which indicates the quality of the Monte Carlo
approximation (see \cref{subsec:sis}), decays at a controlled rate. A
sudden precipitous drop in \gls{ESS} would indicate that only a small number of
particles had non-zero importance weights, which would result in a very poor
Monte Carlo approximation to the target distribution. This situation is
referred to as the collapse of the approximation \add{or particle degeneracy
(see \cref{subsec:smc})} and is mitigated by the adaptive approach. A single
parameter \del{$\alpha$ (not to be confused with the \gls{BA} model parameter)}
controls the decay rate. \add{In the original paper of
\textcite{del2012adaptive}, the parameter is called $\alpha$, but to avoid
confusion with the \gls{BA} parameter of the same name we will refer to it
here as \gls{alphaess}.} The tolerance $\gls{epsilon}_i$ is chosen to satisfy
\[
    \ESS(w_i) = \gls{alphaess} \ESS(w_{i-1}),
\]
where, $w_i$ is the vector of weights at the $i$th step. Note that, since $w_i$
depends on $\gls{epsilon}_i$, this equation solves for the updated weights and
the updated tolerance simultaneously. As pointed out by
\textcite{del2012adaptive}, the equation has no analytic solution, but can be
solved numerically by bisection. The forward kernels $K_i$ are taken to be
\gls{MCMC} kernels with stationary distributions $\pi_{\gls{epsilon}_i}$ and
proposal distributions
\[
    \gls{q}_i\left(\theta^{(k)}, \theta^{(k)\prime}\right) 
    \prod_{j=1}^{\gls{M}} \lik\left(z^{(j,k)\prime} \middle| \theta^{(k)\prime} \right),
\]
where $\theta^{(k)}$ is the vector of model parameters \add{associated with
particle $x^{(k)}$} and $z^{(j,k)\prime}$, $1 \leq j \leq \gls{M}$, are \gls{M}
datasets simulated according to $\theta^{(k)\prime}$. In our implementation,
the $\gls{q}_i$ \add{are constructed component-wise for $\theta$ out of}
Gaussian proposals for continuous parameters and Poisson proposals for discrete
parameters. For the Poisson proposals, the number of \add{discrete} steps to
move the particle is drawn from a Poisson distribution, and the direction in
which to move the particle is chosen uniformly at random. The variance of each
proposal distribution was set equal to twice the empirical variance of the
particles, following~\autocite{beaumont2009adaptive, del2012adaptive}. The
backwards kernels are
\[
    L_{i-1}(x', x) = \frac{\pi_i(x)K_i(x, x')}{\pi_i(x')}.
\]
Here we have written $x'$ for $x_i$ and $x$ for $x_{i-1}$ to emphasize that
$x_{i-1}$ is the current value of the particle and $x_i$ is the proposed value.
When substituted into \cref{eq:smcwt}, the forward kernels $K_i(x, x')$ and
densities $\pi_i(x') = \pi_{\varepsilon_i}(x')$ cancel out, and we are left
with the following weight update.
\begin{align*}
    w_i(x) 
    & \madd{\propto w_{i-1}(x) \frac{\pi_i(x') L_{i-1}(x', x)}{\pi_{i-1}(x)K_i(x, x')}}
        & \madd{\text{importance weight update from SMC}} \\
    &\madd{=w_{i-1}(x) \frac{\pi_i(x') \pi_i(x) K_i(x, x')}{\pi_{i-1}(x)K_i(x, x')\pi_i(x')}}
        & \madd{\text{substitute } L_{i-1}} \\
    &= w_{i-1}(x) \frac{\pi_i(x)}{\pi_{i-1}(x)}
        & \madd{\text{cancel } K_i(x, x') \text{ and } \pi_i(x')} \\
    &= \madd{w_{i-1}(x) \frac{\pi(\theta) \prod_{j=1}^M f(z^{(j)\prime} \mid \theta) 
                        \sum_{j=i}^M \I_{A_{\varepsilon_i, y}}(z^{(j)})}
                       {\pi(\theta) \prod_{j=1}^M f(z^{(j)\prime} \mid \theta) 
                       \sum_{j=i}^M \I_{A_{\varepsilon_{i-1}, y}}(z^{(j)})}}
        & \madd{\text{definition of } \pi_i(x)} \\
    &= w_{i-1}(x) \frac{\sum_{j=i}^M \I_{A_{\varepsilon_i, y}}(z^{(j)})}
        {\sum_{j=i}^M \I_{A_{\varepsilon_{i-1}, y}}(z^{(j)})}
        & \madd{\text{cancel prior and likelihood.}}
\end{align*}
In other words, when the distance threshold $\gls{epsilon}_{i-1}$ is contracted
to $\gls{epsilon}_i$, the particles' weights are multiplied by the proportion of
simulated datasets that are still inside the new threshold. The user may
specify a final tolerance $\gls{epsilon}$, or a final acceptance rate of the
\gls{MCMC} kernel, and the algorithm will be stopped when either of these
termination conditions is reached. The latter condition stops the algorithm
when the particles are not moving around very much, implying little change in
the estimated target.

\subsection{Justification for approach}
\label{subsec:just}

\add{We present here a non-rigorous justification for the use of
\gls{ABC} for the problem at hand, as opposed to more frequently-used
approaches for fitting mathematical models (see \cref{chp:prelim}). Consider a
contact network model with parameters $\theta$, and an estimated transmission
tree \gls{T}. Taking a Bayesian approach, our aim is to obtain a sample from
the posterior distribution on the model's parameters given our data,}
\[
    \madd{\post(\theta \mid \gls{T}) = \frac{\lik(\gls{T} \mid \theta) \prior(\theta)}
    {\int_{\Theta}\lik(\gls{T} \mid \theta) \prior(\theta) \d\theta}.}
\]
\add{For all but the simplest models, the normalizing constant in the
denominator is an intractable integral. What we shall argue here is that, in
contrast to most commonly studied mathematical models, the likelihood
$\lik(\gls{T} \mid \theta)$ is also likely to be intractable in our case.}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{transtree}
    \caption[
        Illustration of an estimated transmission tree without labels and two
        possible underlying complete transmission trees with labels.
    ]{
        Illustration of an estimated transmission tree without labels (left)
        and two possible underlying complete transmission trees with labels
        (right). In the top right scenario, the epidemic began with node $a$
        who transmitted first to $b$ and then to $c$. In the bottom right
        scenario, $b$ was the index case; $b$ infected $c$, who went on to
        infect $a$. A transmission tree estimated from a viral phylogeny would
        have the same topology and tip labels in both cases.
    }
    \label{fig:tt}
\end{figure}

\add{As discussed in \cref{subsec:tt}, the internal nodes of transmission trees
represent transmission events, and are labelled with the donor in the
associated transmission pair. However, when we estimate a transmission tree
from viral sequence data, we generally only know the labels of the tips of the
tree, not the labels of the internal nodes. In viral phylogenies, the
transmissions are at least partially preserved through the evolutionary
relationships among the viruses, but the directionality of those transmissions
is unknown. Thus, a single estimated transmission tree can correspond to many
possible pathways of the epidemic through the network. \Cref{fig:tt}
illustrates this concept for a simple transmission tree with three tips. When
calculating a likelihood given a transmission tree, we must sum over all
possible labellings of the internal nodes. Let $\L$ be the set of such
labellings. Then}
\begin{align}
    \madd{\lik(\gls{T} \mid \theta) = \sum_{l \in \L} \lik(\gls{T},\, l \mid \theta).}
    \label{eq:lik1}
\end{align}
\add{A contact network model assigns a probability density to each possible
contact network. Transmission trees are realized over particular contact
networks, not over the model itself. Therefore, we must also sum over all
contact networks which could be generated by the model. Let $\G$ be the set of
all possible contact networks. Summing \cref{eq:lik1} over $\G$ gives}
\begin{align}
    \madd{\lik(\gls{T} \mid \theta) = \sum_{G \in \G} \sum_{l \in \L} 
        \lik(\gls{T},\, l \mid G, \theta) \lik(G \mid \theta).}
    \label{eq:lik2}
\end{align}
\add{This can be simplified somewhat by noticing that, given a specific
contact network, the labelled transmission tree depends only on that network
and not on the model that generated it. That is, $\lik(\gls{T}, \nu \mid G,
\theta) = \lik(\gls{T},\, l \mid G)$, and}
\begin{align}
    \madd{\lik(\gls{T} \mid \theta) = \sum_{G \in \G} \lik(G \mid \theta) \sum_{l \in \L} 
    \lik(\gls{T}, \nu \mid G)}
    \label{eq:lik3}
\end{align}
\add{Under the assumption that both transmission and removal are Poisson
processes, calculating $\lik(\gls{T},\, l \mid G)$ can be accomplished by a
straightforward modification of the Gillespie simulation algorithm
(\cref{alg:nettree}). Rather than choosing transmission or removal events
according to their probabilities, the events would be deterministically chosen
based on the transmission tree and the probabilities of each event would be
multiplied together. Assuming efficient data structures for storing lists of
nodes and edges, the complexity of this calculation would be $O(|\gls{T}|)$.
The number of possible labellings of internal nodes of $\gls{T}$ is easily seen
to be $2^{(|\gls{T}|-1)/2}$ by noticing that each of the $(|\gls{T}|-1)/2$
internal nodes must be labelled with the same label as either its right child
or its left child. Although exponential calculations of this nature can are
often be simplified on trees using dynamic programming (\eg
\autocite{pupko2000fast}), it cannot be straightforwardly applied in this case
because the subtrees' probabilities depend on the existing epidemic progress
(their parents and siblings). Hence, calculating the inner sum over labels may
take time $O(2^{(|\gls{T}|-1)/2})$.}

\add{The outer sum, over all contact networks, is also difficult to evaluate
in general. There are $2^{\gls{N}(\gls{N}-1)}$ directed graphs on \gls{N}
nodes~\autocite{harary2014graphical}. There must be at least as many nodes in
the contact network as the number of tips in the tree, which is
$(|\gls{T}|+1)/2$. Of course, it is very likely that there are more nodes in
the network than observed tips because some individuals are never infected
and/or some infected individuals are never sampled. The complexity of
calculating $\lik(G \mid \theta)$ is obviously dependent on the particular
model being investigated. For the \gls{BA} model, we might have to sum over all
possible orders in which the nodes could be added, and all assignments of edges
to the nodes which generated them. However, even in the case that calculating
$\lik(G \mid \theta)$ can be done in constant time, the sum (\ref{eq:lik3})
still has at least $O(2^{|\gls{T}|^2})$ terms.}

\add{We have shown that both the normalizing constant $\int_\Theta \lik(\gls{T}
\mid \theta) \prior(\theta) \d\theta$ and the likelihood $\lik(T \mid \theta)$
are likely computationally prohibitive to calculate. If this is the case, the
problem of fitting contact network models to phylogenies seems to be of the
\emph{doubly-intractable} type~\autocite{murray2012mcmc}, which would imply that
these models are not amenable to neither \gls{ML} nor Bayesian inference
techniques. Although both methods are able to cope with an intractable
normalizing constant (for example, by local search for \gls{ML} or Bayesian
\gls{MCMC}), neither can avoid the intractable likelihood calculations. This
justifies the use of \gls{ABC}, which is a likelihood-free method.}

\add{We have not proven here that \cref{eq:lik3} is impossible to calculate in
polynomial time - it could be possible to algebraically simplify the sum into a
tractable expression. Furthermore, under certain models, a large proportion of
$\G$ may have zero probability, which would enable the simplification of the
outer sum on a model-specific basis. It should also be noted that extensions of
Bayesian \gls{MCMC} have been developed for doubly-intractable
problems~\autocite{liang2010double, murray2012mcmc}, which might be adaptable
to the problem at hand. These have not been as widely used as \gls{ABC}, nor
are they as easily parallelizable as \gls{SMC}.}

\section{Analysis of \acrlong{BA} model with synthetic data}
\label{sec:ba}
\glsreset{BA}

\subsection{Why study the \acrlong{BA} model?}
\label{subsec:whyba}

\add{We developed \software{netabc} with the objective of extracting useful,
quantitative information about network structures from viral phylogenies. An
important aspect of ``usefulness'' is model specification and the biological or
epidemiological interpretation of the parameters. We want the model to be
realistic, but no so complicated that it becomes difficult to interpret. At
least some of the parameters should be of interest from a theoretical or
practical perspective, or there would be no point in estimating them. Since
\software{netabc} is a phylodynamic method, intended to be used with viral
sequence data, we would also like to choose parameters which may be difficult
to estimate with more standard methods. Otherwise, our method provides no
advantage. The \gls{BA} model (\cref{subsec:pa}) satisfies these criteria,
albeit some better than others. The purported realism of the model stems from
the fat-tailed degree distributions it produces, which are similar to those
observed in real world sexual networks~\autocite{colgate1989risk,
liljeros2001web, schneeberger2004scale, clemenccon2015statistical,
rothenberg2007large}. Moreover, the ``rich get richer'' phenomenon, where
popular individuals attract new connections at an elevated rate, is intuitively
reasonable for both sexual~\autocite{de2007preferential} and
\gls{IDU}~\autocite{dombrowski2013topological} networks. However, the model is
very simple, assuming that all nodes form the same number of links when added
to the network and share the same preference for popular individuals.}

\add{In this thesis, we consider four parameters related to the \gls{BA} model,
denoted \gls{N}, \gls{m}, \gls{alpha}, and \gls{I} (see \cref{subsec:pa}). The
first three of these parameterize the network structure, while \gls{I} is
related to the simulation of transmission trees over the network. However, we
will refer to all four as \gls{BA} parameters. \gls{N} denotes the total number
of nodes in the network, or equivalently, susceptible individuals in the
population. \gls{m} is the number of new undirected edges added for each new
vertex, or equivalently one-half of the average degree. \gls{alpha} is the
power of preferential attachment -- new nodes are attached to existing nodes of
degree $d$ with probability proportional to $d^{\gls{alpha}} + 1$. Finally,
\gls{I} is the number of infected individuals at the time when sampling occurs.
The \gls{alpha} parameter is unitless, while \gls{m} has units of edges or
connections per vertex, and \gls{N} and \gls{I} both have units of nodes or
individuals.}

\add{From a public health standpoint, all four parameters are of some interest.
The prevalence \gls{I} can be used to estimate the resources required to combat
an ongoing epidemic, while total susceptible population size \gls{N} provides a
similar metric for preventative measures. The average degree of the network, in
this case $2\gls{m}$, is directly related to \gls{R0}, the basic reproductive
number~\autocite{britton2002bayesian}. \gls{R0} quantifies the average number
of secondary infections ultimately caused by one infected individual; higher
\gls{R0} generally indicates faster epidemic growth and/or larger eventual
epidemic size~\autocite{anderson1992infectious}. In a homogeneously mixed
population, the theoretically expected proportion of the population which must
be vaccinated to control an epidemic (the \defn{vaccination threshold}) can be
expressed in terms of \gls{R0}~\autocite{anderson1982directly}. Although
optimal vaccination strategies may differ in heterogeneous contact
structures~\autocite{ma2013importance}, it is reasonable to suppose that there
would still be a relationship between \gls{m}, \gls{R0}, and the vaccination
threshold. The preferential attachment power \gls{alpha} quantifies the degree
to which high-degree nodes, also called
superspreaders~\autocite{kemper1980identification}, characterize the network
structure. Superspreaders have been hypothesized to play a disproportionately
greater role in the spread of several diseases~\autocite{stadler2013uncovering,
shen2004superspreading}. If so, network-based
interventions~\autocite{little2014using, wang2015targeting} may be worth
considering as part of an epidemic control strategy. \gls{alpha} can also offer
some insight into how the network would react to the removal of nodes.
\textcite{dombrowski2013topological} found evidence of preferential attachment
in \gls{IDU} networks, and suggested that as a consequence of this
characteristic, the removal of random nodes (such as through a police
crackdown) might inadvertently make it easier for epidemics to spread. When
individuals with only one or two connections lose them, they might tend to seek
out well-known (that is, high-degree) members of the community, thus increasing
those individuals' connectivity even further. Although we do not consider a 
dynamic network in this work, it is not unreasonable that the network could be
close to static over short periods of time, and this static structure might be
informative of future dynamic behaviour.}

\add{Though it is theoretically possible to estimate all four \gls{BA}
parameters using more conventional approaches, the cost of doing so may be
high, in addition to other situation-specific challenges. In theory, any
network parameter can be estimated by explicitly constructing the contact
network, although this is highly resource intensive and is hampered by
misreporting and other challenges~\autocite{eames2015six}. All parameters
become more difficult to estimate when the infected population is ``hidden''
due to illegal or stigmatized behaviour, as is sometimes the case with
\gls{HIV} outbreaks among \gls{IDU}, \gls{MSM}, or sex workers. In such cases,
phylodynamic methods may offer the advantage of providing information about the
whole population from only a small sample. A survey, for example, will not tell
us if there are large groups of the population that we simply have not sampled.
Our hope is that estimating \gls{N} and \gls{I} phylogenetically might provide
this additional information. The average degree of the network, $2\gls{m}$, is
also estimable by a survey, although individuals may be unwilling or unable to
disclose how many contacts they have had. Sequence data also provide an
advantage in this respect -- they are objective and do not share the same
biases as self-reported partner counts. The estimation of \gls{alpha} is more
complex, as this parameter is most strongly reflected in the connectivity of
very high degree nodes, who are rare in the population. Locating them might
require contact tracing, or respondent-driven
sampling~\autocite{heckathorn1997respondent}. Even if the full degree
distribution of a network is available, there are models other than
preferential attachment which can produce scale-free
networks~\autocite[\eg][]{kumar2000stochastic}. \textcite{de2007preferential}
were able to estimate \gls{alpha} by maximum likelihood using partner count
data from several sequential time intervals, but they admit such detailed data
are not usually available. Moreover, their dataset was constructed via a random
survey, which would likely miss the few high-degree nodes characterizing a
power law degree distribution. In summary, each of the \gls{BA} parameters may
be estimated without phylodynamics, but there are sufficient difficulties that
we believe an alternative method using sequence data is warranted.} 

\subsection{Using synthetic data to investigate identifiability and sources of
estimation error}

\add{We have argued that the parameters of the \gls{BA} model are interesting
and worth estimating with phylodynamic methods. However, these estimates will
only constitute ``useful'' information about the network if the parameters are
identifiable from phylogenetic data. Roughly speaking, the identifiability of a
parameter says how much information about that parameter can possibly be
obtained from the observed data. If the parameters of the \gls{BA} model do not
influence tree shape at all, then we cannot possibly estimate them -- the
posterior distribution will exactly resemble the prior, no matter how accurate
a representation of the posterior we are able to produce. Hence, before
proceeding with a full validation of \software{netabc} on simulated data, we
undertook two experiments designed to assess the identifiability of the
\gls{BA} parameters. These experiments only investigated one parameter of the
\gls{BA} model at a time while holding all others fixed, a strategy commonly
used when performing sensitivity analyses of mathematical models. This allowed
us to perform a fast preliminary analysis without dealing with the ``curse of
dimensionality'' of the full parameter space. The experiments are motivated and
described on a high level here, with more detail provided in the next section.}

\add{First, we simulated trees under three different values of each parameter,
and asked how well we could tell the different trees apart. The better we are
able to distinguish the trees, the more identifiability we might expect for the
corresponding parameter when we attempt to estimate it with \gls{ABC}. This
experiment also had the secondary purpose of validating our choice of the tree
kernel as a distance measure in \gls{ABC}. To tell the trees apart, we used a
classifier based on the tree kernel, but we also tested two other tree shape
statistics: one which considers only the topology, and another which considers
only branching times. Since the tree kernel incorporates both of these sources
of information, we expected it to outperform the other two statistics. Finally,
the tree kernel can be ``tuned'' by adjusting the values of the meta-parameters
\gls{lambda} and \gls{sigma}. The results of this experiment were used to
select values for these meta-parameters to carry forward to the rest of the
thesis, based on their accuracy in distinguishing the different trees. }

\add{A second experiment was designed to test whether we could actually
estimate the parameters numerically, rather than just telling three different
values apart, and also to assess how identifiability varied in different
regions of the parameter space. An individual tree was compared to simulated
trees on a one-dimensional grid of values of one \gls{BA} parameter, to obtain
a ``distribution'' of kernel score values. From this distribution, estimates
and credible intervals of the parameter could be calculated. Repeating this
experiment with trees located throughout the parameter space allowed us to
better quantify the identifiability. Furthermore, doing marginal estimation
(that is, estimating one parameter with all others fixed) can provide insight
into any biases observed when doing joint estimation with \gls{ABC}. If grid
search is inaccurate, it indicates a lack of parameter identifiability.
However, if the marginal grid search estimates are accurate but the estimates
obtained with \gls{ABC} are biased, this points to confounding between the
parameters which could only be observed when they are all estimated jointly.}

\add{After these preliminary experiments, the strategy we used for testing
\software{netabc} was a standard simulation-based validation. Transmission
trees were simulated under several combinations of parameter values, and we
tried to recover these values with \software{netabc}. We then used a
multivariable analysis to investigate how accuracy of these estimates was
influenced by the true parameter values.}

\add{In the previous section, we argued that the \gls{BA} model parameters were
worth investigating, and here we have presented several computational
experiments designed to assess their identifiability. There is a final, more
technical aspect of our method's ``usefulness'' to consider, which is the
accuracy of the \gls{ABC} approximation to the posterior. As discussed in
\cref{sec:abc}, \gls{ABC} does not target the posterior distribution directly,
but rather an approximate posterior derived from simulated data and a distance
function. \gls{ABC} \emph{assumes} that this approximate posterior resembles
the true posterior, and it is critical for our estimates' relevance that this
assumption holds. There are two potential causes of an inaccurate \gls{ABC}
approximation~\autocite{fearnhead2012constructing}. First, the Monte Carlo
approximation to the \gls{ABC} target distribution may be poor, due to the
settings used for \gls{ABC}-\gls{SMC}. Second, the \gls{ABC} target
distribution may not resemble the posterior, due to a poor choice of distance
function. We designed two experiments to investigate the impact of these
sources of error.}

\add{The Monte Carlo approximation error is fairly easily quantified by simply
increasing the computing power used for \gls{SMC}. We ran one simulation using
a larger number of particles, more simulated datasets per particle, and a
higher value for \gls{alphaess}. A substantial improvement in accuracy
resulting from these changes would likely indicate a high Monte Carlo error
with the lower settings. The second issue, the resemblance of the \gls{ABC}
target distribution to the true posterior, is somewhat more difficult to
investigate. We do not have access to the true posterior, even for simulations
where the true parameter values are known. To address this source of error, we
performed marginal parameter estimation with \gls{ABC} by informing
\software{netabc} of some of the true parameter values. Any inaccuracy or bias
observed only in the joint estimation results, but not the marginal estimates, 
is most easily explained by interdependence between parameters in the true
posterior. However, errors observed in both marginal and joint estimates could
be due to either the shape of the true posterior or an inaccurate \gls{ABC}
approximation, and we have no way to distinguish one from the other. In other
words, this experiment provided only an upper bound on the error due to an
inaccurate \gls{ABC} approximation.}

\subsection{Methods}

\del{We investigated four parameters related to the \gls{BA} contact network
model, denoted \gls{N}, \gls{m}, \gls{alpha}, \gls{I} (see \cref{subsec:pa}).
The first three of these are parameters of the model itself, while \gls{I} is
related to the simulation of transmission trees over the network. However, we
will refer to all four as \gls{BA} parameters. \gls{N} denotes the total number
of nodes in the network, or equivalently, susceptible individuals in the
population. When a node is added to the network, \gls{m} new undirected edges
are added incident to it, and are attached to existing nodes of degree $d$ with
probability proportional to $d^{\gls{alpha}} + 1$ (\cref{subsec:pa}). To
simulate transmission trees over a \gls{BA} network, we allowed an epidemic to
spread until \gls{I} nodes were infected, and sampled a transmission tree at
that time.}

\add{For all simulations,} we assumed that all contacts had symmetric
transmission risk, which was implemented by replacing each undirected edge in
the network with two directed edges (one in each direction). Nodes in our
networks followed simple \gls{SI} dynamics, meaning that they became infected
at a rate proportional to their number of infected neighbours, and never
recovered. We did not consider the time scale of the transmission trees in
these simulations, only their shape. Therefore, the transmission rate along
each edge in the network was set to 1, the removal rate of each node was set to
0, and all transmission trees' branch lengths were scaled by their mean. The
\textit{igraph} library's implementation of the BA
model~\autocite{csardi2006igraph} was used to generate the graphs. The analyses
were run on Westgrid (\url{https://www.westgrid.ca/}) and a local computer
cluster. With the exception of our own C programs, all analyses were done in
\software{R}, and all packages listed below are \software{R} packages.
\add{Code to run all experiments is freely available at
    \url{https://github.com/rmcclosk/thesis}.}

\subsubsection*{Classifiers for BA model parameters based on tree shape}
\label{subsec:kernel}

\glsreset{nltt}

\add{Our first computational experiment was designed as an exploratory analysis
of the four \gls{BA} model parameters defined above: \gls{alpha}, \gls{I},
\gls{m}, and \gls{N}. The objective of this experiment was to determine whether
any of the four parameters were identifiable from the shape of the transmission
tree, as quantified by the tree kernel. Each of the \gls{BA} model parameters
was varied one at a time, while holding the other parameters at fixed, known
values. Contact networks were generated according to each set of parameter
values, and transmission trees were simulated over the networks. We then
evaluated how well a classifier based on the tree kernel could differentiate
the trees simulated under distinct parameter values. If the classifier's
cross-validation accuracy was high, this could be taken as an indication that
the parameter in question was identifiable in the range of values considered. 
A caveat of this preliminary analysis is that, since all parameters but one
were held at known values, nothing could be said about the identifiability of
\emph{combinations} of parameters; this issue will be explored later by jointly
estimating all parameters with \gls{ABC}}.

\add{In addition to testing for identifiability, a secondary objective of
this analysis was to validate the use of the tree kernel as a distance measure
for \gls{ABC} in our context. As discussed in \cref{sec:abc}, the choice of
distance function is extremely important for the accuracy of the \gls{ABC}
approximation to the posterior. Therefore, we evaluated two additional tree
statistics in the same manner as we evaluated the tree kernel (that is, by
constructing and testing a classifier). First, we considered Sackin's
index~\autocite{shao1990tree}, which measures the degree of imbalance or
asymmetry in a phylogeny (see \cref{subsec:treeshape}). Sackin's index is
widely used for characterizing phylogenies~\autocite{frost2013modelling} and
has been demonstrated to vary between transmission trees simulated under
different contact network types~\autocite{leventhal2012inferring}. Sackin's
index does not take branch lengths into account, considering only the tree's
topology. The other statistic we considered was the
\gls{nltt}~\autocite{janzen2015approximate}, which compares two trees based on
normalized distributions of their branching times (see
\cref{subsec:treeshape}). In contrast with Sackin's index, the \gls{nltt} does
not explicitly consider the trees' topologies, but it does use their normalized
branch lengths. While the \gls{nltt} is a newly developed statistic not yet in
widespread use, the unnormalized \gls{ltt}~\autocite{nee1992tempo} was the
basis of seminal early work extracting epidemiological information from
phylogenies~\autocite{holmes1995revealing}. We expected the tree kernel to
classify the \gls{BA} parameters more accurately than either Sackin's index or
the \gls{nltt}, since the tree kernel takes both topology and branch lengths
into account.}

This experiment involved a large number of variables that were varied
combinatorially. For ease of exposition, we will describe a single experiment
first, then enumerate the values of all variables for which the experiment was
repeated. The parameters of the tree kernel, $\lambda$ and $\sigma$
(\cref{subsec:treeshape}), will be referred to as \defn{meta-parameters} to
distinguish them from the parameters of the \gls{BA} model. 

The attachment power parameter \gls{alpha} was varied among three values: 0.5,
1.0, and 1.5. For each value, the \software{sample\_pa} function in the
\software{igraph} package was used to simulate 100 networks, with the other
parameters set to \gls{N} = 5000 and \gls{m} = 2. This step yielded a total of
300 networks. An epidemic was simulated on each network using our
\software{nettree} binary until \gls{I} = 1000 nodes were infected, at which
point 500 of them were sampled to form a transmission tree. A total of 300
transmission trees were thus obtained, comprised of 100 trees for each of the
three values of \gls{alpha}. The trees were ``ladderized'' so that the subtree
descending from the left child of each node was not smaller than that
descending from the right child. Summary statistics, such as Sackin's index and
the ratio of internal to terminal branch lengths, were computed for each
simulated tree using our \software{treestat} binary. The trees were visualized
using the \software{ape} package~\autocite{paradis2004ape}. \add{Both the tree
kernel and the \gls{nltt} are pairwise statistics, and the \glspl{SVR}
classifiers we used to investigate them operate on pairwise distance matrices.}
Our \software{treekernel} binary was used to calculate the value of the kernel
for each pair of trees, with the meta-parameters set to $\lambda = 0.3$ and
$\sigma = 4$. These values were stored in a symmetric 300 $\times$ 300 kernel
matrix.  Similarly, we computed the \gls{nltt} statistic between each pair of
trees using our \software{treestat} binary, and stored them in a second $300
\times 300$ matrix.

To investigate the identifiability of \gls{alpha} from tree shape, we
constructed classifiers for \gls{alpha} based on the three tree shape
statistics discussed above. First, we used the \software{kernlab}
package~\autocite{zeileis2004kernlab} to create a \gls{kSVR} classifier using
the computed kernel matrix. Second, we used the \software{e1071}
package~\autocite{meyer2015e1071} to create an ordinary \gls{SVR} classifier
using the pairwise \gls{nltt} matrix. Finally, we performed an ordinary linear
regression of \gls{alpha} against Sackin's index. Each of these classifiers was
evaluated with 1000 two-fold cross-validations. We also performed a \gls{kPCA}
projection of the kernel matrix, and used it to visualize the separation of the
different \gls{alpha} values in the tree kernel's feature space. A schematic of
this experiment is presented in \cref{fig:kernelexpt}.

\begin{figure}[ht]
    \centering
    \includegraphics{kernel-expt.pdf}
    \caption[
        Schematic of classifier experiments investigating identifiability of BA
        model parameters from tree shapes.
    ]{
        Schematic of classifier experiments investigating identifiability of BA
        model parameters from tree shapes. The parameters of the BA model were
        varied one at a time \add{while holding all others fixed}. Transmission
        trees were simulated under three different values of each
        parameter\del{, then compared pairwise using the tree kernel.}
        Classifiers were constructed for each parameter \add{based on three
        tree shape statistics}, and their accuracy was evaluated by
        cross-validation. Kernel-\gls{PCA} projections were used to visually
        examine the separation of the trees in the feature space defined by the
        tree kernel.
    }
    \label{fig:kernelexpt}
\end{figure}

Similar experiments were performed with the values shown in
\cref{tab:kernelexpt}. The other three \gls{BA} parameters, \gls{N}, \gls{m},
and \gls{I}, were each varied while holding the others fixed. The experiments
for \gls{alpha}, \gls{m}, and \gls{N} were repeated with three different values
of \gls{I}. All experiments were repeated with trees having three different
numbers of tips. Kernel matrices were computed for all pairs of the
meta-parameters \gls{lambda} = \sett{0.2, 0.3, 0.4} and \gls{sigma} =
\sett{\nicefrac18, \nicefrac14, \nicefrac12, 1, 2, 4, 8}.

\begin{landscape}
\begin{table}[ht]
    \centering
    \input{\tablepath/kernel-expt}
    \caption[
        Values of parameters and meta-parameters used in classifier experiments
        to investigate identifiability of BA model parameters from tree shapes.
    ]{
        Values of parameters and meta-parameters used in classifier experiments
        to investigate identifiability of BA model parameters from tree shapes.
        Each row corresponds to one of the BA model parameters. One kernel
        matrix was created for every combination of values except the one
        indicated in the ``varied parameter'' column, which was varied when
        producing simulated trees.
    }
    \label{tab:kernelexpt}
\end{table}

\begin{table}[ht]
    \centering
    \input{\tablepath/gridsearch-expt}
    \caption[
        Values of parameters and meta-parameters used in grid search
        experiments to further investigate identifiability of BA model
        parameters.
    ]{
        Values of parameters and meta-parameters used in grid search
        experiments to further investigate identifiability of BA model
        parameters. Trees were simulated under the test values, and compared to
        a grid of trees simulated under the grid values. Kernel scores were
        used to calculate point estimates and credible intervals for each
        parameter, which were compared to the test values.
    }
  \label{tab:gridexpt}
\end{table}
\end{landscape}

\subsubsection*{Grid search}

\add{The previous experiment was an exploratory analysis intended to determine
which of the \gls{BA} parameters were identifiable, and whether the tree kernel
could potentially be used to distinguish different parameter values when all
others were held fixed. In this experiment, which was still of an exploratory
nature, we continued to consider one parameter at a time while fixing the other
three. However, rather than checking for identifiability, we were now
interested in quantifying the accuracy and precision of kernel score-based
estimates. This was done by examining the distribution of kernel scores on a
grid of parameter values, when trees simulated according to those values were
compared with a single simulated test tree.}

As in the previous section, we will begin by describing a single experiment,
and then list the variables for which similar experiments were performed. We
varied \gls{alpha} along a narrowly spaced grid of values: 0, 0.01, \ldots, 2.
For each value, fifteen networks were generated with \software{igraph}, and
transmission trees were simulated over each using \software{nettree}. These
trees will be referred to as ``grid trees''. Next, one further test tree was
simulated with the test value \gls{alpha} = 0. Both the grid trees and the test
tree had 500 tips, and were simulated with the other \gls{BA} parameters set to
\add{the known values} $\gls{N} = 5000$, $\gls{m} = 2$, and $\gls{I} = 1000$.
The test tree was compared to each of the grid trees using the tree kernel,
with the meta-parameters set to $\gls{lambda} = 0.3$ and $\gls{sigma} = 4$,
using the \software{treekernel} binary. The median kernel score was calculated
for each grid value, and the scores were normalized such that the area under
the curve was equal to 1. \del{The grid value with the highest median kernel
score was taken as the point estimate for the test value.} \add{For all
parameters except \gls{m}}, 50\% and 95\% highest density intervals were
obtained using the \software{hpd} function in the \software{TeachingDemos}
package~\autocite{snow2013teachingdemos}. \add{Since the \software{hpd}
function assumes a continuous distribution, we implemented our own version for
discrete distributions to use for \gls{m}.}

Each experiment of the type just described was repeated ten times with the same
test value. Similar experiments were performed for each of the four \gls{BA}
parameters, with several test values and trees of varying sizes. The variables
are listed in \cref{tab:gridexpt}. A graphical schematic of the grid search
experiments is shown in \cref{fig:gridexpt}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{gridsearch-expt}
    \caption[
        Schematic of grid search experiment to further investigate
        identifiability of BA model parameters from tree shapes.
    ]{
        Schematic of grid search experiment to further investigate
        identifiability of BA model parameters from tree shapes. Trees were
        simulated along a narrowly spaced grid of values of one parameter
        (``grid trees'') \add{with all other parameters fixed to known values}.
        Separate trees were simulated for a small subset of the grid values
        (``test trees''), \add{also holding the other parameters fixed}. Each
        test tree was compared to every grid tree using the tree kernel, and
        the resulting kernel scores were normalized to resemble a probability
        density from which the mode and 95\% highest density interval were
        calculated.
    }
    \label{fig:gridexpt}
\end{figure}

\subsubsection*{Approximate Bayesian computation}

\add{Our final synthetic data experiment was designed to test the full
\gls{ABC}-\gls{SMC} algorithm by jointly estimating the four parameters of the
\gls{BA} model. We used the standard validation approach of simulating
transmission trees under the model with known parameter values and attempting
to recover those values with \software{netabc}. The algorithm was not informed
of any of the true parameter values for the main set of simulations. Despite
the fact that the parameter values used to generate the simulated transmission
trees were known, the true posterior distributions on the \gls{BA} parameters
were unknown. Therefore, any apparent errors or biases in the estimates could
be due to either poor performance of our method, or to real features of the
posterior distribution. The latter type of error reflects on the suitability of
the model, but does not invalidate the use of our method in cases where the
parameters are more identifiable. Two retrospective experiments were performed
to disambiguate some of the observed errors: one where we ran a simulation with
increased computational power to test for an increase in accuracy, and a second
where we estimated parameters marginally to remove confounding from the other
parameters in the joint posterior.}

\del{We simulated three trees each under a variety of parameter values and ran
the \software{netabc} program to estimate posterior distributions for the
parameters.} The parameter values and priors used are listed in
\cref{tab:abcexpt}. The tree kernel meta-parameters were set to $\gls{lambda} =
0.3$ and $\gls{sigma} = 4$. The \gls{SMC} algorithm was run with 1000
particles, five sampled datasets per particle, and \gls{alphaess} \del{(not to
be confused with the \gls{BA} preferential attachment parameter, see
\cref{subsec:adaptsmc})} set to 0.95. The algorithm was stopped when the
acceptance rate of the \gls{MH} kernel dropped below 1.5\%, the same criterion
used by \textcite{del2012adaptive}. \add{For visualization,} approximate
marginal posterior densities for each parameter were calculated using the
\software{density} function in \software{R} applied to the final weighted
population of particles. \del{Credible intervals were obtained for each
parameter using the \software{HPDinterval} function in the \software{coda}
package~\autocite{plummer2006coda}.} \add{Posterior means obtained for each
parameter using the \software{wtd.mean} function in the \software{Hmisc}
package~\autocite{harrell2016hmisc}. Credible intervals were obtained using the
\software{hpd} function in the \software{TeachingDemos}
package~\autocite{snow2013teachingdemos} for \gls{alpha} \gls{I}, and \gls{N}, 
and using our own implementation for discrete distributions for \gls{m}.}

\begin{table}[ht]
    \centering
    \input{\tablepath/abc-expt}
    \caption[
        Parameter values used in simulation experiments to test accuracy of
        \gls{BA} model fitting with \software{netabc}.
    ]{
        Parameter values used in simulation experiments to test accuracy of
        \gls{BA} model fitting with \software{netabc}. Trees were simulated
        under the test values, and \software{netabc} was used to estimate
        posterior distributions on the \gls{BA} parameters for each simulated
        tree. \software{Netabc} was na\"ive to the true parameter values.
    }
    \label{tab:abcexpt}
\end{table}

\add{To evaluate the effects of the true parameter values on the accuracy of
the posterior mean estimates, we analyzed the \gls{alpha} and \gls{I}
parameters individually using \glspl{GLM}. The response variable was the error
of the point estimate, and the predictor variables were the true values of
\gls{alpha}, \gls{I}, and \gls{m}. We did not test for differences across true
values of \gls{N}, because \gls{N} was not varied in these simulations. The
distribution family and link function for the \glspl{GLM} were chosen as
Gaussian and inverse, respectively, by examination of residual plots and
\gls{AIC}. The $p$-values of the estimated \glspl{GLM} coefficients were
corrected using Holm-Bonferroni correction~\autocite{holm1979simple} with $n =
6$ (two \glspl{GLM} with three predictors each). Because there was clearly
little to no identifiability of \gls{N} and \gls{m} with \gls{ABC} (see results
in next section), we did not construct \glspl{GLM} for those parameters.}

Two further simulations were performed to address \del{potential sources of
error} \add{the possible impact of two types of model misspecification}. To
consider the effect of heterogeneity among nodes, we generated a network where
half the nodes were attached with power $\gls{alpha} = 0.5$ and the other half
with power $\gls{alpha} = 1.5$. The other parameters for this network were
$\gls{N} = 5000$, $\gls{I} = 1000$, and $\gls{m} = 2$. To investigate the
effects of potential sampling bias~\autocite{karcher2016quantifying}, we
simulated a transmission tree where the tips were sampled in a peer-driven
fashion, rather than at random. That is, the probability to sample a node was
twice as high if any of that node's network peers had already been sampled. The
parameters of this network were $\gls{N} = 5000$, $\gls{I} = 2000$, $\gls{m} =
2$, and $\gls{alpha} = 0.5$.

\add{To assess the impact of the \gls{SMC} settings on \software{netabc}'s
accuracy, we ran \software{netabc} twice on the same simulated transmission
tree. For the first run, the \gls{SMC} settings were the same as in the other
simulations: 1000 particles, 5 simulated transmission trees per particle, and
\gls{alphaess} = 0.95. The second run was performed with 2000 particles, 10
simulated transmission trees per particle, and \gls{alphaess} = 0.99. To
investigate the extent to which errors in the estimated \gls{BA} parameters
were due to true features of the posterior, rather than an inaccurate \gls{ABC}
approximation, we performed marginal estimation for one set of parameter
values. Each combination of 1, 2, or 3 model parameters (14 combinations total)
was fixed to their known values, and the remaining parameters were estimated
with \software{netabc}. The parameter values were $\gls{alpha} = 0.0$, $\gls{m}
= 2$, $\gls{I} = 2000$, and $\gls{N} = 5000$.}

\subsection{Results}

\subsubsection*{Classifiers for BA model parameters based on tree shape}



Trees simulated under different values of \gls{alpha} were visibly quite
distinct (\cref{fig:alphatrees}). In particular, higher values of \gls{alpha}
produce networks with a small number of highly connected nodes, which, once
infected, are likely to transmit to many other nodes. This results in a more
unbalanced, ladder-like structure in the phylogeny, compared to networks with
lower \gls{alpha} values. None of the other three parameters produced trees
that were as easily distinguished from each other
(\cref{fig:Itrees,fig:mtrees,fig:Ntrees,fig:Itrees}).  Sackin's index, which
measures tree imbalance, was significantly correlated with all four parameters
    (for \gls{alpha}, \gls{I}, \gls{m}, and \gls{N} respectively: Spearman's rho =
    0.85,
     \ensuremath{-0.12},
     \ensuremath{-0.13},
     0.09;
     $p$-values
     ${<}10^{-5}$,
     $0.003$,
     ${<}10^{-5}$,
     ${<}10^{-5}$).
The ratio of internal to terminal branch lengths was negatively correlated with
\gls{alpha} and \gls{I}, and positively correlated with \gls{m} and \gls{N}
  (Spearman's rho
    \ensuremath{-0.8},
    \ensuremath{-0.69},
    0.09,
    0.17;
all $p < 10^{-5}$).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{kernel-alpha-tree.pdf}
    \caption[
        Simulated transmission trees under three different values of
        preferential attachment power (\gls{alpha}) parameter of \acrshort{BA}
        model.
    ]{
        Simulated transmission trees under three different values of
        preferential attachment power (\gls{alpha}) parameter of \acrshort{BA}
        model. Epidemics were simulated on \gls{BA} networks of 5000 nodes,
        with \gls{alpha} equal to 0.5, 1.0, or 1.5, until 1000 individuals were
        infected. Transmission trees were created by randomly sampling 500
        infected nodes. Higher \gls{alpha} values produced networks with a
        small number of highly-connected nodes, resulting in highly unbalanced,
        ladder-like trees.
    }
  \label{fig:alphatrees}
\end{figure}

\Cref{fig:kpca} shows \gls{kPCA} projections of the simulated trees onto the
first two principal components of the kernel matrix. The figure shows only the
simulations with 500-tip trees and 1000 infected nodes. The three \gls{alpha}
and \gls{I} values considered are well separated from each other in the feature
space \add{mapped to by the tree kernel}. On the other hand, the three \gls{N}
values overlap significantly, and the three \gls{m} values are virtually
indistinguishable. Similar observations can be made for other values of \gls{I}
and the number of tips (\cref{fig:alphakpca,fig:Nkpca,fig:Ikpca,fig:mkpca}).
The values of \gls{I} and \gls{N} separated more clearly with larger numbers of
tips, and in the case of \gls{N}, with larger epidemic sizes
\add{(\cref{fig:Ikpca,fig:Nkpca})}.

\begin{figure}[ht]
    \centering
    \includegraphics{kernel-kpca.pdf}
    \caption[
        Kernel-\gls{PCA} projections of simulated trees under varying \gls{BA}
        parameter values.
    ]{
        Each parameter of the \gls{BA} model was individually varied to produce
        300 simulated trees with 500 tips each. Kernel matrices were formed
        from all pairwise kernel scores among each set of 300 trees. The trees
        were projected onto the first two principal components of the kernel
        matrix calculated using \gls{kPCA}. The other parameters, which were
        not varied, were set to $\gls{alpha} = 1$, $\gls{I} = 1000$, $\gls{m} =
        2$, and $\gls{N} = 5000$. The tree kernel meta-parameters were
        $\gls{lambda} = 0.3$ and $\gls{sigma} = 4$.
  }
  \label{fig:kpca}
\end{figure}



















