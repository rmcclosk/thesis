\subsection{Analysis of \acrlong{BA} model}

<<setup, include=FALSE>>=
    source("global.R")
@

\subsubsection*{Classifiers for BA model parameters based on tree shape}

<<treestats, include=FALSE>>=
    params <- c("alpha", "I", "m", "N")

    get.treestats <- function (param) {
        stats <- paste0("../../simulations/kernel-", param, "/statistics/*")
        d <- collect.data(stats)
        trees <- paste0("../../simulations/kernel-", param, "/tree/*")
        m <- collect.metadata(trees)
        rownames(m) <- sub("../../simulations/", "", rownames(m))
        d <- merge(d, m, by=0, suffixes=c("", ".1"))
        if (param == "alpha") {
            d <- subset(d, m == 2)
            colnames(d)[colnames(d) == "nsimnode"] <- "I"
        }
        setDT(d)
    }
    
    alpha.stats <- get.treestats("alpha")
    I.stats <- get.treestats("I")
    m.stats <- get.treestats("m")
    N.stats <- get.treestats("N")

    alpha.sackin.cor <- alpha.stats[,cor.test(alpha, sackin, method="spearman")]
    I.sackin.cor <- I.stats[,cor.test(I, sackin, method="spearman")]
    m.sackin.cor <- m.stats[,cor.test(m, sackin, method="spearman")]
    N.sackin.cor <- N.stats[,cor.test(N, sackin, method="spearman")]
    stopifnot(alpha.sackin.cor$p.value < 1e-5)

    alpha.ratio.cor <- alpha.stats[,cor.test(int.tip.ratio, alpha)]
    I.ratio.cor <- I.stats[,cor.test(int.tip.ratio, I)]
    m.ratio.cor <- m.stats[,cor.test(int.tip.ratio, m)]
    N.ratio.cor <- N.stats[,cor.test(int.tip.ratio, N)]
    stopifnot(alpha.ratio.cor$p.value < 1e-5)
    stopifnot(I.ratio.cor$p.value < 1e-5)
    stopifnot(m.ratio.cor$p.value < 1e-5)
    stopifnot(N.ratio.cor$p.value < 1e-5)
@

Trees simulated under different values of \gls{alpha} were visibly quite
distinct (\cref{fig:alphatrees}). In particular, higher values of \gls{alpha}
produce networks with a small number of highly connected nodes which, once
infected, are likely to transmit to many other nodes. This results in a more
unbalanced, ladder-like structure in the phylogeny, compared to networks with
lower \gls{alpha} values. None of the other three parameters produced trees
which were as easily distinguished from each other
(\cref{fig:Itrees,fig:mtrees,fig:Ntrees,fig:Itrees}).  Sackin's index, which
measures tree imbalance, was significantly correlated with all four parameters
    (for $\alpha$, $I$, $m$, and $N$ respectively: Spearman's rho =
     \Sexpr{round(alpha.sackin.cor$estimate, 2)},
     \Sexpr{round(I.sackin.cor$estimate, 2)},
     \Sexpr{round(m.sackin.cor$estimate, 2)},
     \Sexpr{round(N.sackin.cor$estimate, 2)};
     $p$-values
     $\Sexpr{pp(alpha.sackin.cor$p.value)}$,
     $\Sexpr{pp(I.sackin.cor$p.value)}$,
     $\Sexpr{pp(m.sackin.cor$p.value)}$,
     $\Sexpr{pp(N.sackin.cor$p.value)}$)
The ratio of internal to terminal branch lengths was negatively correlated with
\gls{alpha} and \gls{I}, and positively corelated with \gls{N} and \gls{m}
  (Spearman's rho
    \Sexpr{round(alpha.ratio.cor$estimate, 2)},
    \Sexpr{round(I.ratio.cor$estimate, 2)},
    \Sexpr{round(m.ratio.cor$estimate, 2)},
    \Sexpr{round(N.ratio.cor$estimate, 2)};
  all $p < 10^{-5}$).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{kernel-alpha-tree.pdf}
  \caption[Simulated transmission trees under three different values of BA parameter $\alpha$]{
    Simulated transmission trees under three different values of BA parameter
    $\alpha$. Epidemics were simulated on \gls{BA} networks of 5000 nodes, with
    \gls{alpha} equal to 0.5, 1.0, or 1.5, until 1000 individuals were
    infected. Transmission trees were created by sampling 500 infected nodes.
    Higher \gls{alpha} values produced networks with a small number of
    highly-connected nodes, resulting in highly unbalanced, ladder-like trees.
  }
  \label{fig:alphatrees}
\end{figure}

\Cref{fig:kpca} shows \gls{kPCA} projections of the simulated trees onto the
first two principal components of the kernel matrix. The figure shows only the
simulations with 500-tip trees and 1000 infected nodes. The three \gls{alpha}
and \gls{I} values considered are well separated from each other in feature
space. On the other hand, the three \gls{N} values overlap significantly, and
the three \gls{m} values are virtually indistinguishable. Similar observations
can be made for other values of \gls{I} and the number of tips
(\cref{fig:alphakpca,fig:Nkpca,fig:Ikpca,fig:mkpca}). The values of \gls{I} and
\gls{N} separated more clearly with larger numbers of tips, and in the case of
\gls{N}, larger epidemic sizes.

\begin{figure}[ht]
  \centering
  \includegraphics{kernel-kpca.pdf}
  \caption[Kernel-PCA projections of simulated trees under varying BA
           parameter values.]{
    Each parameter of the \gls{BA} model was individually varied to produce 300
    simulated trees. Kernel matrices were formed from all pairwise kernel
    scores among each set of 300 trees. The trees were projected onto the first
    two principal components of the kernel matrix calculated using \gls{kPCA}.
    All trees had 500 tips. The parameters not being varied were set to
    \gls{alpha} = 1, \gls{I} = 1000, \gls{m} = 2, and \gls{N} = 5000. The tree
    kernel meta-parameters were $\lambda = 0.3$ and $\sigma = 4$.
  }
  \label{fig:kpca}
\end{figure}

<<classifiers, include=FALSE>>=
    get.kernel.data <- function (param, step) {
        md <- collect.metadata(paste0('../../simulations/kernel-', param, '/', step, '/*'))
        if ('rbf_variance' %in% colnames(md)) {
          md <- subset(md, rbf_variance == 4 & decay_factor == 0.3)
        }
        if (param == 'alpha') {
            md <- subset(md, m == 2)
        }
        d <- setDT(collect.data(rownames(md)))
        if (param %in% c('alpha', 'm')) {
            d[,list(rsquared=mean(rsquared)), by=c('nsimnode', 'ntip')]
        } else if (param == 'I') {
            d[,list(rsquared=mean(rsquared)), by=ntip]
        } else {
            d[,list(rsquared=mean(rsquared)), by=c('I', 'ntip')]
        }
    }
    kernel.alpha <- get.kernel.data('alpha' ,'classifier')
    kernel.m <- get.kernel.data('m', 'classifier')
    kernel.I <- get.kernel.data('I', 'classifier')
    kernel.N <- get.kernel.data('N', 'classifier')

    sackin.alpha <- get.kernel.data('alpha', 'stats-classifier')
    sackin.m <- get.kernel.data('m', 'stats-classifier')
    sackin.I <- get.kernel.data('I', 'stats-classifier')
    sackin.N <- get.kernel.data('N', 'stats-classifier')

    nltt.alpha <- get.kernel.data('alpha', 'nltt-classifier')
    nltt.m <- get.kernel.data('m', 'nltt-classifier')
    nltt.I <- get.kernel.data('I', 'nltt-classifier')
    nltt.N <- get.kernel.data('N', 'nltt-classifier')
@

Accuracy of the \gls{kSVR} classifiers varied based on the parameter being
tested (\cref{fig:rsquared}, left). Classifiers based on two other tree
statistics, the \gls{nltt} and Sackin's index, generally exhibited worse
performance than the tree kernel, although the magnitude of the disparity
varied between the parameters (\cref{fig:rsquared}, centre and right). The
results were largely robust to variations in the tree kernel meta-parameters
$\lambda$ and $\sigma$, although accuracy varied between different epidemic and
sampling scenarios
(\cref{fig:alphacrossv,fig:mcrossv,fig:Icrossv,fig:Ncrossv}).

When classifying $\alpha$, the \gls{kSVR} classifier had an average $R^2$ of 
    \Sexpr{kernel.alpha[,round(mean(rsquared), 2)]},
compared to 
    \Sexpr{nltt.alpha[,round(mean(rsquared), 2)]}
for the \gls{nltt}-based SVR, and
    \Sexpr{sackin.alpha[,round(mean(rsquared), 2)]}
for the linear regression against Sackin's index. There was little variation
about the mean for different tree and epidemic sizes. No classifier could
accurately identify the $m$ parameter in any epidemic scenario, with average
$R^2$ values of 
  \Sexpr{kernel.m[,round(mean(rsquared), 2)]} for \gls{kSVR},
  \Sexpr{nltt.m[,round(mean(rsquared), 2)]} for the \gls{nltt}, and
  \Sexpr{sackin.m[,round(mean(rsquared), 2)]}
for Sackin's index. Again, there was little variation in accuracy between
epidemic scenarios, although the accuracy of the \gls{kSVR} was slightly higher
on 1000-tip trees 
    (average $R^2$ 
     \Sexpr{kernel.m[ntip == 100,round(mean(rsquared), 2)]},
     \Sexpr{kernel.m[ntip == 500,round(mean(rsquared), 2)]},
     \Sexpr{kernel.m[ntip == 1000,round(mean(rsquared), 2)]}
     for 100, 500, and 1000 tips respectively).

The accuracy of classifiers $I$ varied significantly with the number of tips in
the tree. For 100-tip trees, the average $R^2$ values were
  \Sexpr{kernel.I[ntip == 100, round(mean(rsquared), 2)]},
  \Sexpr{nltt.I[ntip == 100, round(mean(rsquared), 2)]}, and
  \Sexpr{sackin.I[ntip == 100, round(mean(rsquared), 2)]}
for the tree kernel, \gls{nltt}, and Sackin's index respectively. For 500-tip
trees, the values increased to
  \Sexpr{kernel.I[ntip == 500, round(mean(rsquared), 2)]},
  \Sexpr{nltt.I[ntip == 500, round(mean(rsquared), 2)]}, and
  \Sexpr{sackin.I[ntip == 500, round(mean(rsquared), 2)]}.
Finally, the performance of classifiers for $N$ depended heavily on the
epidemic scenario. The $R^2$ of the \gls{kSVR} classifier ranged from
  \Sexpr{kernel.N[,round(min(rsquared), 2)]}
for the smallest epidemic and smallest sample size, to
  \Sexpr{kernel.N[,round(max(rsquared), 2)]}
for the largest. Likewise, $R^2$ for the \gls{nltt}-based SVR ranged from 
  \Sexpr{nltt.N[,round(min(rsquared), 2)]}
to
  \Sexpr{nltt.N[,round(max(rsquared), 2)]}.
Sackin's index did not accurately classify $N$ in any scenario, with an average
$R^2$ of
  \Sexpr{sackin.N[,round(mean(rsquared), 2)]}
and little variation between scenarios.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{kernel-rsquared.pdf}
  \caption[Cross-validation accuracy of kernel-SVR, nLTT-based SVR, and
  Sackin's index regression classifiers for BA model parameters.]{
      Cross-validation accuracy of kernel-SVR classifier (left), SVR classifier
      using \gls{nltt} (centre), and linear regression using Sackin's index
      (right) for \gls{BA} model parameters. Kernel meta-parameters were set to
      $\lambda = 0.3$ and $\sigma = 4$. Each point was calculated based on 300
      simulated transmission trees over networks with three different values of
      the parameter being tested. Vertical lines are empirical 95\% confidence
      intervals based on 1000 two-fold cross-validations.
  }
  \label{fig:rsquared}
\end{figure}

\subsubsection*{Marginal parameter estimates with grid search}

<<gridsearch, include=FALSE>>=
    d <- setDT(collect.data("../../simulations/aggregates/gridsearch/*"))
    d[,error := abs(true_value - point.est)]
    setkey(d, parameter)

    # correlation between number of tips and error
    tip.cor <- d[,cor.test(error, tips, method="spearman")[c("p.value", "estimate")], by=parameter]
    setkey(tip.cor, parameter)
    stopifnot(tip.cor["alpha", p.value] < 0.05)
    stopifnot(tip.cor["I", p.value] < 0.05)
    stopifnot(tip.cor["N", p.value] < 0.05)
    stopifnot(tip.cor["m", p.value] > 0.05)

    # CI takes up >75% of grid
    stopifnot(d["alpha", all(upper - lower >= 0.75*2)])
    stopifnot(d["I", all(upper - lower >= 0.75*5000)])
    stopifnot(d["N", all(upper - lower >= 0.75*15000)])
    stopifnot(d["m", all(upper - lower >= 0.75*6)])

    # correlation between value and error
    av <- d[,list(p=summary(aov(error~factor(true_value)))[[1]][["Pr(>F)"]][1]),
            by=parameter]
    stopifnot(av["alpha", p] < 0.05)
    stopifnot(av["I", p] < 0.05)
    stopifnot(av["m", p] < 0.05)
    stopifnot(av["N", p] < 0.05)

    alpha.test <- d["alpha", wilcox.test(.SD[true_value %in% c(1.0, 1.25),error], 
                                         .SD[!true_value %in% c(1.0, 1.25),error])]
    stopifnot(alpha.test$p.value < 0.05)

    I.test <- d["I", wilcox.test(.SD[true_value >= 2000 & true_value <= 3000, error], 
                                 .SD[true_value < 2000 | true_value > 3000, error])]
    stopifnot(I.test$p.value < 0.05)

    m.test <- d["m", wilcox.test(.SD[true_value == 1, error], 
                                 .SD[true_value > 1, error])]
    stopifnot(m.test$p.value < 0.05)

    N.test <- d["N", wilcox.test(.SD[true_value <= 3000, error], 
                                 .SD[true_value > 3000, error])]
    stopifnot(N.test$p.value < 0.05)

    value.cor <- d[,cor.test(error, true_value)[c("p.value", "estimate")], by=parameter]
    setkey(value.cor, parameter)
    stopifnot(value.cor["I", p.value] >= 0.05)
@

The accuracy of grid search estimates largely paralleled that of the \gls{kSVR}
classifiers. \Cref{fig:gridest} shows point estimates and 95\% highest density
intervals for each of the \gls{BA} parameters, for one replicate experiment
with 500-tip trees. Plots showing the point estimates for all replicates can be
found in \cref{fig:gridptalpha,fig:gridptI,fig:gridptm,fig:gridptN}. For all
parameters except $m$, the error of point estimates was negatively correlated
with the number of sampled tips in the tree (for
\gls{alpha}, \gls{I}, and \gls{N} respectively: Spearman's $\rho$ = 
    \Sexpr{tip.cor["alpha", round(estimate, 2)]},
    \Sexpr{tip.cor["I", round(estimate, 2)]},
    \Sexpr{tip.cor["N", round(estimate, 2)]};
$p$-values
    $\Sexpr{tip.cor["alpha", pp(p.value)]}$,
    $\Sexpr{tip.cor["I", pp(p.value)]}$,
    $\Sexpr{tip.cor["N", pp(p.value)]}$).
The highest density intervals obtained for all parameters were extremely wide,
occcupying $>$75\% of the grid in all cases (\cref{fig:gridest}).

The \gls{alpha} parameter was the most accurately estimated, with point
estimates having an average deviation of 
    \Sexpr{d["alpha", round(mean(error), 2)]}
from the true value, on a grid from 0 to 2. The error of point estimates varied
significantly between true values of \gls{alpha}
    (one-way \gls{ANOVA}, $p \Sexpr{pp(av["alpha", p], eq=TRUE)}$). In
particular, errors were lower for the values \gls{alpha} = 1.0 and 1.25 than
for the other values
    (average errors 
    \Sexpr{d["alpha", .SD[true_value %in% c(1.0, 1.25), round(mean(error), 2)]]}
    for \gls{alpha} = 1.0 or 1.5 vs.
    \Sexpr{d["alpha", .SD[!true_value %in% c(1.0, 1.25), round(mean(error), 2)]]}
    for \gls{alpha} $\neq$ 1.0 or 1.5),
and this difference was significant
    (Wilcoxon rank-sum test, $p \Sexpr{pp(alpha.test$p.value, eq=TRUE)}$,
     \cref{fig:gridptalpha}).
These two values exhibited different qualitative behaviour than the other
values in terms of the distribution of kernel scores along the grid
(\cref{fig:gridalpha}). In particular, there was a pronounced peak in scores
around the true value, in contrast to the other values where the scores were
flat around the true value. The effect was most obvious for the value
\gls{alpha} = 1.25.

The average absolute error of the point estimates for \gls{I} was 
    \Sexpr{d["I", round(mean(error))]} individuals,
on a grid of 500 to 5000, and these errors differed between true values of
\gls{I}
    (one-way \gls{ANOVA}, $p \Sexpr{pp(av["I", p], eq=TRUE)}$).
The errors for $2000 \leq I \leq 3000$ were higher than those for the other
values
    (average errors
     \Sexpr{d["I", .SD[true_value >= 2000 & true_value <= 3000, round(mean(error))]]}
     for $2000 \leq I \leq 3000$ vs.
     \Sexpr{d["I", .SD[true_value < 2000 | true_value > 3000, round(mean(error))]]}
     for $I < 2000$ or $I > 3000$),
and this difference was significant
    (Wilcoxon rank-sum test, $p \Sexpr{pp(I.test$p.value, eq=TRUE)}$,
     \cref{fig:gridptI}).
Kernel score distributions for all test values exhibited a similar rounded
shape (\cref{fig:gridI}). 

The average error for \gls{m} was
    \Sexpr{d["m", round(mean(error), 2)]} edges per vertex,
on a grid from 1 to 6. The error varied significantly between the true values
of \gls{m} 
    (one-way \gls{ANOVA}, $p \Sexpr{pp(av["m", p], eq=TRUE)}$).
Errors for the value \gls{m} = 1 were lower than the other values
    (average errors
    \Sexpr{d["m", .SD[true_value == 1, round(mean(error), 2)]]}
    for $m = 1$ vs.
    \Sexpr{d["m", .SD[true_value > 1, round(mean(error), 2)]]}
    for $m > 1$),
and this difference was significant
    (Wilcoxon rank-sum test, $p \Sexpr{pp(m.test$p.value, eq=TRUE)}$,
     \cref{fig:gridptm}).
The value $m = 1$ causes the network to take on a distinct shape relative to
higher \gls{m} values, namely a tree (\ie there are no cycles, see
\cref{subsec:treeshape}). The kernel score distribution had a peak at $m = 1$
when this was the true value, and a valley at $m = 1$ when the true value of
$m$ was greater that 1 (\cref{fig:gridm}).

The average error for \gls{N} was 
    \Sexpr{d[parameter == "N", round(mean(error))]} individuals,
on a grid from 1000 to 15000, and was varied significantly with the true value
of \gls{N}
    (one-way \gls{ANOVA}, $p \Sexpr{pp(av["N", p], eq=TRUE)}$).
The errors were lower for $N \leq 3000$
    (average errors
     \Sexpr{d["N", .SD[true_value <= 3000, round(mean(error))]]}
     for $N \leq 3000$ vs.
     \Sexpr{d["N", .SD[true_value > 3000, round(mean(error))]]}
     for $N > 3000$),
and this diffrence was significant
    (Wilcoxon rank-sum test, $p \Sexpr{pp(N.test$p.value, eq=TRUE)}$,
     \cref{fig:gridptN}).
The kernel score distribution had a peak at $N = 1000$ when this was the true
value, and a valley there otherwise (\cref{fig:gridN}). Except for this valley,
the distributions were flat for $N > 3000$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{gridsearch-example}
  \caption[Grid search estimates of \gls{BA} model parameters.]{Point estimates
      and 95\% highest density intervals for each \gls{BA} model parameter,
      obtained using grid search. Networks and transmission trees were
      simulated over a grid of values for each parameter while holding the
      others fixed. For a subset of the grid values ($x$-axis), test networks
      and trees were created and compared to each tree on the grid using the
      tree kernel. The kernel scores along the grid were normalized to resemble
      a probability distribution, from which the mode and highest density
      interval were calculated. Shown values correspond to one replicate
      experiment, with trees of size 500.
  } 
  \label{fig:gridest}
\end{figure}

\subsubsection*{Joint parameter estimates with kernel-ABC}

<<point_est, include=FALSE>>=
    f <- Sys.glob("../../simulations/abc-pa-free-m/point-estimate/*")
    d <- fread(f)
    d[,m := floor(m)]
    d[,alpha_error := abs(true_alpha - alpha)]
    d[,N_error := abs(true_N - N)]
    d[,I_error := abs(true_I - I)]
    d[,m_error := abs(true_m - m)]

    alpha.av <- anova(lm(alpha_error ~ factor(true_alpha) + factor(true_m) + factor(true_I), d))
    m.av <- anova(lm(m_error ~ factor(true_alpha) + factor(true_m) + factor(true_I), d))
    N.av <- anova(lm(N_error ~ factor(true_alpha) + factor(true_m) + factor(true_I), d))
    I.av <- anova(lm(I_error ~ factor(true_alpha) + factor(true_m) + factor(true_I), d))

    m.tbl <- d[,prop.table(table(m_error))]
    zero.alpha.test <- wilcox.test(d[true_alpha == 0, alpha_error], 
                                   d[true_alpha != 0, alpha_error])
    alpha.I.test <- wilcox.test(d[true_alpha < 1, I_error],
                                d[true_alpha >= 1, I_error])
    I.I.test <- wilcox.test(d[true_I == 1000, I_error],
                            d[true_I == 2000, I_error])
    alpha.m.test <- wilcox.test(d[true_alpha == 0 | true_alpha == 1, m_error],
                                d[true_alpha == 0.5 | true_alpha == 1.5, m_error])
    stopifnot(alpha.av["factor(true_m)", "Pr(>F)"] <= 0.05)
    stopifnot(alpha.av["factor(true_I)", "Pr(>F)"] > 0.05)
    stopifnot(I.I.test$p.value < 1e-5)
    stopifnot(I.av["factor(true_m)", "Pr(>F)"] > 0.05)
    stopifnot(m.av["factor(true_m)", "Pr(>F)"] > 0.05)
    stopifnot(m.av["factor(true_I)", "Pr(>F)"] > 0.05)
    stopifnot(m.av["factor(true_alpha)", "Pr(>F)"] > 0.05)
    stopifnot(min(N.av[1:3,"Pr(>F)"]) > 0.05)

    alpha.m.cor <- cor.test(d[,alpha_error], d[,true_m], method="spearman")
@

\Cref{fig:abcptm2} shows \gls{MAP} point estimates of the BA model parameters
obtained with kernel-ABC on simulated data. The estimates shown correspond only
to the simulations where the $m$ parameter was set to 2, however the results
for $m = 3$ and $m = 4$ were similar (\cref{fig:abcptm3,fig:abcptm4}). Average
boundaries of 95\% HPD intervals are given in \cref{tab:abchpd}.

The accuracy of the parameter estimates obtained with kernel-ABC
paralleled the results from the \gls{kSVR} classifier. Of the four parameters,
$\alpha$ was the most accurately estimated, with point estimates having a
median [IQR] absolute error of 
    \Sexpr{d[,round(median(alpha_error), 2)]} 
    [\Sexpr{d[,round(quantile(alpha_error, 0.25), 2)]} - 
    \Sexpr{d[,round(quantile(alpha_error, 0.75), 2)]}].
The errors when the true value of $\alpha$ was zero were significantly greater
than those for the other values 
    (Wilcoxon rank-sum test, $p$ = $\Sexpr{latexSN(round(zero.alpha.test$p.value, 4))}$).
Errors in estimating $\alpha$ also varied with the true value of $m$ just at
the threshold of statistical significance
    ($p \Sexpr{pp(alpha.av["factor(true_m)", "Pr(>F)"])}$),
but did not vary across the true values of $N$ or $I$ (both one-way ANOVA).
Estimates for $I$ were relatively accurate, with point estimate errors of
    \Sexpr{d[,round(median(I_error))]} 
    [\Sexpr{d[,round(quantile(I_error, 0.25))]} - 
    \Sexpr{d[,round(quantile(I_error, 0.75))]}] individuals.
These errors were significantly higher when the true value of $\alpha$ was
at least 1
    (Wilcoxon rank-sum test, $p$ = $\Sexpr{latexSN(round(alpha.I.test$p.value, 4))}$)
and when the true value of $I$ was 2000 ($p < 10^{-5}$). The true value of $m$
did not affect the estimates of $I$ (one-way ANOVA).

The $m$ parameter was estimated correctly in only
    \Sexpr{as.integer(m.tbl[1] * 100)} \%
of simulations, barely better than random guessing. The true values of the
other parameters did not significantly affect the estimates of $m$ (both
one-way ANOVA). Finally, the total number of nodes $N$ was consistently
over-estimated by about a factor of two
    (error \Sexpr{d[,round(median(N_error))]} 
    [\Sexpr{d[,round(quantile(N_error, 0.25))]} - 
     \Sexpr{d[,round(quantile(N_error, 0.75))]}] individuals).
No parameters influenced the accuracy of the $N$ estimates (all one-way ANOVA).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{abc-point-estimate-m2}
  \vspace{6pt}
  \caption{
    Point estimates of BA model parameters obtained by running kernel-ABC
    on simulated phylogenies without training, for simulations with $m = 2$.
    Dotted lines indicate true values, and limits of the $y$-axes are regions
    of uniform prior density. (A) Estimates for $\alpha$ and $I$ against their
    true values in simulations. (B) Estimates for $m$ and $N$, which were held
    fixed in these simulations, against true values of $\alpha$.
  }
  \label{fig:abcptm2}
\end{figure}

\begin{table*}[ht]
  \centering
  \input{\tablepath/abc-hpd.tex}
  \caption{
      Average maximum \textit{a posteriori} point estimates and 95\% highest
      posterior density (HPD) interval widths for BA model parameter estimates
      obtained with kernel-ABC. Three transmission trees were simulated under
      each combination of the listed parameter values, and the parameters were
      estimated with kernel-ABC without training.
  }
  \label{tab:abchpd}
\end{table*}

<<supp_labels, echo=FALSE, results="asis">>=
    N <- 5000
    replicate <- 0
    lab <- NULL
    for (m in c(2, 3, 4)) {
    for (I in c(1000, 2000)) {
    for (alpha in c(0, 0.5, 1, 1.5)) {
        lab <- c(lab, sprintf("fig:%.1f-%d-%d-%d-%d", alpha, I, m, N, replicate))
    }
    }
    }
    lab <- paste(lab, collapse = ",")
@

The dispersion of the ABC approximation to the posterior also varied between
the parameters, with narrower HPD intervals for the parameters with more
accurate point estimates (\cref{tab:abchpd}). \Cref{fig:abcex} shows
the distributions for for one simulation. Equivalent plots for one replicate
simulation with each studied parameter combination can be found in
\cref{\Sexpr{lab}}. HPD intervals around $\alpha$ and $I$ were often narrow
relative to the region of nonzero prior density, whereas the intervals for $m$
and $N$ were more widely dispersed.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{abc-posterior-example}
  \vspace{6pt}
  \caption{
    Marginal posterior distributions of BA model parameters estimated
    with kernel-ABC for a single simulated transmission tree. Dotted
    lines and shaded polygon indicate true values.
  }
  \label{fig:abcex}
\end{figure}

<<mixed_peerdriven, include=FALSE>>=
  library(coda)
  d <- fread("bzcat ../../simulations/abc-pa-mixed-alpha/abc/*")
  d <- d[iter == max(iter)]
  d <- d[sample(1:nrow(d), prob=weight, replace=TRUE)]
  params <- c("alpha", "m", "N", "I")
  d <- melt(d, measure.vars=params, variable.name="parameter")
  f <- function (x) {
    dens <- density(x)
    hpd <- HPDinterval(mcmc(x))
    list(map=dens$x[which.max(dens$y)], hpd.min=hpd[,"lower"], 
         hpd.max=hpd[,"upper"])
  }
  d <- d[,f(value), by=parameter]
  setkey(d, parameter)

  m <- collect.metadata("../../simulations/abc-pa-peerdriven/abc/*")
  p <- fread(paste("bzcat", rownames(m)[m$sample_peer == 1]))
  p <- p[iter == max(iter)]
  p <- p[sample(1:nrow(p), prob=weight, replace=TRUE)]
  params <- c("alpha", "m", "N", "I")
  p <- melt(p, measure.vars=params, variable.name="parameter")
  f <- function (x) {
    dens <- density(x)
    hpd <- HPDinterval(mcmc(x))
    list(map=dens$x[which.max(dens$y)], hpd.min=hpd[,"lower"], 
         hpd.max=hpd[,"upper"])
  }
  p <- p[,f(value), by=parameter]
  setkey(p, parameter)
  options(scipen=5)
@

To test the effect of model misspecification, we simulated one network where
the nodes exhibited heterogeneous preferential attachment power (half 0.5, the
other half 1.5), with $m$ = 2, $N$ = 5000, and $I$ = 1000. The MAP [95\%
HPD] estimates for each parameter were: 
$\alpha$, 
  \Sexpr{round(d["alpha", map], 2)} 
  [\Sexpr{round(d["alpha", hpd.min], 2)} -
   \Sexpr{round(d["alpha", hpd.max], 2)}];
$I$,
  \Sexpr{d["I", round(map)]} 
  [\Sexpr{d["I", round(hpd.min)]} -
   \Sexpr{d["I", round(hpd.max)]}];
$m$,
  \Sexpr{d["m", floor(map)]} 
  [\Sexpr{d["m", floor(hpd.min)]} -
   \Sexpr{d["m", floor(hpd.max)]}];
$N$,
  \Sexpr{d["N", round(map)]} 
  [\Sexpr{d["N", round(hpd.min)]}-
   \Sexpr{d["N", round(hpd.max)]}].
The approximate posterior distributions for this simulation are shown in
\cref{fig:mixed}. To test the effect of sampling bias, we sampled one
transmission tree in a peer-driven fashion, where the probability to sample a
node was twice as high if one of its peers had already been sampled. The
parameters for this experiment were $N$ = 5000, $m$ = 2, $\alpha$ = 0.5, and
$I$ = 2000. The estimated values were
$\alpha$, 
  \Sexpr{p["alpha", round(map, 2)]} 
  [\Sexpr{p["alpha", round(hpd.min, 2)]} -
   \Sexpr{p["alpha", round(hpd.max, 2)]}];
$I$,
  \Sexpr{p["I", floor(map)]} 
  [\Sexpr{p["I", floor(hpd.min)]} -
   \Sexpr{p["I", floor(hpd.max)]}];
$m$,
  \Sexpr{p["m", floor(map)]} 
  [\Sexpr{p["m", floor(hpd.min)]} -
   \Sexpr{p["m", floor(hpd.max)]}];
$N$,
  \Sexpr{p["N", floor(map)]} 
  [\Sexpr{p["N", floor(hpd.min)]} -
   \Sexpr{p["N", floor(hpd.max)]}].
The approximate posterior distributions are shown in \cref{fig:peerdriven}. Both
of these results were in line with estimates obtained on other simulated
datasets (\cref{tab:abchpd}), although the estimate of peer-driven sampling for
$\alpha$ was somewhat lower than typical.

%<<abc_glm, include=FALSE>>=
%    source("global.R")
%    options(scipen=-1, digits=2)
%@
%
%<<alpha_glm, include=FALSE>>=
%    # alpha_error is influenced by alpha and m but not I
%    stopifnot(alpha.glm[Parameter == "alpha", min(p) < 0.05])
%    stopifnot(alpha.glm[Parameter == "m", min(p) < 0.05])
%    stopifnot(alpha.glm[Parameter == "I", min(p) > 0.05])
%
%    # alpha_error is correlated with true_alpha
%    alpha.test <- d[,cor.test(alpha_error, true_alpha, method="spearman")]
%    stopifnot(alpha.test$p.value < 0.05)
%
%    # alpha_error is not correlated with true_m
%    alpha.m.cor <- d[,cor.test(alpha_error, true_m)]
%    stopifnot(alpha.m.cor$p.value < 0.05)
%@
%
%We used \software{netabc} to estimate the parameters of the \gls{BA} model on
%simulated trees where the true parameter values were known. Point estimates for
%each parameter are shown in \cref{fig:abcptm2} for the simulations with \gls{m} =
%2. The results for the other values of \gls{m} were similar
%(\cref{fig:abcptm3,fig:abcptm4}). The median [IQR] absolute error of estimates
%of \gls{alpha} across all simulations was
%    \Sexpr{d[,median(alpha_error)]} 
%    [\Sexpr{d[,quantile(alpha_error, 0.25)]}-\Sexpr{d[,quantile(alpha_error, 0.75)]}].
%\Gls{GLM} analysis indicated that the true values of both \gls{alpha} and
%\gls{m} had significant effects on the error in estimated \gls{alpha}
%    ($p$ values $\Sexpr{pp(alpha.glm[Parameter == "alpha", min(p)], eq=FALSE)}$
%     $\Sexpr{pp(alpha.glm[Parameter == "m", min(p)], eq=FALSE)}$,
%     \cref{tab:glmalpha}),
%but the true value of \gls{I} did not. There was a significant negative
%correlation between the true value of \gls{alpha} and the error
%    (Spearman's $\rho$ = \Sexpr{round(alpha.test$estimate, 2)},
%     $p \Sexpr{pp(alpha.test$p.value, eq=TRUE)}$).
%There was a negative correlation between \gls{m} and the error in \gls{alpha}
%    (Spearman's $\rho$ = \Sexpr{round(alpha.m.cor$estimate, 2)},
%    $p \Sexpr{pp(alpha.m.cor$p.value, eq=FALSE)}$).
%
%<<I_glm, include=FALSE>>=
%    # I_error is influenced by alpha and I but not by m
%    stopifnot(I.glm[Parameter == "alpha", min(p) < 0.05])
%    stopifnot(I.glm[Parameter == "m", min(p) > 0.05])
%    stopifnot(I.glm[Parameter == "I", min(p) < 0.05])
%
%    # correlation between I_error and true_alpha
%    I.alpha.cor <- d[,cor.test(I_error, true_alpha, method="spearman")]
%    stopifnot(I.alpha.cor$p.value < 0.05)
%
%    # correlation between I_error and true_I
%    I.I.cor <- d[,cor.test(I_error, true_I, method="spearman")]
%    stopifnot(I.alpha.cor$p.value < 0.05)
%@
%
%The mean error in the estimated value of \gls{I} was
%    \Sexpr{d[,round(median(I_error))]} 
%    [\Sexpr{d[,round(quantile(I_error, 0.25))]}-\Sexpr{d[,round(quantile(I_error, 0.75))]}],
%an over-estimate of roughly a factor of 1.5
%(\cref{fig:abcptm2,fig:abcptm1,fig:abcptm3,fig:abcptm4}). \Gls{GLM} analysis
%indicated a relationship between the error in estimated \gls{I}, and the true
%values of \gls{alpha} and \gls{I}
%    ($p$ values $\Sexpr{pp(I.glm[Parameter == "alpha", min(p)], eq=FALSE)}$
%     $\Sexpr{pp(alpha.glm[Parameter == "I", min(p)], eq=FALSE)}$,
%     \cref{tab:glmalpha}),
%but not the true value of \gls{m}. There was a significant correlation between 
%the true value of \gls{alpha} and the error in estimated \gls{I}
%    (Spearman's $\rho$ = \Sexpr{I.alpha.cor$estimate},
%     $p \Sexpr{pp(I.alpha.cor$p.value, eq=TRUE)}$),
%and between the true value of \gls{I} and the error in estimated \gls{I}
%    (Spearman's $\rho$ = \Sexpr{I.I.cor$estimate},
%     $p \Sexpr{pp(I.I.cor$p.value, eq=TRUE)}$).
%
%<<m_glm, include=FALSE>>=
%    # effects of parameters on m_error
%    stopifnot(m.glm[Parameter == "m", min(p)] < 0.05)
%    stopifnot(m.glm[Parameter == "alpha", min(p)] > 0.05)
%    stopifnot(m.glm[Parameter == "I", min(p)] > 0.05)
%
%    # m was only estimated correctly in just over 20\% of simulations with m > 2
%    m.tbl <- prop.table(table(d[true_m > 1,m_error]))
%    stopifnot(m.tbl[1] > 0.2 & m.tbl[1] < 0.3)
%
%    # with m = 1, over 95% of simulations are right
%    m1.est <- d[true_m == 1, m_error]
%    stopifnot(sum(m1.est == 0) / length(m1.est) > 0.95)
%    stopifnot(sum(m1.est == 0) / length(m1.est) < 1)
%@
%
%\gls{GLM} analysis showed an effect of the true value of \gls{m} on the error
%in the estimated \gls{m}
%    ($p \Sexpr{pp(m.glm[Parameter == "m", min(p)])}$,
%     \cref{tab:glmm}).
%When the true value of \gls{m} was 1, the correct value was recovered by
%\software{netabc} in virtually every case
%    (\Sexpr{sum(m1.est == 0)} out of \Sexpr{length(m1.est)} simulations).
%However, when the true value of \gls{m} was 2 or higher, the correct value was
%recovered in only 
%    \Sexpr{as.integer(m.tbl[1] * 100)}\%
%of simulations, little better than random guessing. The \gls{GLM} analysis did
%not indicate any effects of the true parameter values on the error in estimated
%\gls{m} (\cref{tab:glmm}).
%
%<<N_glm, include=FALSE>>=
%    # alpha_error is influenced by alpha and m but not I
%    stopifnot(N.glm[Parameter == "alpha", min(p) < 0.05])
%    stopifnot(N.glm[Parameter == "I", min(p) < 0.05])
%    stopifnot(N.glm[Parameter == "m", min(p) > 0.05])
%
%    N.alpha.test <- d[,wilcox.test(.SD[true_alpha == 1.5, N_error], 
%                                   .SD[true_alpha < 1.5, N_error])]
%    stopifnot(N.alpha.test$p.value < 0.05)
%    N.I.test <- d[,wilcox.test(.SD[true_I == 3000, N_error], 
%                               .SD[true_I < 3000, N_error])]
%    stopifnot(N.I.test$p.value < 0.05)
%@
%
%Finally, the total number of nodes \gls{N} was consistently over-estimated by
%about a factor of two
%    (error \Sexpr{d[,format(median(N_error), scientific=FALSE)]} 
%    [\Sexpr{d[,format(quantile(N_error, 0.25), scientific=FALSE)]} - 
%     \Sexpr{d[,format(quantile(N_error, 0.75), scientific=FALSE)]}]).
%The fitted \gls{GLM} indicated that the true values of both \gls{alpha} and
%\gls{I} had an effect on the error in the estimated \gls{N}
%    ($p$-values $\Sexpr{pp(N.glm[Parameter == "alpha", min(p)], eq=FALSE)}$ and 
%     $\Sexpr{pp(N.glm[Parameter == "I", min(p)], eq=FALSE)}$,
%     \cref{tab:glmN}),
%but that the true value of \gls{m} did not. The error in the estimated \gls{N}
%when \gls{alpha} was equal to 1.5 was slightly lower than for other values of
%\gls{alpha}
%    (median [IQR] error rates 
%     \Sexpr{d[true_alpha == 1.5, format(median(N_error), scientific=FALSE)]} 
%    [\Sexpr{d[true_alpha == 1.5, format(quantile(N_error, 0.25), scientific=FALSE)]} - 
%     \Sexpr{d[true_alpha == 1.5, format(quantile(N_error, 0.75), scientific=FALSE)]}]
%     for \gls{alpha} = 1.5 vs. 
%     \Sexpr{d[true_alpha < 1.5, format(median(N_error), scientific=FALSE)]} 
%    [\Sexpr{d[true_alpha < 1.5, format(quantile(N_error, 0.25), scientific=FALSE)]} - 
%     \Sexpr{d[true_alpha < 1.5, format(quantile(N_error, 0.75), scientific=FALSE)]}]
%     for \gls{alpha} < 1.5),
%and this difference was statistically significant
%    (Wilcoxon rank-sum test, $p \Sexpr{pp(N.alpha.test$p.value, eq=TRUE)}$).
%Similarly, the error in the estimated \gls{N} when \gls{I} was equal to 3000 was
%lower than for other values of \gls{I}
%    (median [IQR] error rates 
%     \Sexpr{d[true_I == 3000, format(median(N_error), scientific=FALSE)]} 
%    [\Sexpr{d[true_I == 3000, format(quantile(N_error, 0.25), scientific=FALSE)]} - 
%     \Sexpr{d[true_I == 3000, format(quantile(N_error, 0.75), scientific=FALSE)]}]
%     for \gls{I} = 3000 vs. 
%     \Sexpr{d[true_alpha < 3000, format(median(N_error), scientific=FALSE)]} 
%    [\Sexpr{d[true_alpha < 3000, format(quantile(N_error, 0.25), scientific=FALSE)]} - 
%     \Sexpr{d[true_alpha < 3000, format(quantile(N_error, 0.75), scientific=FALSE)]}]
%     for \gls{I} < 3000),
%and this difference was statistically significant
%    (Wilcoxon rank-sum test, $p \Sexpr{pp(N.I.test$p.value, eq=TRUE)}$).
%
%\begin{figure}
%  \includegraphics{abc-point-estimate-m2}
%  \caption[\Acrlong{MAP} point estimates for \gls{BA} model parameters obtained
%    by running \software{netabc} on simulated data, for simulations with $m = 2$.] 
%  {
%    \Acrlong{MAP} point estimates for \gls{BA} model parameters obtained by         
%    running \software{netabc} on simulated data. Values shown are for               
%    simulations with \gls{m} = 2. Dashed lines indicate true values. (A)            
%    Estimates of \gls{alpha} and \gls{I} which were varied in these simulations  
%    against known values. (B) Estimates of \gls{m} and \gls{N} which were held   
%    fixed in these simulations at the values \gls{m} = 2 and \gls{N} = 5000. 
%  }
%  \label{fig:abcptm2}
%\end{figure}
%
%<<posterior_sims, echo=FALSE>>=
%    N <- 5000
%    replicate <- 0
%    lab <- NULL
%    for (m in c(2, 3, 4)) {
%    for (I in c(1000, 2000)) {
%    for (alpha in c(0, 0.5, 1, 1.5)) {
%        lab <- c(lab, sprintf("fig:%.1f-%d-%d-%d-%d", alpha, I, m, N, replicate))
%    }
%    }
%    }
%    lab <- paste(lab, collapse=",")
%@
%
%\Cref{fig:abcex} shows the \gls{ABC} approximation to the posterior
%distribution on the \gls{BA} parameters for one simulation. Equivalent plots
%for one replicate simulation with each combination of parameters can be found
%in \cref{\Sexpr{lab}}. \Gls{HPD} intervals around \gls{alpha} and \gls{I} were
%narrow relative to the region of nonzero prior density, whereas the intervals
%for $m$ and \gls{N} were widely dispersed. \Cref{tab:abchpd} shows point
%estimates and 95\% \gls{HPD} intervals averaged over all simulations.
%
%\begin{figure}
%  \includegraphics{{abc-posterior/1.0_1000_2_5000_0}.pdf}
%  \caption[Approximate marginal posterior distributions of BA model parameters
%      obtained by applying \textit{netabc} to a simulated transmission tree
%      with values $\alpha$ = 1.0, $I$ = 1000, $m$ = 2, and $N$ = 5000.]
%    {
%        Approximate marginal posterior distributions of BA model parameters
%        obtained by applying \textit{netabc} to a simulated transmission tree
%        with BA parameter values $\alpha$ = 1.0, $I$ = 1000, $m$ = 2, and $N$ =
%        5000. Vertical dashed lines indicate true values. Shaded areas are 95\%
%        highest posterior density intervals. $x$-axes indicate regions of
%        nonzero prior density.
%    }
%  \label{fig:abcex}
%\end{figure}
%
%
%\begin{table}
%    \centering
%    \input{\tablepath/abc-hpd}
%    \caption[
%        Maximum \textit{a priori} estimates and 95\% highest posterior density
%        (HPD) interval boundaries for \gls{BA} model parameters estimated with
%        \software{netabc}, averaged over simulated transmission trees.
%    ]
%    {
%        Maximum \textit{a priori} estimates and 95\% highest posterior density
%        (HPD) interval boundaries for \gls{BA} model parameters estimated with
%        \software{netabc}, averaged over simulated transmission trees.
%    }
%    \label{tab:abchpd}
%\end{table}
%
%<<mixed_peerdriven, include=FALSE>>=
%  library(coda)
%  d <- fread("bzcat ../../simulations/abc-pa-mixed-alpha/abc/*")
%  d <- d[iter == max(iter)]
%  d <- d[sample(1:nrow(d), prob=weight, replace=TRUE)]
%  params <- c("alpha", "m", "N", "I")
%  d <- melt(d, measure.vars=params, variable.name="parameter")
%  f <- function (x) {
%    dens <- density(x)
%    hpd <- HPDinterval(mcmc(x))
%    list(map=dens$x[which.max(dens$y)], hpd.min=hpd[,"lower"], 
%         hpd.max=hpd[,"upper"])
%  }
%  d <- d[,f(value), by=parameter]
%  setkey(d, parameter)
%
%  m <- collect.metadata("../../simulations/abc-pa-peerdriven/abc/*")
%  p <- fread(paste("bzcat", rownames(m)[m$sample_peer == 1]))
%  p <- p[iter == max(iter)]
%  p <- p[sample(1:nrow(p), prob=weight, replace=TRUE)]
%  params <- c("alpha", "m", "N", "I")
%  p <- melt(p, measure.vars=params, variable.name="parameter")
%  f <- function (x) {
%    dens <- density(x)
%    hpd <- HPDinterval(mcmc(x))
%    list(map=dens$x[which.max(dens$y)], hpd.min=hpd[,"lower"], 
%         hpd.max=hpd[,"upper"])
%  }
%  p <- p[,f(value), by=parameter]
%  setkey(p, parameter)
%  options(scipen=5)
%@
%
%To test the effect of model misspecification, we simulated one network where
%the nodes exhibited heterogeneous preferential attachment power (half 0.5, the
%other half 1.5), with $m$ = 2, $N$ = 5000, and $I$ = 1000. The MAP [95\%
%HPD] estimates for each parameter were: 
%$\alpha$, 
%  \Sexpr{round(d["alpha", map], 2)} 
%  [\Sexpr{round(d["alpha", hpd.min], 2)} -
%   \Sexpr{round(d["alpha", hpd.max], 2)}];
%$I$,
%  \Sexpr{d["I", round(map)]} 
%  [\Sexpr{d["I", round(hpd.min)]} -
%   \Sexpr{d["I", round(hpd.max)]}];
%$m$,
%  \Sexpr{d["m", floor(map)]} 
%  [\Sexpr{d["m", floor(hpd.min)]} -
%   \Sexpr{d["m", floor(hpd.max)]}];
%$N$,
%  \Sexpr{d["N", round(map)]} 
%  [\Sexpr{d["N", round(hpd.min)]} -
%   \Sexpr{d["N", round(hpd.max)]}].
%The approximate posterior distributions for this simulation are shown in
%\cref{fig:mixed}. To test the effect of sampling bias, we sampled one
%transmission tree in a peer-driven fashion, where the probability to sample a
%node was twice as high if one of its peers had already been sampled. The
%parameters for this experiment were $N$ = 5000, $m$ = 2, $\alpha$ = 0.5, and
%$I$ = 2000. The estimated values were
%$\alpha$, 
%  \Sexpr{p["alpha", round(map, 2)]} 
%  [\Sexpr{p["alpha", round(hpd.min, 2)]} -
%   \Sexpr{p["alpha", round(hpd.max, 2)]}];
%$I$,
%  \Sexpr{p["I", floor(map)]} 
%  [\Sexpr{p["I", floor(hpd.min)]} -
%   \Sexpr{p["I", floor(hpd.max)]}];
%$m$,
%  \Sexpr{p["m", floor(map)]} 
%  [\Sexpr{p["m", floor(hpd.min)]} -
%   \Sexpr{p["m", floor(hpd.max)]}];
%$N$,
%  \Sexpr{p["N", floor(map)]} 
%  [\Sexpr{p["N", floor(hpd.min)]} -
%   \Sexpr{p["N", floor(hpd.max)]}].
%The approximate posterior distributions are shown in \cref{fig:peerdriven}. Both
%of these results were in line with estimates obtained on other simulated
%datasets (\cref{tab:abchpd}), although the estimate of peer-driven sampling for
%$\alpha$ was somewhat lower than typical.

%\subsubsection*{Effect of parameters on power-law exponent}
%
%\Cref{tab:glm} shows the estimated parameters for a log-link \gls{GLM} fitted
%to the observed distribution of \gls{gamma} values. The coefficients are
%interpretable as multiplicative effects.
%
%\begin{table}
%    \centering
%    \input{\tablepath/pa-gamma-glm}
%    \caption{Estimated \gls{GLM} parameters for relationship between power-law
%    exponent \gls{gamma} and \gls{BA} model parameters.}
%    \label{tab:glm}
%\end{table}

\subsection{Application to HIV data}

<<realdata, include=FALSE>>=
  m <- collect.metadata("../../simulations/aggregates/hpd/*")
  d1 <- fread(rownames(m)[m$m_min == 1])
  d2 <- fread(rownames(m)[m$m_min == 2])

  map1 <- dcast.data.table(d1, dataset~parameter, value.var="map")
  up1 <- dcast.data.table(d1, dataset~parameter, value.var="hpd.max")
  low1 <- dcast.data.table(d1, dataset~parameter, value.var="hpd.min")
  setkey(map1, dataset)
  setkey(up1, dataset)
  setkey(low1, dataset)

  map2 <- dcast.data.table(d2, dataset~parameter, value.var="map")
  up2 <- dcast.data.table(d2, dataset~parameter, value.var="hpd.max")
  low2 <- dcast.data.table(d2, dataset~parameter, value.var="hpd.min")
  setkey(map2, dataset)
  setkey(up2, dataset)
  setkey(low2, dataset)
@

We applied kernel-ABC to five published HIV datasets (\cref{tab:data}),
and found substantial heterogeneity among the parameter estimates
(\cref{fig:abchpd,fig:abchpdm2}). Plots of the marginal posterior distributions
for each dataset are shown in
\cref{fig:cuevas,fig:li,fig:niculescu,fig:novitsky,fig:wang}.
Two of the datasets (\textcite{niculescu2015recent, wang2015targeting}) had
estimated $\alpha$ values near unity for the prior allowing $m = 1$ (\gls{MAP}
estimates [95\% \gls{HPD}] 
  \Sexpr{map1["niculescu2015", round(alpha, 2)]} 
  [\Sexpr{low1["niculescu2015", round(alpha, 2)]} - 
   \Sexpr{up1["niculescu2015", round(alpha, 2)]}]
and
  \Sexpr{map1["wang2015", round(alpha, 2)]} 
  [\Sexpr{low1["wang2015", round(alpha, 2)]} -
   \Sexpr{up1["wang2015", round(alpha, 2)]}] respectively).
The MAP estimates did not change appreciably when $m = 1$ was disallowed by the
prior, although the credible interval of the \textcite{niculescu2015recent}
data was narrower
  (\Sexpr{low1["niculescu2015", round(alpha, 2)]} - 
   \Sexpr{up1["niculescu2015", round(alpha, 2)]}).
When $m = 1$ was permitted, the \textcite{li2015hiv, cuevas2009hiv} both had
low estimated $\alpha$ values
  (\Sexpr{map1["li2015", round(alpha, 2)]} 
  [\Sexpr{low1["li2015", round(alpha, 2)]} - 
  \Sexpr{up1["li2015", round(alpha, 2)]}]
and
  \Sexpr{map1["cuevas2009", round(alpha, 2)]} 
  [\Sexpr{low1["cuevas2009", round(alpha, 2)]} -
   \Sexpr{up1["cuevas2009", round(alpha, 2)]}]). 
However, the MAP estimates increased when $m = 1$ was not permitted, although
the HPD intervals remained roughly the same
  (\Sexpr{map2["li2015", round(alpha, 2)]} 
  [\Sexpr{low2["li2015", round(alpha, 2)]} - 
  \Sexpr{up2["li2015", round(alpha, 2)]}]
and
  \Sexpr{map2["cuevas2009", round(alpha, 2)]} 
  [\Sexpr{low2["cuevas2009", round(alpha, 2)]} -
   \Sexpr{up2["cuevas2009", round(alpha, 2)]}]).
The \textcite{novitsky2014impact} data had a fairly low estimated $\alpha$
for both priors on $m$
  (\Sexpr{map1["novitsky2014", round(alpha, 2)]} for $m \geq 1$;
   \Sexpr{map2["novitsky2014", round(alpha, 2)]} for $m \geq 2$).
However, the confidence interval was much wider when $m = 1$ was allowed
  ([\Sexpr{low1["novitsky2014", round(alpha, 2)]} -
    \Sexpr{up1["novitsky2014", round(alpha, 2)]}] for $m \geq 1$ vs.
   [\Sexpr{low2["novitsky2014", round(alpha, 2)]} -
    \Sexpr{up2["novitsky2014", round(alpha, 2)]}] for $m \geq 2$).

For all the datasets except \citeauthor{novitsky2014impact}, estimated values
of $I$ were below 2000 when $m = 1$ was allowed, with relatively narrow HPD
intervals compared to the nonzero prior density region
  (\citeauthor{cuevas2009hiv}, \Sexpr{map1["cuevas2009", floor(I)]} 
  [\Sexpr{low1["cuevas2009", floor(I)]} -
   \Sexpr{up1["cuevas2009", floor(I)]}];
   \citeauthor{niculescu2015recent}, \Sexpr{map1["niculescu2015", floor(I)]}
  [\Sexpr{low1["niculescu2015", floor(I)]} - 
   \Sexpr{up1["niculescu2015", floor(I)]}];
  \citeauthor{li2015hiv}, \Sexpr{map1["li2015", floor(I)]} 
  [\Sexpr{low1["li2015", floor(I)]} -
   \Sexpr{up1["li2015", floor(I)]}];
   \citeauthor{wang2015targeting}, \Sexpr{map1["wang2015", floor(I)]}
  [\Sexpr{low1["wang2015", floor(I)]} - 
   \Sexpr{up1["wang2015", floor(I)]}]).
The \citeauthor{novitsky2014impact} data was the outlier, with a very high
estimated $I$, and HPD interval spanning almost the entire prior region
  (\Sexpr{map1["novitsky2014", floor(I)]} 
  [\Sexpr{low1["novitsky2014", floor(I)]} -
   \Sexpr{up1["novitsky2014", floor(I)]}]).
The $I$ estimates and HPD intervals were generally robust to the choice of
prior on $m$, with slightly narrower HPD intervals (compare
\cref{fig:abchpd,fig:abchpdm2}).

The MAP estimate of $m$ was equal to 1 for all but the
\citeauthor{novitsky2014impact} data, when this value was allowed. However, the
upper bound of the HPD interval was different for each dataset
  (\citeauthor{niculescu2015recent}, \Sexpr{up1["niculescu2015", round(m)]};
   \citeauthor{wang2015targeting}, \Sexpr{up1["wang2015", round(m)]};
   \citeauthor{li2015hiv}, \Sexpr{up1["li2015", round(m)]};
   \citeauthor{cuevas2009hiv}, \Sexpr{up1["cuevas2009", round(m)]}).
When $m = 1$ was disallowed, the MAP for all datasets was either 2 or 3, with
HPD intervals spanning the entire prior region. The estimates for the total
number of nodes $N$ were largely uninformative for all samples, with almost all
MAP estimates greater than 7500 and HPD intervals spanning almost the entire
nonzero prior density region. The only exception was the \citeauthor{li2015hiv}
data, for which the MAP estimate was lower 
  (\Sexpr{map1["li2015", floor(N)]})
when $m = 1$ was allowed.

\begin{figure*}[ht]
  \centering
  \includegraphics{realdata-hpd-bc}
  \caption[
      Maximum \textit{a posteriori} point estimates and 95\% HPD intervals for
      parameters of the BA network model, fitted to six HIV datasets
      with \software{netabc}.]
  {
      Maximum \textit{a posteriori} point estimates and 95\% HPD intervals for
      parameters of the BA network model, fitted to six HIV datasets with
      \software{netabc}. Legend labels indicate risk group and country of
      origin. Abbreviations: IDU, injection drug users; MSM, men who have sex
      with men; HET, heterosexual.
  }
  \label{fig:abchpd}
\end{figure*}

To make our analyses comparable to the existing network literature, we
estimated values of the power law exponent $\gamma$ for each of the datasets 
investigated. The results are summarized in \cref{tab:gamma}. All of the
estimated exponents were in the range $2 \leq \gamma \leq 4$ commonly reported
in the literature~\autocite{liljeros2001web, schneeberger2004scale,
colgate1989risk, brown2011transmission}. 

\begin{table}
    \centering
    \input{\tablepath/realdata-gamma.tex}
    \caption[
        Estimated power law exponents for six HIV datasets based on maximum
        \textit{a priori} estimates of BA model parameters.
    ]{
        Estimated power law exponents for six HIV datasets based on maximum
        \textit{a priori} estimates of BA model parameters. 100 networks were
        simulated using \textit{MAP} parameter estimates obtained with
        \software{netabc}. The power law exponent $\gamma$ was estimated for
        each, and the median of those estimates was used as a point estimate
        for the corresponding dataset.
    }
    \label{tab:gamma}
\end{table}
