\subsection{Separable parameters}

Our first simulation-based experiment was designed to determine which network
model parameters were potentially estimable by kernel-ABC, and to find the best
set of kernel meta-parameters to carry through to future experiments. For each
parameter of the \gls{BA} model, we simulated trees under three distinct
values, and computed the cross-validation accuracy of a kernel-\gls{SVM}
classifier for the parameter.

In Figure~\ref{fig:patrees}, we show that trees simulated under different
values of \gls{alpha} are visibly quite distinctive. In particular, higher
values of \gls{alpha} result in networks with a small number of highly
connected nodes (see Figure~\ref{fig:pabounds}) which, once infected, are
likely to transmit to many other nodes. This results in a more unbalanced,
ladder-like structure in the phylogeny. Kernel-\gls{PCA} projections show that
all three \gls{alpha} values are well separated from each other under several
different sampling scenarios (Figure~\ref{fig:pakpca}). With smaller trees,
it becomes harder to visually disinguish \gls{alpha} = 0.5 from \gls{alpha} =
1.0 using the first two principal components.

With suitably chosen meta-parameters, the accuracy of the kernel-\gls{SVM}
classifier for \gls{alpha} was very high under a variety of prevalence and
sampling scenarios (Figure~\ref{fig:pacrossv}). Accuracy was highest, with
$R^2$ values above 0.95, for the largest trees and complete sampling (bottom
center panel). However, even for trees of size 100 sampled from an epidemic on
2000 nodes, the $R^2$ was above 0.8 (top right panel). In all cases, the
accuracy of a Sackin's index-based classifier was also quite high, at about
0.75. An \gls{SVM} classifier using only the \gls{nltt}
statistic~\autocite{janzen2015approximate} was slightly worse than Sackin's
index, with an $R^2$ of X.

The fact that Sackin's index is informative of \gls{alpha} was
unsurprising, given the clear differences in tree balance observed under
different \gls{alpha} values. 


\begin{figure}[ht]
  \centering
  \label{fig:patrees}
  \includegraphics{kernel_pa_trees}
  \caption[Visibly distinctive trees simulated under three values of \gls{alpha}]{
    Epidemics of 1000 infected were simulated on \gls{BA} networks of 5000
    nodes, with \gls{alpha} equal to 0.5, 1.0, or 1.5. Transmission trees were
    created by sampling 500 infected nodes. Higher \gls{alpha} values produce
    networks with a small number of highly-connected nodes, which results in a
    highly unbalanced, ladder-like tree structure.
  }
\end{figure}

\begin{figure}[ht]
  \centering
  \label{fig:pacrossv}
  \includegraphics{kernel_pa_crossv}
  \caption[Cross-validation performance of kernel support vector machine
  classifier for preferential attachment power]{
    Cross-validation performance of kernel support vector machine classifier
    for preferential attachment power.
  }
\end{figure}

\begin{figure}[ht]
  \centering
  \label{fig:pakpca}
  \includegraphics{kernel_pa_kpca}
  \caption[Projection of kernel matrix for different attachment power values
  onto its first two principal components]{
    Projection of the kernel matrix for different preferential attachment power
    values onto its first two principal components, for eight simulation
    scenarios. Each point corresponds to a simulated transmission tree, and is
    coloured by preferential attachment power. Facets are number of infected
    nodes (horizontal), and number of sampled tips (vertical). The parameters
    to the tree kernel were $\lambda = 0.3$ and $\sigma = 4$, and the nLTT was
    not used. Qualitatively, trees with a larger number of tips are easier to
    separate in kernel space, regardless of what sampling proportion they
    represent. In all cases, the highest attachment power can be separated from
    the other two, but the two lowest values become hand to distinguish with in
    the 100-tip trees.
  }
\end{figure}

Figure~\ref{fig:rsquared} shows the cross-validation accuracy of classifiers
for all four parameters of the \gls{BA} model. The classifier for \gls{I} also
exhibited high performance, although not quite as high as that for \gls{alpha}.
The average $R^2$ values were 0.70 for 100-tip trees, or 0.93 for 500-tip
trees. On the other hand, $m$ and $N$ were much harder to classsify. For $m$,
all $R^2$ values were below 0.5, falling to virtually zero in four scenarios.
The $R^2$ values for $N$ were highly variable, ranging from 0.08 to 0.82
depending on $I$ and the number of tips in the tree. In almost all cases, both
the nLTT and Sackin's index classifiers were worse than the tree kernel, except
for classifying $N$ with $I = 2000$ and $100$ tips, where the nLTT gave a small
advantage (see the supplements). The greatest disadvantage for the Sackin's
index-based classifier was on the $I$ parameter, where it had an $R^2$ of less
than 0.1, despite the relatively high accuracy of the tree kernel (see the
supplements).

\begin{figure}[ht]
  \centering
  \includegraphics{kernel_rsquared}
  \caption[Cross-validation accuracy of kSVM classifiers for \gls{BA}
  model parameters.]{Cross-validation $R^2$ values of kernel-SVM classifier for
    \gls{BA} model parameters. Points and ranges are median and 95\% quantiles
    over 1000 2-fold cross-validations.}
  \label{fig:rsquared}
\end{figure}

% needs to go in methods

%It is important to note that, if we cut off the epidemic simulation when 500
%nodes are infected, the resulting tree will be shorter (in calendar time) than
%if we continue until 2000 nodes are infected. However, this information is not
%used when building the classifier, since the branch lengths in each tree are
%scaled by their mean. Therefore, the high performance of the classifier is due
%to structural differences captured by the tree kernel, rather than the trees
%simply having different heights.

\subsection{Accuracy of marginal estimates}

We used grid search to obtain \defn{marginal} estimates for each network
parameter while holding all other parameters fixed. We observed that kernel
scores were highest at the values of \gls{alpha} on the grid closest to the
true values, as shown in Figure~\ref{fig:gridkernel}. However, there was a much
stronger spike in kernel scores near the true value for \gls{alpha} = 1.0 and
1.25. This is recapitulated when we look at the accuracy of point estimates
obtained by taking the grid value with the highest median kernel score. As
shown in Figure~\ref{fig:gridest}, while the estimates are generally close to
the true value, they are much closer for \gls{alpha} = 1.25 than for the other
values.

\begin{figure}[ht]
  \centering
  \includegraphics{gridsearch_pa_kernel}
  \caption[Grid search kernel scores]{
    Grid search kernel scores for testing trees simulated under various
    \gls{alpha} values. All epidemics had \gls{I} = 1000 infected nodes, on
    \gls{BA} networks of size \gls{N} = 5000 with \gls{m} fixed at 2. Colours
    indicate the number of sampled tips.
  }
  \label{fig:gridkernel}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics{gridsearch_pa_estimates}
  \caption[Marginal estimates of \gls{alpha} obtained with grid search]{
    Marginal estimates of \gls{alpha} obtained with grid search. Training trees
    were simulated on a narrowly spaced grid of \gls{alpha} values, and
    compared to testing trees using the tree kernel. The \gls{alpha} value in
    the grid with the highest median kernel score was taken as the point
    estimate for the testing tree. These point estimates are shown as black
    dots. The dashed line is the identity.
  }
  \label{fig:gridest}
\end{figure}

\subsection{Accuracy of estimates with full ABC}
