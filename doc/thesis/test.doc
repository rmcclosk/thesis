<h1 id="abstract" class="unnumbered">Abstract</h1>
<p>Models of the spread of disease in a population often make the simplifying assumption that the population is homogeneously mixed, or is divided into homogeneously mixed compartments. However, human populations have complex structures formed by social contacts, which can have a significant influence on the rate of epidemic spread. Contact network models capture this structure by explicitly representing each contact which could possibly lead to a transmission. We developed a method based on kernel approximate Bayesian computation (kernel-ABC) for estimating structural parameters of the contact network underlying an observed viral phylogeny. The method combines adaptive sequential Monte Carlo for ABC, Gillespie simulation for propagating epidemics though networks, and a kernel-based tree similarity score. We used the method to fit the Barabási-Albert network model to simulated transmission trees, and also applied it to viral phylogenies estimated from five published HIV sequence datasets. On simulated data, we found that the preferential attachment power and the number of infected nodes in the network can often be accurately estimated. On the other hand, the mean degree of the network, as well as the total number of nodes, were not estimable with kernel-ABC. We observed substantial heterogeneity in the parameter estimates on real datasets, with point estimates for the preferential attachment power ranging from 0.06 to 1.05. These results underscore the importance of considering contact structures when performing phylodynamic inference. Our method offers the potential to quantitatively investigate the contact network structure underlying viral epidemics.</p>
<h1 id="preface" class="unnumbered">Preface</h1>
<p>The initial idea to use kernel approximate Bayesian computation to infer contact network model parameters was Dr. Poon’s. The tree kernel was originally developed by Dr. Poon, but the version used here was implemented by me to improve computational efficiency. The idea to apply sequential Monte Carlo is credited to Dr. Alexandre Bouchard-Côté. Dr. Sarah Otto suggested the experiments involving a network with a heterogeneous <span class="math"><em>α</em></span> parameter and peer-driven sampling. Dr. Richard H. Liang provided guidance in the development of the Gillespie simulation algorithm. The <em>netabc</em> program, and all supplementary analysis programs, were written by me.</p>
<p>A version of chapter 2 has been submitted for publication in Virus Evolution with the title “Reconstructing network parameters from viral phylogenies.” An oral presentation entitled “Phylodynamic inference of contact network parameters with kernel-ABC” was given based on chapter 2 at the 23rd HIV Dynamics and Evolution meeting on April 25, in Woods Hole, Massachussets, USA. A poster based on chapter 2 entitled “Likelihood-free estimation of contact network parameters from viral phylogenies” was presented at the Intelligent Systems for Molecular Biology meeting in Orlando, Florida, USA.</p>
<p>Use of the BC data is in accordance with an ethics application that was reviewed and approved by the UBC/Providence Health Care Research Ethics Board (H07-02559).</p>
<p>Throughout this work, the pronouns <em>we</em> and <em>our</em> refer to Rosemary M. McCloskey unless otherwise stated.</p>
<h1 id="acknowledgements" class="unnumbered">Acknowledgements</h1>
<h1 id="introduction">Introduction</h1>
<h2 id="objective">Objective</h2>
<p>The spread of a disease is most often modelled by assuming either a homogeneously mixed population <span class="citation"></span>, or a population divided into a small number of homogeneously mixed groups <span class="citation"></span>. This assumption, also called <em>mass action</em> <span class="citation"></span>, or <em>panmixia</em>, implies that any two individuals in the same compartment are equally likely to come into contact causing transmission. Although this provides a reasonable approximation in many cases <span class="citation"></span>, the error introduced by assuming a panmictic population can be substantial when significant contact heterogeneity exists in the underlying population <span class="citation"></span>. Contact network models provide an alternative to compartmental models which do not require the assumption of panmixia. In addition to more accurate predictions, the parameters of the networks themselves may be of interest from a public health perspective. For example, certain vaccination strategies may be more or less effective in curtailing an epidemic depending on the underlying network’s degree distribution <span class="citation"></span>. Phylodynamic methods have been used to fit many different types of model to phylogenetic data <span class="citation"></span>, but as far as we know, no methods have yet been developed to fit contact network models. The primary objective of this work is to develop such a method.</p>
<p>Calculating the likelihood of the parameters of a contact network models seems likely to be an intractable problem, which would imply that these models are amenable to neither nor Bayesian inference. We have not proven this is the case, but some intuition can be provided by examining the process involved in the likelihood calculation. Consider a contact network model with parameters <span class="math"><em>θ</em></span>, and an estimated transmission tree <span class="math"><em>T</em></span> with <span class="math"><em>n</em></span> tips. In general, we do not know the labels of the internal nodes of <span class="math"><em>T</em></span>, only the labels of its tips. To fit this model using likelihood-based methods, we must calculate the likelihood of <span class="math"><em>θ</em></span>, that is, <span class="math">Pr(<em>T</em> ∣ <em>θ</em>)</span>. Let <span class="math">G</span> be the set of all possible contact networks, and <span class="math">N</span> be the set of all possible labellings of the internal nodes of <span class="math"><em>T</em></span>. We can write the likelihood as</p>
<p><br /><span class="math">$$\begin{aligned}
\begin{split}
  \label{eq:netlik}
  \Pr(T \mid \theta)
    &amp;= \sum_{\nu \in \mathcal{N}} \Pr(T, \nu \mid \theta) \\
    &amp;= \sum_{G \in \mathcal{G}} \sum_{\nu \in \mathcal{N}} \Pr(T, \nu \mid G, \theta) \Pr(G \mid \theta) \\
    &amp;= \sum_{G \in \mathcal{G}} \sum_{\nu \in \mathcal{N}} \Pr(T, \nu \mid G) \Pr(G \mid \theta),
\end{split}\end{aligned}$$</span><br /></p>
<p>the last equality following from the fact that <span class="math"><em>T</em></span> and <span class="math"><em>ν</em></span> depend only on <span class="math"><em>G</em></span>, not on <span class="math"><em>θ</em></span>. Although <span class="math">Pr(<em>T</em>, <em>ν</em> ∣ <em>G</em>)</span> and <span class="math">Pr(<em>G</em> ∣ <em>θ</em>)</span> may individually be straightforward to calculate, the number of possible directed graphs on <span class="math"><em>N</em></span> nodes is <span class="math">2<sup><em>N</em>(<em>N</em> − 1)</sup></span> <span class="citation"></span>, larger if the nodes and edges in the graph may have different labels or attributes. Hence, the number of terms in the sum is at least exponential in <span class="math"><em>n</em></span>, as there must be at least <span class="math"><em>n</em></span> nodes in the network. In addition, assumes that <span class="math"><em>T</em></span> is complete, meaning that all infected individuals were sampled. This is rarely the case in practice - most often, we only have access to a subset of the infected individuals. In this case, the likelihood calculation becomes even more complex, because we must also sum over all possible complete trees.</p>
<p>Depending on the network model studied, it is possible that could be simplified into a tractable expression. However, a simpler alternative to likelihood-based methods, which would apply to any network model, is provided by  <span class="citation"></span>. All of the ingredients required to apply to this problem are readily available. Simulating networks is straightforward under a variety of models. Epidemics on those networks, and the corresponding transmission trees, can also be easily simulated. As mentioned above, contact networks can profoundly affect transmission tree shape, and those shapes can be compared using a highly informative similarity measure called the “tree kernel” <span class="citation"></span>. can be implemented with , which has several advantages over other algorithms <span class="citation"></span>. A recently-developed adaptive algorithm requiring minimal tuning on the part of the user makes an even more attractive approach <span class="citation"></span>. In summary, our method to infer contact network parameters will combine the following: stochastic simulation of epidemics on networks, the tree kernel, and adaptive -. Since our distance measure is a kernel function, our method is a type of kernel-. For ease of exposition, we will often use the term “kernel-” to refer to our method specifically.</p>
<p>Empirical studies of sexual contact networks have found that these networks tend to be scale-free <span class="citation"></span>, meaning that their degree distributions follow a power law (although there has been some disagreement, see <span class="citation"></span>). Preferential attachment has been postulated as a mechanism by which scale-free networks could be generated <span class="citation"></span>. The model <span class="citation"></span> is one of the simplest preferential attachment models, which makes it a natural choice to explore with our method. The second aim of this work is to use simulations to investigate the parameters of the model, including whether they have a detectable impact on tree shape, and whether they can be accurately recovered using kernel-.</p>
<p>Due to its high global prevalence and fast mutation rate, is one of the most commonly-studied viruses in a phylodynamic context. Consequently, a large volume of sequence data is publicly available, more than for any other pathogen, and including sequences sampled from diverse geographic and demographic contexts. At the time of this writing, there were <span class="math">635400</span> HIV sequences publicly available in GenBank, annotated with 172 distinct countries of origin. Since is almost always spread through either sexual contact or sharing of injection drug supplies, the contact networks underlying epidemics are driven by social dynamics and are therefore likely to be highly nonrandom. Moreover, since no cure yet exists, efforts to curtail the progression of an epidemic have relied on preventing further transmissions through measures such as and education leading to behaviour change. The effectiveness of this type of intervention can vary significantly based on the underlying structure of the network and the particular nodes to whom the intervention is targeted <span class="citation"></span>. Due to this combination of data availability and potential public health impact, is an obvious context in which our method could be applied. Therefore, the third and final aim of this work is to apply kernel- to fit the model to existing outbreaks.</p>
<p>To summarize, this work has three objectives. First, we will develop a method which uses kernel- to infer parameters of contact network models from observed transmission trees. Second, we will use simulations to characterize the parameters of the network model in terms of their effect on tree shape and how accurately they can be recovered with kernel-. Finally, we will apply the method to fit the model to several real-world datasets.</p>
<h2 id="sec:phylo">Phylogenetics and phylodynamics</h2>
<h3 id="phylogenetic-trees">Phylogenetic trees</h3>
<p>In evolutionary biology, a <em>phylogeny</em>, or <em>phylogenetic tree</em>, is a graphical representation of the the evolutionary relationships among a group of organisms or species (generally, <em>taxa</em>) <span class="citation"></span>. The <em>tips</em> of a phylogeny, that is, the nodes without any descendants, correspond to <em>extant</em>, or observed, taxa. The <em>internal nodes</em> correspond to their (usually extinct) common ancestors. The edges or <em>branches</em> of the phylogeny connect ancestors to their descendants. Phylogenies may have a <em>root</em>, which is a node with no descendants distinguished as the most recent common ancestor of all the extant taxa <span class="citation"></span>. When such a root exists, the tree is referred to as being <em>rooted</em>; otherwise, it is <em>unrooted</em>. The structural arrangement of nodes and edges in the tree is referred to as its <em>topology</em> <span class="citation"></span>.</p>
<p>The branches of the tree may have associated lengths, representing either evolutionary distance or calendar time between ancestors and their descendants. The term “evolutionary distance” is used here imprecisely to mean any sort of quantitative measure of evolution, such as the number of differences between the DNA sequences of an ancestor its descendant, or the difference in average body mass or height. A phylogeny with branch lengths in calendar time units is often referred to as <em>time-scaled</em>. In a time-scaled phylogeny, the internal nodes can be mapped onto a timeline by using the tips of the tree, which usually correspond to the present day, as a reference point <span class="citation"></span>. The corresponding points on the timeline are called <em>branching times</em>, and the rate of their accumulation is referred to as the <em>branching rate</em>. Rooted trees whose tips are all the same distance from the root are called <em>ultrametric</em> trees <span class="citation"></span>. These concepts are illustrated in .</p>
<p><embed src="speciestree.pdf" /> [fig:speciestree]</p>
<h3 id="transmission-trees">Transmission trees</h3>
<p>In epidemiology, a <em>transmission tree</em> is a graphical representation of an epidemic’s progress through a population. Like phylogenies, transmission trees have tips, nodes, edges, and branch lengths. However, rather than recording an evolutionary process (speciation), they record an epidemiological process (transmission). The tips of a transmission tree represent the removal of infected hosts, while internal nodes correspond to transmissions from one host to another. Transmission trees generally have branch lengths in units of calendar time, with branching times indicating times of transmission. The root of a transmission tree corresponds to the initially infected patient who introduced the epidemic into the network, also known as the <em>index case</em>. The internal nodes may be labelled with the donor of the transmission pair, if this is known. The tips of the tree, rather than being fixed at the present day, are placed at the time at which the individual was removed from the epidemic, such as by death, recovery, isolation, behaviour change, or migration. Consequently, the transmission tree may not be ultrametric, but may have tips located at varying distances from the root. Such trees are said to have <em>heterochronous</em> taxa <span class="citation"></span>, in contrast to the <em>isochronous</em> taxa found in most phylogenies of macro-organisms. A transmission tree is illustrated in (right).</p>
<p>Each infected individual in an epidemic may appear in the transmission tree more than once. This is different from the transmission <em>network</em>, in which each infected individual appears exactly once, and edges are in one-to-one correspondence with transmissions <span class="citation"></span>. Transmission networks are discussed further in , and the distinction between the two objects is illustrated in . However, since transmission networks generally have no cycles (unless re-infection occurs), they are trees in the graph theoretical sense, and hence are sometimes also referred to as transmission trees <span class="citation"></span>. In this work, we reserve the term “transmission tree” for the objects depicted on the right side of , following <em>e.g.</em> <span class="citation"></span>. The term “transmission network” is taken to mean the subgraph of the contact network along which transmissions occurred, following <em>e.g.</em> <span class="citation"></span>.</p>
<p><embed src="contactnet.pdf" /> [fig:contactnet]</p>
<p>Since transmission trees are essentially a detailed record of an epidemic’s progress, they contain substantial epidemiological information. As a basic example, the plot <span class="citation"></span>, which plots the number of lineages in a phylogeny against time, can be used to quantify the incidence of new infections over the course of an epidemic <span class="citation"></span>. However, in all but the most well-studied of epidemics, transmission trees are not possible to assemble through traditional epidemiological methods <span class="citation"></span>. The time and effort to conduct detailed interviews and contact tracing of a sufficient number of infected individuals is usually prohibitive, and may be additionally be confounded by misreporting <span class="citation"></span>. However, it turns out that for viral epidemics, some of the epidemiological information contained in the transmission tree leaves a mark on the viral genetic material circulating in the population. A family of methods called <em>phylodynamics</em> <span class="citation"></span> addresses the challenge of estimating epidemiological parameters from viral sequence data <span class="citation"></span>.</p>
<h3 id="phylodynamics-linking-evolution-and-epidemiology">Phylodynamics: linking evolution and epidemiology</h3>
<p>The basis of phylodynamics is the fact that, for RNA viruses, epidemiological and evolutionary processes occur on similar time scales <span class="citation"></span>. In fact, these two processes interact, such that it is possible to detect the influence of host epidemiology on the evolutionary history of the virus as recorded in an <em>inter-host viral phylogeny</em>. Phylodynamic methods aim to detect and quantify the signatures of epidemiological processes in these phylogenies <span class="citation"></span>, which relate one representative viral genotype from each host in an infected population. These methods have been used to investigate parameters such as transmission rate, recovery rate, and basic reproductive number <span class="citation"></span>. The majority of phylodynamic studies attempt to infer the parameters of an epidemiological model for which the likelihood of an observed phylogeny can be calculated. Most often, this is some variation of the birth-death <span class="citation"></span> or coalescent <span class="citation"></span> models. These methods either assume the viral phylogeny is known, as we do in this work, or (more commonly) integrate over phylogenetic uncertainty using Bayesian methods. Phylogenetic inference is a complex topic which we shall not discuss here; see <em>e.g.</em><span class="citation"></span> for a full review.</p>
<p>Due to the relationship between the aforementioned processes, there is a degree of correspondence between viral phylogenies and transmission trees <span class="citation"></span>. In particular, the transmission process is quite similar to <em>allopatric speciation</em> <span class="citation"></span>, where genetic divergence follows the geographic isolation of a sub-population of organisms. Thus, transmission, which is represented as branching in the transmission tree, causes branching in the viral phylogeny as well <span class="citation"></span>. Similarly, the removal of an individual from the transmission tree causes the extinction of their viral lineage in the phylogeny. Consequently, the topology of the viral phylogeny is sometimes used as a proxy for the topology of the transmission tree <span class="citation"></span>. Modern likelihood-based methods of phylogenetic reconstruction <span class="citation"></span> produce unrooted trees whose branch lengths measure genetic distance in units of expected substitutions per site. On the other hand, transmission trees are rooted, and have branches measuring calendar time <span class="citation"></span>. Therefore, estimating a transmission tree from a viral phylogeny requires the phylogeny to be rooted and time-scaled. Methods for performing this process include root-to-tip regression <span class="citation"></span>, which we apply in this work, and least-square dating <span class="citation"></span>. Alternatively, the tree may be rooted separately with an outgroup <span class="citation"></span> before time-scaling.</p>
<p>A caveat of estimating transmission trees in this manner is that the correspondence between the topologies of the viral phylogeny and transmission tree is far from exact <span class="citation"></span>. Due to intra-host diversity, the viral strain which is transmitted may have split from another lineage within the donor long before the transmission event occurred. Hence, the branching point in the viral phylogeny may be much earlier than that in the transmission tree. Another possibility is that one host transmitted to two or more recipients, but the lineages they each received originated within the donor host in a different order than that in which the transmissions occurred. In this case, the topology of the transmission tree and the viral phylogeny will be mismatched. In practice, this discordance has not proven an insurmountable problem: for example, <span class="citation"></span> were able to accurately recover a known transmission tree using a viral phylogeny. The problem of accurately estimating transmission trees is an ongoing area of research <span class="citation"></span>.</p>
<h3 id="subsec:treeshape">Tree shapes</h3>
<p>To perform phylodynamic inference, we must be able to extract quantitative information from viral phylogenies. What is informative about a phylogeny, beyond the demographic characteristics of the individuals it relates, is its <em>shape</em>. The shape of a phylogeny has two components: the topology, and the distribution of branch lengths <span class="citation"></span>. Methods of quantifying tree shape fall into two categories: summary statistics, and pairwise measures. Summary statistics assign a numeric value to each individual tree, while pairwise measures quantify the similarity between pairs of trees.</p>
<p>One of the most widely used tree summary statistics is Sackin’s index <span class="citation"></span>, which measures the imbalance or asymmetry in a rooted tree. For the <span class="math"><em>i</em></span>th tip of the tree, we define <span class="math"><em>N</em><sub><em>i</em></sub></span> to be the number of branches between that tip and the root. The unnormalized Sackin’s index is defined as the sum of all <span class="math"><em>N</em><sub><em>i</em></sub></span>. It is called unnormalized because it does not account for the number of tips in the tree. Among two trees having the same number of tips, the least-balanced tree will have the highest Sackin’s index. However, among two equally balanced trees, the larger tree will have a higher Sackin’s index. This makes it challenging to compare balances among trees of different sizes. To correct this, <span class="citation"></span> derive the expected value of Sackin’ index under the Yule model <span class="citation"></span>. Dividing by this expected value normalizes Sackin’s index, so that it can be used to compare trees of different sizes. An example of a pairwise measure is the  <span class="citation"></span>, which compares the  <span class="citation"></span> plots of two trees. Specifically, the two plots are normalized so that they begin at <span class="math">(0, 0)</span> and end at <span class="math">(1, 1)</span>, and the absolute difference between the two plots is integrated between 0 and 1. In the context of infectious diseases, the is related to the prevalence <span class="citation"></span>, so large values may indicate that the trees being compared are the products of different epidemic trajectories <span class="citation"></span>.</p>
<p><span class="citation"></span> developed an alternative pairwise measure which applies the concept of a <em>kernel function</em> to phylogenies. Kernel functions, originally developed for  <span class="citation"></span>, compare objects in a space <span class="math">X</span> by mapping them into a feature space <span class="math">F</span> of high or infinite dimension via a function <span class="math"><em>φ</em></span>. The similarity between the objects is defined as <br /><span class="math"><em>K</em>(<em>x</em>, <em>x</em>ʹ) = ⟨<em>φ</em>(<em>x</em>), <em>φ</em>(<em>x</em>ʹ)⟩, </span><br /> that is, the inner product of the objects’ representations in the feature space. Computing <span class="math"><em>φ</em>(<em>x</em>)</span> may be computationally prohibitive due to the dimension of <span class="math">F</span>. The utility of a kernel function <span class="math"><em>K</em></span> is that it is constructed in such a way that it can compute the inner product without explicitly computing <span class="math"><em>φ</em>(<em>x</em>)</span>. The kernel function developed in <span class="citation"></span> will henceforth be referred to as the <em>tree kernel</em>. This kernel maps trees into the space of all possible possible <em>subset trees</em>, which are subtrees that do not necessarily extend all the way to the tips. The subset-tree kernel was originally developed for comparing parse trees in natural language processing <span class="citation"></span> and did not incorporate branch length information. The version developed by <span class="citation"></span> includes a radial basis function to compare the differences in branch lengths, thus incorporating both the trees’ topologies and their branch lengths in a single similarity score.</p>
<p>The kernel score of a pair of trees, denoted <span class="math"><em>K</em>(<em>T</em><sub>1</sub>, <em>T</em><sub>2</sub>)</span>, is defined as a sum over all pairs of nodes <span class="math">(<em>n</em><sub>1</sub>, <em>n</em><sub>2</sub>)</span>, where <span class="math"><em>n</em><sub>1</sub></span> is a node in <span class="math"><em>T</em><sub>1</sub></span> and <span class="math"><em>n</em><sub>2</sub></span> is a node in <span class="math"><em>T</em><sub>2</sub></span>. Folloting <span class="citation"></span>, let <span class="math"><em>N</em>(<em>T</em>)</span> denote the set of all nodes in <span class="math"><em>T</em></span>, <span class="math">$\nc(n)$</span> be the number of children of node <span class="math"><em>n</em></span>, <span class="math"><em>c</em><sub><em>n</em></sub><sup><em>j</em></sup></span> be the <span class="math"><em>j</em></span>th child of node <span class="math"><em>n</em></span>, and <span class="math"><em>l</em><sub><em>n</em></sub></span> be the vector of branch lengths connecting node <span class="math"><em>n</em></span> to its descendants. The <em>production rule</em> of <span class="math"><em>n</em></span> is its total number of children, and its number of leaf children. That is, if two nodes have the same number of children and among these, the same number of leaves, then they have the same production rule. Let <span class="math"><em>k</em><sub><em>G</em></sub>(<em>x</em>, <em>y</em>)</span> be a Gaussian radial basis function of the vectors <span class="math"><em>x</em></span> and <span class="math"><em>y</em></span>, <br /><span class="math">$$k_G(x, y) = \exp\left(-\frac{1}{2\sigma} \left\lVertx - y\right\rVert_2^2\right),$$</span><br /> where <span class="math">∥ ⋅ ∥<sub>2</sub></span> is the Euclidian norm and <span class="math"><em>σ</em></span> is a variance parameter. The tree kernel is defined as <br /><span class="math"><em>K</em>(<em>T</em><sub>1</sub>, <em>T</em><sub>2</sub>) = ∑ <sub><em>n</em><sub>1</sub> ∈ <em>N</em>(<em>T</em><sub>1</sub>)</sub>∑ <em>n</em><sub>2</sub> ∈ <em>N</em>(<em>T</em><sub>2</sub>)Δ (<em>n</em><sub>1</sub>, <em>n</em><sub>2</sub>), </span><br /> where <br /><span class="math">$$\Delta(n_1, n_2) =
  \begin{cases}
    \lambda &amp; n_1 \text{ and } n_2 \text{ are leaves} \\
    \lambda k_G(l_{n_1}, l_{n_2}) \displaystyle\prod_{j=1}^{\nc(n_1)} \left(1 +
    \Delta(c_{n_1}^j, c_{n_2}^j) \right) &amp; \begin{aligned} n_1 \text{ and } n_2 \text{ have the same} \\ \text{production rule} \end{aligned} \\
    0 &amp; \text{otherwise}.

  \end{cases}$$</span><br /> Here <span class="math"><em>λ</em></span> is a decay factor parameter, which penalizes large matches that tend to dominate the kernel score. In this work, we refer to the parameters <span class="math"><em>λ</em></span> and <span class="math"><em>σ</em></span> as <em>meta-parameters</em>, to avoid confusing them with model parameters we are trying to estimate.</p>
<h2 id="sec:contactnet">Contact networks</h2>
<h3 id="subsec:netoverview">Overview</h3>
<p>Epidemics spread through populations of hosts through <em>contacts</em> between those hosts. The definition of contact depends on the mode of transmission of the pathogen in question. For an airborne pathogen like influenza, a contact may be simple physical proximity, while for , contact could be via unprotected sexual relations or blood-to-blood contact (such as through needle sharing). A <em>contact network</em> is a graphical representation of a host population and the contacts among its members <span class="citation"></span>. The <em>nodes</em> in the network represent hosts, and <em>edges</em> or <em>links</em> represent contacts between them. A contact network is shown in (left). Contact networks are a particular type of <em>social network</em> <span class="citation"></span>, which is a network in which edges may represent any kind of social or economic relationship. Social networks are frequently used in the social sciences to study phenomena where relationships between people or entities are important <span class="citation"></span>.</p>
<p>Edges in a contact networks may be <em>directed</em>, representing one-way transmission risk, or <em>undirected</em>, representing symmetric transmission risk. For example, a network for an airborne epidemic would use undirected edges, because the same physical proximity is required for a host to infect or to become infected. However, an infection which may be spread through blood-to-blood contact through transfusions transfusions would use directed edges, since the donor has no chance of transmitting to the recipient. Directed edges are also useful when the transmission risk is not equal between the hosts, such as with transmission among , where the receptive partner carries a higher risk of infection than the insertive partner <span class="citation"></span>. In this case, a contact could be represented by two directed edges, one in each direction between the two hosts, with the edges annotated by what kind of risk they imply <span class="citation"></span>. An undirected contact network is equivalent to a directed network where each contact is represented by two symmetric directed edges. The <em>degree</em> of a node in the network is how many contacts it has. In directed networks, we may make the distinction between <em>out-degree</em> and <em>in-degree</em>, which count respectively the number incoming and outgoing edges. The <em>degree distribution</em> of a network denotes the probability that a node has any given number of links. The set of edges attached to a node are referred to as its <em>incident</em> edges.</p>
<p>Epidemiological models most often assume some form of contact homogeneity. The simplest models, such as the model <span class="citation"></span>, assume a completely homogeneously mixed population, where every pair of contacts is equally likely. More sophisticated models partition the population into groups with different contact rates between and among each group <span class="citation"></span>. However, these models still assume that every possible contact between a member of group <span class="math"><em>i</em></span> and a member of group <span class="math"><em>j</em></span> is equally likely. This assumption is clearly unrealistic for the majority of human communities, and can lead to significant errors in predicted epidemic trajectories when there is substantial heterogeneity present <span class="citation"></span>. Contact networks provide a way to relax this assumption by representing individuals and their contacts explicitly. It is important to note that, although panmixia is an unrealistic modelling assumption, it has not proven a substantial hurdle to epidemic modelling in practice <span class="citation"></span>. Using this assumption, researchers have been able to derive estimates of the transmission rate and the basic reproductive number of various outbreaks, which have agreed with values obtained by on-the-ground data collection <span class="citation"></span>. Therefore, if one is interested only in these population-level variables, the additional complexity of contact network models may not be warranted. Rather, these models are most useful when we are interested in properties of the network itself, such as centrality, structural balance, and transitivity <span class="citation"></span>.</p>
<p>From a public health perspective, knowledge of contact networks has the potential to be extremely useful. On a population level, network structure can dramatically affect the speed and pattern of epidemic spread <span class="citation"></span>. For example, epidemics are expected to spread more rapidly in networks having the “small world” property, where the average path length between two nodes in the network is relatively low <span class="citation"></span>. Some sexually transmitted infections would not be expected to survive in a homogeneously mixed population, but their long-term persistence can be explained by contact heterogeneity <span class="citation"></span>. Hence, the contact network can provide an idea of what to expect as an epidemic unfolds. In terms of actionable information, the efficacy of different vaccination strategies may depend on the topology of the network <span class="citation"></span>. On a local level, contact networks can be informative about the groups or individuals who are at highest risk of acquiring or transmitting infection, and would therefore benefit most from public health interventions <span class="citation"></span>.</p>
<p>Contact networks are a challenging type of data to collect, requiring extensive epidemiological investigation in the form of contact tracing <span class="citation"></span>. Therefore, it has been necessary to explore less resource-intensive alternatives which still contain information about population structure. For instance, it is possible to obtain limited information about the contact network by individual interviews without contact tracing. Variables which can be estimated in this fashion are referred to as <em>node-level</em> measures <span class="citation"></span>. One of the most well-studied of these is the degree distribution mentioned above, which can theoretically be estimated by simply asking each person how many contacts they had in some interval of time. However, the degree distributions often observed in real-world sexual networks are heavy-tailed <span class="citation"></span>, so dense or peer-driven sampling would be needed to capture the high-degree nodes characterizing the tail of the distribution.</p>
<p>An alternative approach has been the analysis of other types of network, which can be directly estimated with phylogenetic methods from viral sequence data. Some work focuses on the <em>phylogenetic network</em>, in which two nodes are connected if the genetic distance between their viral sequences is below some threshold. Primarily, this work has focused on the detection of <em>phylogenetic clusters</em>, which are groups of individuals whose viral sequences are significantly more similar to each other’s than to the general population’s. The phylogenetic network is informative about “hotspots” of transmission and can be used to identify demographic groups to whom targeted interventions are likely to have the greatest effect <span class="citation"></span>. However, this network may show little to no agreement with a contact data obtained through epidemiological methods <span class="citation"></span>, and therefore may be a poor proxy for the contact network. Other studies <span class="citation"></span> have investigated the <em>transmission network</em>, which is the subgraph of the contact network consisting of infected nodes and the edges which led to their infections <span class="citation"></span> (, left). It is possible to estimate the transmission network phylogenetically, although the methods required for doing so are more sophisticated than for estimating the phylogenetic network <span class="citation"></span>. These studies again mostly focusing on clustering, and also on degree distributions.</p>
<p>Other statistical methods have been developed to infer contact network parameters strictly from the timeline of an epidemic, using neither genetic data nor reported contacts. <span class="citation"></span> developed a Bayesian method to infer the <span class="math"><em>p</em></span> parameter of an network, along with the transmission and removal rate parameters of the model, using observed infection and optionally removal times. However, it was designed for only a small number of observations, and was unable to estimate <span class="math"><em>p</em></span> independently from the transmission rate. <span class="citation"></span> significantly updated and extended the methodology of <span class="citation"></span>, and applied it to a measles outbreak affecting 188 individuals. They were able to obtain a much more informative estimate of <span class="math"><em>p</em></span>, although this data set included both symptom onset and recovery times for all individuals, and was unusual in that the entire contact network was presumed to be infected. <span class="citation"></span> developed differential equations describing the dynamics of the model on a wide variety of random networks defined by their degree distributions. Although the topic of estimation was not addressed in the original paper, <span class="citation"></span>’s method could in principle be used to fit such models to observed epidemic trajectories, similar to what is done with the ordinary model. <span class="citation"></span> later extended the method to dynamic contact networks and applied it to a sexual network relating 99 individuals investigated during a syphilis outbreak.</p>
<h3 id="subsec:pa">Scale-free networks and preferential attachment</h3>
<p>A <em>scale-free</em> network is one whose degree distribution follows a power law, meaning that the number of nodes in the network with degree <span class="math"><em>k</em></span> is proportional to <span class="math"><em>k</em><sup> − <em>γ</em></sup></span> for some constant <span class="math"><em>γ</em></span> <span class="citation"></span>. Scale-free networks are characterized by a large number of nodes of low degree, with relatively few “hub” nodes of very high degree. Epidemiological surveys have indicated that human sexual networks tend to be scale-free <span class="citation"></span>. Interestingly, many other types of network, including computer networks <span class="citation"></span>, biological metabolic networks <span class="citation"></span>, and academic co-author networks <span class="citation"></span>, also have the scale-free property.</p>
<p>Several properties of scale-free networks are relevant in epidemiology. The high-degree hub nodes are known as <em>superspreaders</em> <span class="citation"></span>, which have been postulated to contribute in varying degree to the spread of diseases such as  <span class="citation"></span> and  <span class="citation"></span>. Scale-free networks have no epidemic threshold <span class="citation"></span>, meaning that diseases with arbitrarily low transmissibility can persist at low levels indefinitely. This is in contrast with homogeneously mixed populations, in which transmissibility below the epidemic threshold would result in exponential decay in the number of infected individuals and eventual extinction of the pathogen <span class="citation"></span>.</p>
<p>One mechanism which has been shown to lead to scale-free networks is <em>preferential attachment</em> <span class="citation"></span>. The simplest preferential attachment model is known as the (BA) model after its inventors <span class="citation"></span>. Under this model, networks are formed by starting with a small number <span class="math"><em>m</em><sub>0</sub></span> of nodes. New nodes are added one at a time until there are a total of <span class="math"><em>N</em></span> in the network. Each time a new node is added, <span class="math"><em>m</em> ≥ 1</span> edges are added from it to other nodes in the graph. In the original formulation, the partners of the new node are chosen with probability linearly proportional to their degree. However, <span class="citation"></span> suggest extending the model such that the probability of choosing a partner of degree <span class="math"><em>d</em></span> is proportional to <span class="math"><em>d</em><sup><em>α</em></sup> + 1</span> for some constant <span class="math"><em>α</em></span>, and we use this extension here. When <span class="math"><em>m</em> = 1</span>, the network takes on the distinctive shape of a tree, that is, it does not contain any cycles. Cycles are present in the network for all all other <span class="math"><em>m</em></span> values.</p>
<p>There has been some contention of the idea that contact networks are scale-free. <span class="citation"></span> fit several stochastic models of partner formation to empirical degree distributions derived from population surveys of sexual behaviour. They found that a negative binomial distribution, rather than a power law, was the best fit to five out of six datasets, although the difference in goodness of fit was extremely small in four out of these five. <span class="citation"></span> found that an exponential distribution, rather than a power law, was the best fit to degree distributions of six social and sexual networks.</p>
<h3 id="relationship-between-network-structure-and-transmission-trees">Relationship between network structure and transmission trees</h3>
<p>The contact network underlying an epidemic constrains the shape of the transmission network, which in turn determines the topology of the transmission tree relating the infected hosts (). The index case who introduces the epidemic into the network becomes the root of the tree. Each time a transmission occurs, the lineage corresponding to the donor host in the tree splits into two, representing the recipient lineage and the continuation of the donor lineage. illustrates this correspondence. It must be emphasized that, although the order and timing of transmissions determines the tree topology uniquely, the converse does not hold. That is, for any given topology, there are in general many transmission networks which would lead to that topology. In other words, it impossible to distinguish who transmitted to whom from a transmission tree alone <span class="citation"></span>.</p>
<p>A number of studies have made progress in quantifying the relationship between contact networks and transmission trees. <span class="citation"></span> simulated epidemics over networks with four types of degree distribution. They then estimated the Bayesian skyride <span class="citation"></span> population size trajectory in two ways: from the phylogeny, using ; and from the incidence and prevalence trajectories, using the method developed by <span class="citation"></span>. The concordance between the two skyrides, as well as the relationship between the skyride and prevalence curve, was qualitatively different for each degree distribution. <span class="citation"></span> investigated the relationship between transmission tree imbalance and several epidemic parameters under four contact network models, and found that these relationships varied considerably depending on which model was being considered. The authors also investigated a real-world phylogeny and found a level of unbalancedeness inconsistent with a randomly mixing population. <span class="citation"></span> simulated transmission trees over networks with varying degrees of community structure. They found that transmission trees simulated under networks with low clustering could not generally be distinguished from those simulated under highly clustered networks, and concluded that contact network clusters do not affect transmission tree shape. However, more recently, <span class="citation"></span> investigated the correspondence between contact network clusters and transmission tree clusters, and did find a moderate correspondence between the two in some cases.</p>
<h2 id="sec:smc">Sequential Monte Carlo</h2>
<h3 id="overview-and-notation">Overview and notation</h3>
<p>is the name for a family of statistical inference methods which rely on approximating probability distributions of interest with large collections of <em>particles</em>, here denoted <span class="math">{<em>x</em><sup>(<em>k</em>)</sup>}</span> <span class="citation"></span>. These collections or <em>populations</em> are constructed to form a <em>Monte Carlo approximation</em> to some distribution of interest <span class="math"><em>π</em></span>, meaning that the empirical distribution of the particles converges in distribution to <span class="math"><em>π</em></span> as the population size gets large <span class="citation"></span>. The word <em>sequential</em> is used because the particle populations are modified in an iterative fashion over time, for example, to incorporate new evidence.</p>
<p>To fully describe , we will introduce some notation and terminology. The definitions of these terms will become clearer as they are used. For a sequence <span class="math"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>d</em></sub></span>, we will write <span class="math"><strong>x_i</strong></span> to mean the partial sequence <span class="math"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>i</em></sub></span>. The subscript <span class="math"><em></em><sup>(<em>k</em>)</sup></span> will be used to indicate the <span class="math"><em>k</em></span>th particle in a population. To ease the notational burden we will omit the superscripts and subscripts on the weight functions <span class="math"><em>w</em></span>.</p>
<p>We define a <em>Markov kernel</em> as the continuous analogue of the transition matrix in a finite-state Markov model. For some spaces <span class="math"><em>X</em></span> and <span class="math"><em>Y</em></span>, <span class="math"><em>K</em>: <em>X</em> × <em>Y</em> → [0, 1]</span> such that</p>
<p><br /><span class="math">$$\begin{aligned}
    \label{eq:mk}
    \int_Y K(x, y) \mathrm{d}\,y = 1\end{aligned}$$</span><br /></p>
<p>for all <span class="math"><em>x</em> ∈ <em>X</em></span>. This is an “operational” definition of Markov kernel which will be suitable for our purposes. A more rigorous definition can be found in <em>e.g.</em> <span class="citation"></span>. Note that a Markov kernels have nothing to do with the kernel functions defined in , other than sharing a name (the word “kernel” is ubiquitous in mathematics).</p>
<h3 id="subsec:sis">Sequential importance sampling for sequential Monte Carlo</h3>
<p>is one type of method, whose aim is to sample from a distribution <span class="math"><em>π</em></span> on an high-dimensional space, say <span class="math"><em>π</em>(<strong>x</strong>) = <em>π</em>(<em>x</em><sub>1</sub>, …, <em>x</em><sub><em>d</em></sub>)</span>. The basis of is , which is a method of estimating summary statistics of distributions which are known only up to a normalizing constant, and therefore cannot be sampled from directly. That is, if <span class="math"><em>π</em></span> is such a distribution and <span class="math"><em>f</em></span> is any real-valued function, is concerned with estimating <br /><span class="math">$$\pi(f) = \int f(x)\pi(x)\mathrm{d}\,x = \int f(x) \frac{\gamma(x)}{Z} \mathrm{d}\,x,$$</span><br /> where the integral is over the space on which <span class="math"><em>π</em></span> is defined, <span class="math"><em>γ</em>(<em>x</em>)</span> is known pointwise, and <span class="math"><em>Z</em> = ∫ <em>γ</em>(<em>x</em>)d <em>x</em></span> is the unknown normalizing constant. Suppose we have at hand another distribution <span class="math"><em>η</em></span>, called the <em>importance distribution</em>, from which we are able to sample. Define the <em>importance weight</em> as the ratio ratio <span class="math"><em>w</em>(<em>x</em>) = <em>γ</em>(<em>x</em>) / <em>η</em>(<em>x</em>)</span>. We can express the normalizing constant <span class="math"><em>Z</em></span> in terms of the importance weight and distribution, <span class="math"><em>Z</em> = ∫ <em>w</em>(<em>x</em>)<em>η</em>(<em>x</em>)d <em>x</em></span>, and in turn write the expectation of interest as <br /><span class="math">$$\int f(x) \pi(x) \mathrm{d}\,x = \frac{\int f(x) \gamma(x) \mathrm{d}\,x}
                               {\int w(x) \eta(x) \mathrm{d}\,x}.$$</span><br /> If we sample a large number of points from <span class="math"><em>η</em></span>, then <span class="math"><em>η</em>(<em>x</em>)</span> can be approximated by a Monte Carlo estimate. Since the remaining quantities <span class="math"><em>f</em></span>, <span class="math"><em>γ</em></span>, and <span class="math"><em>w</em></span> can all be evaluated pointwise, these are all the ingredients we need to obtain an estimate of <span class="math"><em>π</em>(<em>f</em>)</span>. Although this is a simple and elegant approach, the drawback is that the variance of the estimate is proportional to the variance of the importance weights <span class="citation"></span>, which may be quite large if <span class="math"><em>η</em></span> and <span class="math"><em>γ</em></span> are very different. Therefore, the practical use of on its own is limited, since it depends on finding an importance distribution similar to <span class="math"><em>π</em></span>, which we usually know very little about <em>a priori</em>.</p>
<p>The objective of is to build up an importance distribution <span class="math"><em>η</em></span> for <span class="math"><em>π</em></span> sequentially. By the general product rule, <span class="math"><em>π</em>(<strong>x</strong>)</span> can be decomposed as <br /><span class="math">$$\pi(\mathbf{x}) 
  = \pi(x_1) \pi(x_2 \mid x_1) \cdots
    \pi(x_{d-1} \mid \mathbf{x_{d-2}}) \pi(x_d \mid \mathbf{x_{d-1}}).$$</span><br /> This decomposition is natural in many contexts, particularly for on-line estimation. For example, in a stateful model like an , <span class="math"><em>x</em><sub><em>i</em></sub></span> may represent the state at time <span class="math"><em>i</em></span>, with <span class="math"><em>π</em>(<strong>x</strong>)</span> being the posterior distribution over possible paths. The importance distribution <span class="math"><em>η</em></span> for <span class="math"><em>π</em></span> will be constructed using a similar decomposition, <br /><span class="math">$$\eta(\mathbf{x}) 
  = \eta(x_1) \eta(x_2 \mid x_1) \cdots
    \eta(x_{d-1} \mid \mathbf{x_{d-2}}) \eta(x_d \mid \mathbf{x_{d-1}}).$$</span><br /> The importance weights for <span class="math"><em>η</em></span> can be written recursively as</p>
<p><br /><span class="math">$$\begin{aligned}
  \label{eq:sisw}
  w(\mathbf{x_i}) = \frac{\pi(\mathbf{x_i})}{\eta(\mathbf{x_i})}
  = \frac{\pi(x_i \mid \mathbf{x_{i-1}})\pi(\mathbf{x_{i-1}})}
         {\eta(x_i \mid \mathbf{x_{i-1}})\eta(\mathbf{x_{i-1}})}
  = \frac{\pi(x_i \mid \mathbf{x_{i-1}})}
         {\eta(x_i \mid \mathbf{x_{i-1}})}\cdot w(\mathbf{x_{i-1}}).\end{aligned}$$</span><br /></p>
<p>Thus, we can choose <span class="math">$\eta(x_i \mid \mathbf{x_{i-1}})$</span> such that the variance of the importance weights is as small as possible at every step, eventually arriving at a full importance distribution. This choice is made on a problem-specific basis, taking any available information about <span class="math">$\pi(x_i \mid
\mathbf{x_{i-1}})$</span> into account (see <em>e.g.</em> <span class="citation"></span> for many examples). One potential choice for <span class="math">$\eta(x_i \mid \mathbf{x_{i-1}})$</span> is simply <span class="math">$\pi(x_i \mid
\mathbf{x_{i-1}})$</span>, if it is possible to compute. In a Bayesian setting, the prior distribution may be used. The exact form of <span class="math">$\eta(x_i, \mathbf{x_{i-1}})$</span> which minimizes the variance of the weights is called the <em>optimal kernel</em> <span class="citation"></span>, the name deriving from the fact that <span class="math">$k(x_i, \mathbf{x_{i-1}}) = \eta(x_i, \mathbf{x_{i-1}})$</span> is a Markov kernel. In some applications, it is possible to approximate the optimal kernel or even compute it explicitly.</p>
<p>The recursive definition suggests an algorithm for obtaining a sample from <span class="math"><em>π</em></span> (). We begin with <span class="math"><em>n</em></span> “particles” which have been sampled from the importance distribution <span class="math"><em>η</em>(<em>x</em><sub>0</sub>)</span> for <span class="math"><em>π</em>(<em>x</em><sub>0</sub>)</span>. The particles are updated and reweighted <span class="math"><em>d</em></span> times, corresponding to the <span class="math"><em>d</em></span> elements of the decomposition of <span class="math"><em>π</em></span>. At the <span class="math"><em>i</em></span>th step, each particle is extended to include <span class="math"><em>x</em><sub><em>i</em></sub></span> drawn according to the chosen <span class="math">$\eta(x_i \mid
\mathbf{x_{i-1}})$</span>, and the importance weights are recalculated and normalized.</p>
<p>Sample <span class="math"><em>x</em><sub>1</sub><sup>(<em>k</em>)</sup></span> from <span class="math"><em>η</em>(<em>x</em><sub>1</sub>)</span> <span class="math">$w^{(k)} \gets \dfrac{\pi\left(x_1^{(k)}\right)}{\eta\left(x_1^{(k)}\right)}$</span> Sample <span class="math"><em>x</em><sub><em>i</em></sub><sup>(<em>k</em>)</sup></span> from <span class="math">$\eta\left(x_i \mid \mathbf{x_{i-1}^{(k)}}\right)$</span> Extend the <span class="math"><em>k</em></span>th particle <span class="math">$w(\mathbf{x_i}^{(k)}) \gets \dfrac{\pi\left(x_i^{(k)} \mid \mathbf{x_{i-1}^{(k)}}\right)}{\eta\left(x_i^{(k)} \mid \mathbf{x_{i-1}^{(k)}}\right)} \cdot w(\mathbf{x_{i-1}}^{(k)})$</span> Normalize the weights so that <span class="math">∑ <em>w</em> = 1</span> Sample <span class="math"><em>n</em></span> particles with probabilities <span class="math"><em>w</em></span></p>
<p>[alg:sis]</p>
<p>Of course, <span class="math"><em>η</em></span> is merely an approximation to <span class="math"><em>π</em></span>, and may be a fairly poor one depending on the application. Try as we might to keep the variances of the weights low, the cumulative errors at each sequential step tend to push many of the weights to very low values. This results in a poor approximation to <span class="math"><em>π</em></span>, since only a few particles retain high importance weights after all <span class="math"><em>d</em></span> sequential steps. To mitigate this problem, we periodically apply a resampling step when the variance in the importance weights becomes too high. Several different criteria have been proposed for when to resample, but we focus here on the one described by <span class="citation"></span>, namely the decay of the below a prescribed threshold, conventionally <span class="math"><em>n</em> / 2</span>. The of the population of particles is defined as <br /><span class="math">$$\ESS(w) = \frac{n}{1 + \Var(w)},$$</span><br /> where <span class="math"><em>n</em></span> is the number of particles <span class="citation"></span>. When the drops below the threshold, we resample the particles according to their weights. This results in the removal of low-weight particles from the population, and also equalizes all the weights. Various resampling strategies beyond the basic sampling with replacement have been proposed, but we will not discuss those here.</p>
<h3 id="subsec:smcsamp">The sequential Monte Carlo sampler</h3>
<p>The algorithm described above aims to sample from a high-dimensional distribution <span class="math"><em>π</em>(<em>x</em>)</span>, by sequentially sampling from <span class="math"><em>d</em></span> distributions of lower but increasing dimension. <span class="citation"></span> developed an <em> sampler</em> with an alternative objective: to sample sequentially from <span class="math"><em>d</em></span> distributions <span class="math"><em>π</em><sub>1</sub>, …, <em>π</em><sub><em>d</em></sub></span>, all of the same dimension and defined on the same space. The <span class="math"><em>π</em><sub><em>i</em></sub></span> are assumed to form a related sequence, such as posterior distributions attained by sequentially considering new evidence. As with , we assume that <span class="math"><em>π</em><sub><em>i</em></sub>(<em>x</em>) = <em>γ</em><sub><em>i</em></sub>(<em>x</em>) / <em>Z</em><sub><em>i</em></sub></span>, where <span class="math"><em>γ</em><sub><em>i</em></sub></span> is known pointwise and the normalizing constant <span class="math"><em>Z</em><sub><em>i</em></sub></span> is unknown.</p>
<p>Both algorithms involve progression through a sequence of related distributions. For , these distributions are lower-dimensional marginals of the target distribution, while for the sampler, they are of the same dimension and constitute a smooth progression from an initial to a final distribution. In both cases, the neighbouring distributions in the sequence are related to each other in some way, and we can take advantage of that relationship to create a sequence of importance distributions alongside the sequence of targets. In , the neighbouring marginals <span class="math"><em>π</em>(<strong>x_i</strong>)</span> and <span class="math">$\pi(\mathbf{x_{i+1}})$</span> were related by the conditional density <span class="math">$\pi(x_i \mid \mathbf{x_{i-1}})$</span>, which we used to inform the importance distribution. In , the relationship between subsequent distributions is less explicit, but it is assumed that they are related closely enough that an importance distribution for <span class="math"><em>π</em><sub><em>i</em></sub></span> can be easily transformed into one for <span class="math"><em>π</em><sub><em>i</em> + 1</sub></span>. In particular, the sequence of importance distributions <span class="math"><em>η</em><sub><em>i</em></sub></span> is constructed as</p>
<p><br /><span class="math">$$\begin{aligned}
  \label{eq:impint}
  \eta_i(x') = \int \eta_{i-1}(x) K_i(x, x') \mathrm{d}\,x,\end{aligned}$$</span><br /></p>
<p>where <span class="math"><em>K</em><sub><em>i</em></sub></span> is a Markov kernel and the integral is over the space on which the <span class="math"><em>π</em><sub><em>i</em></sub></span> are defined. The choice of <span class="math"><em>K</em><sub><em>i</em></sub></span> should be based on the percieved relationship between <span class="math"><em>π</em><sub><em>i</em> − 1</sub></span> and <span class="math"><em>π</em><sub><em>i</em></sub></span>. <span class="citation"></span> propose the use of a kernel with equilibrium distribution <span class="math"><em>π</em><sub><em>i</em></sub></span>. That is, <br /><span class="math">$$K_i(x, x') = \max\left(1, \frac{q(x', x)\pi_i(x)}{q(x, x')\pi_i(x')}\right),$$</span><br /> where <span class="math"><em>q</em>(<em>ξ</em>, <em>x</em>)</span> is a proposal function such as a Gaussian distribution centered at <span class="math"><em>ξ</em></span> (see ).</p>
<p>Although this method of building up <span class="math"><em>η</em></span> appears straightforward, the drawback is that the importance distribution itself becomes intractible. In particular, evaluating <span class="math"><em>η</em><sub><em>i</em></sub>(<em>x</em>)</span> involves a <span class="math"><em>i</em></span>-dimensional integral of the type in . As it is necessary to evaluate <span class="math"><em>η</em>(<em>x</em>)</span> pointwise to perform , this construction appears to have defeated the purpose of providing an importance distribution for each <span class="math"><em>π</em><sub><em>i</em></sub></span>. <span class="citation"></span> overcome this problem with two “artificial” objects. First, they propose the existence of <em>backward</em> Markov kernels <span class="math"><em>L</em><sub><em>i</em> − 1</sub>(<em>x</em><sub><em>i</em></sub>, <em>x</em><sub><em>i</em> − 1</sub>)</span>. For now, these kernels are arbitrary, and will be precisely defined on a problem-specific basis. Second, they define an alternative sequence of target distributions <br /><span class="math"><em>π̃</em><sub><em>i</em></sub>(<strong>x_i</strong>) = <em>π</em><sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub>)∏ <sub><em>k</em> = 1</sub><sup><em>i</em> − 1</sup><em>L</em><sub><em>k</em></sub>(<em>x</em><sub><em>k</em> + 1</sub>, <em>x</em><sub><em>k</em></sub>)</span><br /> of increasing dimension. This brings us back to the setting described above in , namely of building up an importance distribution of dimension <span class="math"><em>d</em></span> sequentially through lower-dimensional distributions. We can write <span class="math"><em>π̃</em><sub><em>i</em></sub></span> in terms of <span class="math"><em>π̃</em><sub><em>i</em> − 1</sub></span> by noticing that</p>
<p><br /><span class="math">$$\begin{aligned}
  \frac{\tilde{\pi}_i(\mathbf{x_i})}{\tilde{\pi}_{i-1}(\mathbf{x_{i-1}})} 
  = \frac{\pi_i(x_i) \prod_{k=1}^{i-1} L(x_{k+1}, x_k)}
  {\pi_{i-1}(x_{i-1}) \prod_{k=1}^{i-2} L(x_{k+1}, x_k)}
  = \frac{\pi_i(x_i) L(x_i, x_{i-1})}{\pi_{i-1}(x_{i-1})},\end{aligned}$$</span><br /></p>
<p>and hence <br /><span class="math">$$\tilde{\pi}_i = \frac{\pi_i(x_i) L(x_i, x_{i-1})}{\pi_{i-1}(x_{i-1})} \cdot \tilde{\pi}_{i-1}.$$</span><br /> Therefore, the importance weights for these new targets are defined recursively as</p>
<p><br /><span class="math">$$\begin{aligned}
  w(\mathbf{x_i}) 
    &amp;= \frac{\tilde{\pi}_i(\mathbf{x_i})}{\eta_i(\mathbf{x_i})} \\
    &amp;= \frac{\tilde{\pi}_{i-1}(\mathbf{x_{i-1}}) \pi_i(x_i) L(x_i, x_{i-1})}
           {\eta_{i-1}(\mathbf{x_{i-1}}) \pi_{i-1}(x_{i-1}) K_i(x_{i-1}, x_i)} \\
    &amp;= w(\mathbf{x_{i-1}}) \cdot
      \frac{\pi_i(x_i) L_{i-1}(x_i, x_{i-1})}
           {\pi_{i-1}(x_{i-1}) K_i(x_{i-1}, x_i)} \\
    &amp;\propto w(\mathbf{x_{i-1}}) \cdot
      \frac{\gamma_i(x_i) L_{i-1}(x_i, x_{i-1})}
           {\gamma_{i-1}(x_{i-1}) K_i(x_{i-1}, x_i)}.
    \label{eq:smcwt}\end{aligned}$$</span><br /></p>
<p>The final key piece of information is to notice that, because the <span class="math"><em>L</em><sub><em>i</em></sub></span> are Markov kernels, <span class="math"><em>π</em><sub><em>i</em></sub></span> is simply the marginal in <span class="math">$\mathbf{x_{i-1}}$</span> of <span class="math"><em>π̃</em></span>. Therefore, a sample from <span class="math"><em>π̃</em><sub><em>i</em></sub></span> automatically gets us a sample from <span class="math"><em>π</em><sub><em>i</em></sub></span>, by considering only the <span class="math"><em>i</em></span>th component of <span class="math"><strong>x_i</strong></span>. These are all the ingredients we need to apply . The sequences of kernels <span class="math"><em>L</em></span> and <span class="math"><em>K</em></span> should be chosen based on the problem at hand to inimize the variance in the importance weights as well as possible. For a fixed choice of <span class="math"><em>K</em><sub><em>i</em></sub></span>, the backward kernels <span class="math"><em>L</em><sub><em>i</em></sub></span> which minimize this variance are called the <em>optimal</em> backward kernels. The full sampler algorithm is presented as . A resampling step is applied whenever the of the population drops too low, as discussed in the previous section.</p>
<p>Sample <span class="math"><em>x</em><sub>1</sub><sup>(<em>k</em>)</sup></span> from <span class="math"><em>η</em><sub>1</sub>(<em>x</em><sub>1</sub>)</span> <span class="math">$w^{(k)} \gets \dfrac{\gamma_1\left(x_1^{(k)}\right)}{\eta_1\left(x_1^{(k)}\right)}$</span> Normalize the weights so that <span class="math">∑ <em>w</em> = 1</span> Sample <span class="math"><em>x</em><sub><em>i</em></sub><sup>(<em>k</em>)</sup></span> from <span class="math"><em>K</em>(<em>x</em><sub><em>i</em> − 1</sub><sup>(<em>k</em>)</sup>, <em>x</em><sub><em>i</em></sub>)</span> Extend the <span class="math"><em>k</em></span>th particle <span class="math">$w^{(k)} \gets w^{(k)} \cdot \dfrac{\gamma_i(x_i) L_{i-1}(x_i, x_{i-1})}{\gamma_{i-1}(x_{i-1}) K_i(x_{i-1}, x_i)}$</span> Normalize the weights so that <span class="math">∑ <em>w</em> = 1</span> Resample the particles according to <span class="math"><em>w</em></span> <span class="math"><em>w</em><sup>(<em>k</em>)</sup> ← 1 / <em>n</em></span> Sample the <span class="math"><em>i</em></span>th component of <span class="math"><em>n</em></span> particles with probabilities <span class="math"><em>w</em></span></p>
<p>[alg:smcsamp]</p>
<h2 id="sec:abc">Approximate Bayesian computation</h2>
<h3 id="subsec:mfit">Model fitting</h3>
<p>A <em>mathematical model</em> is a formal description of a hypothesized relationship between some observed data, <span class="math"><em>x</em></span> and outcomes <span class="math"><em>y</em></span>. A <em>parametric</em> model defines a family of possible relationships between data and outcomes, indexed by one or more numeric parameters <span class="math"><em>θ</em></span>. A <em>statistical</em> model describes the relationship between data and outcomes in terms of probabilities. Statistical models define, either explicitly or implicitly, the probability of observing <span class="math"><em>y</em></span> given <span class="math"><strong>x</strong></span> and, if the model is parametric, <span class="math"><em>θ</em></span>. Note that it is entirely possible to have no data <span class="math"><strong>x</strong></span>, only observed outcomes <span class="math"><em>y</em></span>. In this case, a model would describe the process by which <span class="math"><em>y</em></span> is generated.</p>
<p>To illustrate these concepts, consider the well-known linear model. For clarity, we will restrict our attention to the case of one-dimensional data and outcomes where <span class="math"><em>x</em> = {<em>x</em><sub>1</sub>, …, <em>x</em><sub><em>n</em></sub>}</span> and <span class="math"><em>y</em> = {<em>y</em><sub>1</sub>, …, <em>y</em><sub><em>n</em></sub>}</span> are vectors of real numbers. The linear model postulates that the outcomes are linearly related to the data, modulo some noise introduced by measurement error, environmental fluctuations, and other external factors. Formally, <span class="math"><em>y</em><sub><em>i</em></sub> = <em>β</em><em>x</em><sub><em>i</em></sub> + <em>ɛ</em><sub><em>i</em></sub></span>, where <span class="math"><em>β</em></span> is the slope of the linear relationship, and <span class="math"><em>ɛ</em><sub><em>i</em></sub></span> is the error associated with measurement <span class="math"><em>i</em></span>. We can make this model a statistical one by hypothesizing a distribution for the error terms <span class="math"><em>ɛ</em><sub><em>i</em></sub></span>; most commonly, it is assumed that they are normally distributed with variance <span class="math"><em>σ</em></span>. In mathematical terms, <span class="math"><em>Y</em><sub><em>i</em></sub> ∼ <em>β</em><em>x</em><sub><em>i</em></sub> + N(0, <em>σ</em><sup>2</sup>)</span>, where “<span class="math"> ∼ </span>” means “is distributed as”. We can see from this formulation that the model is parametric, with parameters <span class="math"><em>θ</em></span> = (<span class="math"><em>β</em></span>, <span class="math"><em>σ</em></span>). Moreover, we can write down the probability density <span class="math"><em>π</em></span> of observing outcome <span class="math"><em>y</em><sub><em>i</em></sub></span> given the parameters, <br /><span class="math"><em>π</em>(<em>y</em> ∣ <em>β</em>, <em>σ</em>) = ∏ <sub><em>i</em> = 1</sub><sup><em>n</em></sup><em>f</em><sub>N(0, <em>σ</em><sup>2</sup>)</sub>(<em>y</em><sub><em>i</em></sub> − <em>β</em><em>x</em><sub><em>i</em></sub>), </span><br /> where <span class="math"><em>f</em><sub>N(0, <em>σ</em><sup>2</sup>)</sub></span> is the probability density of the normal distribution with mean zero and variance <span class="math"><em>σ</em><sup>2</sup></span>. Note that we are treating the <span class="math"><em>x</em><sub><em>i</em></sub></span> as fixed quantities, and therefore have not conditioned the probability density on <span class="math"><strong>x</strong></span>. Also, we have assumed that all the <span class="math"><em>y</em><sub><em>i</em></sub></span> are independent.</p>
<p>For a general model, the probability density of <span class="math"><em>y</em></span> given the parameters <span class="math"><em>θ</em></span> is also known as the <em>likelihood</em>, written <span class="math">L</span>, of <span class="math"><em>θ</em></span>. That is, <span class="math">L(<em>θ</em> ∣ <em>y</em>) = <em>f</em>(<em>y</em> ∣ <em>θ</em>)</span> for the model’s <span class="math"><em>f</em></span>. The higher the value of the likelihood, the more likely the observations <span class="math"><em>y</em></span> are under the model. Thus, the likelihood provides a natural criterion for fitting the model parameters: we want to pick <span class="math"><em>θ</em></span> such that the probability density of our observed outcomes <span class="math"><em>y</em></span> is as high as possible. The parameters which optimize the likelihood are known as the <em></em> estimates, denoted <span class="math"><em>θ̂</em></span>. That is, <br /><span class="math">$$\hat{\theta} = \argmax_\theta\; \mathcal{L}(\theta \mid y).$$</span><br /> estimation is usually performed with numerical optimization. In the simplest terms, many possible values for <span class="math"><em>θ</em></span> are examined, <span class="math">L(<em>θ</em> ∣ <em>y</em>)</span> is calculated for each, and the parameters which produce the highest value are accepted. Many sophisticated numerical optimization methods exist, although they may not be guaranteed to find the true estimates if the likelihood function is complex.</p>
<p>estimation makes use only of the data and outcomes to estimate the model parameters <span class="math"><em>θ</em></span>. However, it is frequently the case that the investigator has some additional information or belief about what <span class="math"><em>θ</em></span> are likely to be. For example, in the linear regression case, the instrument used to measure the outcomes may have a well-known margin of error, or the sign of the slope may be obvious from previous experiments. The Bayesian approach to model fitting makes use of this information by codifying the investigator’s beliefs as a <em>prior distribution</em> on the parameters, denoted <span class="math"><em>π</em>(<em>θ</em>)</span>. Instead of considering only the likelihood, Bayesian inference focuses on the product of the likelihood and the prior, <span class="math"><em>f</em>(<em>y</em> ∣ <em>θ</em>)<em>π</em>(<em>θ</em>)</span>. Bayes’ theorem tells us that this product is related to the <em>posterior distribution</em> on <span class="math"><em>θ</em></span>,</p>
<p><br /><span class="math">$$\begin{aligned}
  f(\theta \mid y) 
    = \frac{f(y \mid \theta) \pi(\theta)}
           {\int f(y \mid \theta) \pi(\theta) \mathrm{d}\,\theta}.
  \label{eq:bayes}\end{aligned}$$</span><br /></p>
<p>In principle, <span class="math"><em>f</em>(<em>y</em> ∣ <em>θ</em>)<em>π</em>(<em>θ</em>)</span> can be optimized numerically just like <span class="math">L(<em>θ</em> ∣ <em>y</em>)</span>, which would also optimize the posterior distribution. The resulting optimal parameters are called the estimates. However, from a Bayesian perspective, <span class="math"><em>θ</em></span> is not a fixed quantity to be estimated, but rather a random variable with an associated distribution (the posterior). Therefore, the estimate by itself is of limited value without associated statistics about the posterior distribution, such as the mean or credible intervals. Unfortunately, to calculate such statistics, it is necessary to evaluate the normalizing constant in the denominator of , which is almost always an intractable integral.</p>
<p>A popular method for circumventing the normalizing constant is the use of to obtain a sample from the posterior distribution. works by defining a Markov chain whose states are indexed by possible model parameters. The transition probability from state <span class="math"><em>θ</em><sub>1</sub></span> to state <span class="math"><em>θ</em><sub>2</sub></span> is taken to be <br /><span class="math">$$\max\left(1, \frac{f(y \mid \theta_2) \pi(\theta_2) q(\theta_2, \theta_1)}
                    {f(y \mid \theta_1) \pi(\theta_2) q(\theta_1, \theta_2)} \right),$$</span><br /> where <span class="math"><em>q</em>(<em>θ</em>, <em>θ</em>ʹ)</span> is a symmetric <em>proposal distribution</em> used in the algorithm to generate the chain. The stationary distribution of this Markov chain is equal to the posterior distribution on <span class="math"><em>θ</em></span>. Therefore, if a long enough random walk is performed on the chain, the distribution of states visited will be a Monte Carlo approximation of <span class="math"><em>f</em>(<em>θ</em> ∣ <em>y</em>)</span>, from which we can calculate statistics of interest. Actually performing this random walk is straightforward and can be accomplished via the Metropolis-Hastings algorithm ().</p>
<p>Draw <span class="math"><em>θ</em></span> according to the prior <span class="math"><em>π</em>(<em>θ</em>)</span> Propose <span class="math"><em>θ</em>ʹ</span> according to <span class="math"><em>q</em>(<em>θ</em>, <em>θ</em>ʹ)</span> Accept <span class="math"><em>θ</em> ← <em>θ</em>ʹ</span> with probability <span class="math">$\max \left( 1, 
       \dfrac{f(y \mid \theta') \pi(\theta') q(\theta', \theta)}
             {f(y \mid \theta\phantom{'}) \pi(\theta\phantom{'}) q(\theta, \theta')}
       \right)$</span></p>
<p>[alg:mh]</p>
<h3 id="subsec:abcoverview">Overview of ABC</h3>
<p>Most mathematical models are amenable to fitting via one or both of the approaches, or Bayesian inference, discussed above. However, there are some, particularly in the domain of population genetics <span class="citation"></span>, for which calculation of either the likelihood or the product of the likelihood and the prior may be infeasible. For example, one or both of these quantities may be expressible only as an intractable integral. is designed for such cases, where standard likelihood-based techniques for model fitting cannot be applied.</p>
<p>Ordinarily, Bayesian inference targets the posterior distribution <span class="math"><em>f</em>(<em>θ</em> ∣ <em>y</em>)</span>. That is, in the Bayesian framework, model parameters with higher posterior density are “better” in the sense that they offer a more credible explanation for the observed data. Approximate Bayesian computation offers an alternative metric for parameter credibility, namely the similarity of simulated datasets to the observed data. If datasets simulated under the model closely resemble the real data, it follows that the model is a reasonable approximation to the real-world process generating the observed data. More formally, suppose we have a distance measure <span class="math"><em>ρ</em></span> defined on the space of all possible data our model could generate. aims to sample from the joint posterior distribution of model parameters and simulated datasets <span class="math"><em>z</em></span> which are within some small distance <span class="math"><em>ɛ</em></span> of the observed data <span class="math"><em>y</em></span>, <br /><span class="math">$$\pi_{\varepsilon}(\theta, z \mid y) =
  \frac{\pi(\theta) f(z \mid \theta) \mathbb{I}_{A_{\varepsilon, y}} (z)}
  {\int_{A_{\varepsilon, y} \times \Theta} \pi(\theta) f(z \mid \theta) \mathrm{d}\,\theta}.$$</span><br /> Here, <span class="math"><em>A</em><sub><em>ɛ</em>, <em>y</em></sub></span> is an <span class="math"><em>ɛ</em></span>-ball around <span class="math"><em>y</em></span> with respect to <span class="math"><em>ρ</em></span>, <span class="math">Θ </span> is the space of all possible model parameters, and <span class="math">I</span> is the indicator function <span class="citation"></span>. As we shall see in the next section, this distribution can be sampled from exactly. The word “approximate” derives from the assumption that, for a suitably chosen distance <span class="math"><em>ρ</em></span> and a small enough <span class="math"><em>ɛ</em></span>, the marginal in <span class="math"><em>z</em></span> of this distribution approximates the posterior of interest <span class="citation"></span>. That is, <br /><span class="math">∫ <em>π</em><sub><em>ɛ</em></sub>(<em>θ</em>, <em>z</em> ∣ <em>y</em>)d <em>z</em> ≈ <em>f</em>(<em>θ</em> ∣ <em>y</em>). </span><br /> This distribution is variously referred to as the <em> target distribution</em> or the approximation to the posterior. Note that in many formulations, the distance function <span class="math"><em>ρ</em></span> is defined as <span class="math"><em>ρ</em>(<em>S</em>( ⋅ ), <em>S</em>( ⋅ ))</span> where <span class="math"><em>S</em></span> is a function which maps data points into a vector of summary statistics. This can be useful if the data are high-dimensional or of a complex type, but it is not strictly necessary. For instance, if the data are numeric and of low dimension, the distance function may simply be the Euclidian distance <span class="citation"></span>. For more complex data, <span class="citation"></span> proposed the use of a kernel function (defined in ), an approach they dubbed <em>kernel-</em>.</p>
<h3 id="subsec:abcalg">Algorithms for ABC</h3>
<p>Algorithms for performing fall into one of three categories: rejection, , and . To simplify the math, we shall restrict the descriptions of these algorithms to the case of one simulated dataset per parameter particle (the meaning of this will become clear shortly). The extension to multiple datasets per particle is straightforward and will be given at the end of the section. We use the variable <span class="math"><em>x</em></span> to refer to the pair <span class="math">(<em>θ</em>, <em>z</em>)</span>, so that the target distribution can be written <span class="math"><em>π</em><sub><em>ɛ</em></sub>(<em>x</em> ∣ <em>y</em>)</span>.</p>
<p>Rejection ABC is the simplest method, and also the one which was first proposed <span class="citation"></span>. The algorithm, outlined in , repeats the following steps until a desired number of samples from the target distribution are obtained. Parameter values <span class="math"><em>θ</em></span> are sampled according to the prior distribution <span class="math"><em>π</em>(<em>θ</em>)</span>. Then, a simulated dataset <span class="math"><em>z</em></span> is generated from the model with the sampled parameter values. By definition, the probability density of obtaining the particular dataset <span class="math"><em>z</em></span> is <span class="math"><em>f</em>(<em>z</em> ∣ <em>θ</em>)</span>. Finally, the parameters are sampled if the distance of <span class="math"><em>z</em></span> from the observed data <span class="math"><em>y</em></span> is less than <span class="math"><em>ɛ</em></span>, that is, with probability <span class="math">I<sub><em>A</em><sub><em>ɛ</em>, <em>y</em></sub></sub>(<em>z</em>)</span>. Putting this all together, the parameters <span class="math"><em>θ</em></span> are sampled with probability proportional to <br /><span class="math"><em>π</em>(<em>θ</em>)<em>f</em>(<em>z</em> ∣ <em>θ</em>)I<sub><em>A</em><sub><em>ɛ</em>, <em>y</em></sub></sub>(<em>z</em>), </span><br /> which is exactly the numerator of the target distribution. Thus, <span class="math"><em>θ</em></span> represents an unbiased sample from the approximate posterior.</p>
<p>Draw <span class="math"><em>θ</em></span> according to <span class="math"><em>π</em>(<em>θ</em>)</span> Simulate a dataset <span class="math"><em>z</em></span> from the model with parameters <span class="math"><em>θ</em></span> Sample <span class="math"><em>θ</em></span></p>
<p>[alg:abcrej]</p>
<p>Rejection is easy to understand and implement, but it is not generally computationally feasible. If the posterior is very different from the prior, a very large number of samples may need to be taken in order to find a simulated dataset which is close to <span class="math"><em>z</em></span>. The inefficiency is compounded by the curse of dimensionality - the measure of the <span class="math"><em>ɛ</em></span>-ball around <span class="math"><em>y</em></span> decreases exponentially with the number of dimensions. - () was designed to overcome these hurdles <span class="citation"></span>. The approach is similar to ordinary Bayesian (), except that a distance cutoff replaces the likelihood ratio. That is, the transition probability between states <span class="math"><em>x</em></span> and <span class="math"><em>x</em>ʹ</span> is defined as <br /><span class="math">$$\max\left(1, \frac{f(z' \mid \theta') q(\theta', \theta)}
                    {f(z \mid \theta) q(\theta, \theta')} 
    \cdot \mathbb{I}_{A_{\varepsilon, y}}(z') \right).$$</span><br /></p>
<p>Draw <span class="math"><em>θ</em></span> according to <span class="math"><em>π</em>(<em>θ</em>)</span> Propose <span class="math"><em>θ</em>ʹ</span> according to <span class="math"><em>q</em>(<em>θ</em>, <em>θ</em>ʹ)</span> Simulate a dataset <span class="math"><strong>z'</strong></span> according to the model with parameters <span class="math"><em>θ</em></span> Accept <span class="math"><em>θ</em> ← <em>θ</em>ʹ</span> with probability <span class="math">$\max \left( 1, 
       \dfrac{\pi(\theta') q(\theta', \theta)}
             {\pi(\theta\phantom{'}) q(\theta, \theta')} 
       \cdot \mathbb{I}_{A_{\varepsilon, y}}(z') \right)$</span></p>
<p>[alg:abcmcmc]</p>
<p>Some of the same computational inefficiencies arise with - as with rejection. For example, in regions of low posterior density, the probability to simulate a dataset proximal to the observed data is low. Various strategies have been developed to mitigate this, including reducing the tolerance level <span class="math"><em>ɛ</em></span> as the chain progresses <span class="citation"></span>.</p>
<p>The most recently developed class of algorithm for is - <span class="citation"></span>. As with -, the algorithm is a straightforward modification of an existing Bayesian inference method, in this case the sampler (). The sequence of target distributions is defined as <span class="math"><em>π</em><sub><em>i</em></sub> = <em>π</em><sub><em>ɛ</em><sub><em>i</em></sub></sub>(<em>x</em> ∣ <em>y</em>)</span> for a decreasing sequence of tolerances <span class="math"><em>ɛ</em><sub><em>i</em></sub></span>. The intention is for the algorithm to progress smoothly through a sequence of target distributions which ends at the approximation to the posterior. As discussed in , the choices of the kernels <span class="math"><em>K</em></span> and <span class="math"><em>L</em></span> is problem-specific, and so appropriate kernels must be chosen for . Several options have been proposed <span class="citation"></span>.</p>
<p>All the algorithms discussed in this section can be straightforwardly extended to sample from the joint distribution <br /><span class="math"><em>π</em><sub><em>ɛ</em></sub>(<em>θ</em>, <em>z</em><sub>1</sub>, …, <em>z</em><sub><em>M</em></sub> ∣ <em>y</em>), </span><br /> which is equivalent to associating <span class="math"><em>M</em></span> simulated datasets to each parameter particle instead of just one. The simulated dataset <span class="math"><em>z</em></span> is replaced by <span class="math"><em>z</em> = <em>z</em><sub>1</sub>, …, <em>z</em><sub><em>M</em></sub></span>, and the indicator function for the <span class="math"><em>ɛ</em></span>-ball around <span class="math"><em>y</em></span> is replaced by <br /><span class="math">∑ <sub><em>k</em> = 1</sub><sup><em>M</em></sup>I<sub><em>A</em><sub><em>ɛ</em>, <em>y</em></sub></sub>(<em>z</em><sub><em>i</em></sub>). </span><br /> For - and -, the proposal distribution <span class="math"><em>q</em>(<em>θ</em>, <em>θ</em>ʹ)<em>f</em>(<em>z</em> ∣ <em>θ</em>ʹ)</span> is replaced by <br /><span class="math"><em>q</em><sub><em>i</em></sub>(<em>θ</em>, <em>θ</em>ʹ)∏ <sub><em>k</em> = 1</sub><sup><em>M</em></sup><em>f</em>(<em>z</em><sub><em>i</em></sub> ∣ <em>θ</em>ʹ). </span><br /></p>
<h1 id="reconstructing-contact-network-parameters-from-viral-phylogenies">Reconstructing contact network parameters from viral phylogenies</h1>
<h2 id="methods">Methods</h2>
<h3 id="netabc-a-computer-program-for-estimation-of-contact-network-parameters-with-kernel-abc"><em>Netabc</em>: a computer program for estimation of contact network parameters with kernel-ABC</h3>
<p><em>Netabc</em> is a computer program to perform statistical inference of contact network parameters from an estimated transmission tree using kernel-. The program combines three major components: Gillespie simulation, to simulate transmission trees on contact networks; the tree kernel, to compare simulated to observed transmission trees; and adaptive -, to maintain a population of particles and advance it toward the target distribution. We give a high-level overview of the program here, before describing these three components in detail.</p>
<p>As described in , <em>netabc</em> keeps track of a population of particles <span class="math"><em>x</em><sup>(<em>k</em>)</sup></span>, each of which contains particular parameter values <span class="math"><em>θ</em><sup>(<em>k</em>)</sup></span> for the model we are trying to fit. A small number of contact networks <span class="math"><em>z</em><sup>(<em>k</em>)</sup></span> are generated for each particle, in accordance with that particle’s parameters. An epidemic is simulated over each of these networks using Gillespie simulation, and by keeping track of its progress, a transmission tree is obtained. Thus, each particle becomes associated with several simulated transmission trees. These trees are compared to the observed tree using the tree kernel. Particles are weighted according to the similarity of their associated simulated trees with the true tree, with more similar trees receiving higher weights. The particles are iteratively perturbed to explore the parameter space, and particles with simulated trees too distant from the true tree are periodically dropped and resampled. Once a convergence criterion is attained, the final set of particles is used as a Monte Carlo approximation to the target distribution of , which is assumed to resemble the posterior distribution on model parameters (see ). A graphical schematic of this algorithm is given in .</p>
<p><embed src="abc-smc.pdf" /> [fig:abcsmc]</p>
<p><em>Netabc</em> is written in the <em>C</em> programming language. The <em>igraph</em> library <span class="citation"></span> is used to generate and store contact networks and phylogenies. Judy arrays <span class="citation"></span> are used for hash tables and dynamic programming matrices. The  <span class="citation"></span> is used to generate random draws from probability distributions, and to perform the bisection step in the adaptive - algorithm. Parallelization is implemented with POSIX threads <span class="citation"></span>. In addition to the <em>netabc</em> binary to perform kernel-, we provide three additional stand-alone utilities: <em>treekernel</em>, to calculate the tree kernel; <em>nettree</em>, to simulate a transmission tree over a contact network; and <em>treestat</em>, to compute various summary statistics of phylogenies. The programs are freely available at <a href="https://github.com/rmcclosk/netabc">https://github.com/rmcclosk/netabc</a>.</p>
<h4 id="subsubsec:nettree" class="unnumbered">Epidemic simulation</h4>
<p>The simulation of epidemics, and the corresponding transmission, trees over contact networks is performed in <em>netabc</em> using the Gillespie simulation algorithm <span class="citation"></span>. This method has been independently implemented and applied by several authors <span class="citation"></span>. <span class="citation"></span> published their implementation as an <em>R</em> package, but since the algorithm is quite computationally intensive, we chose to implement our own version in <em>C</em>.</p>
<p>Let <span class="math"><em>G</em> = (<em>V</em>, <em>E</em>)</span> be a directed contact network. We assume the individual nodes and edges of <span class="math"><em>G</em></span> follow the dynamics of the model <span class="citation"></span>. Each directed edge <span class="math"><em>e</em> = (<em>u</em>, <em>v</em>)</span> in the network is associated with a transmission rate <span class="math"><em>β</em><sub><em>e</em></sub></span>, which indicates that, once <span class="math"><em>u</em></span> becomes infected, the waiting time until <span class="math"><em>u</em></span> infects <span class="math"><em>v</em></span> is distributed as <span class="math">$\Exponential(\beta_e)$</span>. Note that <span class="math"><em>v</em></span> may become infected before this time has elapsed, if <span class="math"><em>v</em></span> has other incoming edges. <span class="math"><em>v</em></span> also has a removal rate <span class="math"><em>γ</em><sub><em>v</em></sub></span>, so that the waiting time until removal of <span class="math"><em>v</em></span> from the population is <span class="math">$\Exponential(\gamma_v)$</span>. Removal may correspond to death or recovery with immunity, or a combination of both, but in our implementation recovered nodes never re-enter the susceptible population. We define a <em>discordant edge</em> as an edge <span class="math">(<em>u</em>, <em>v</em>)</span> where <span class="math"><em>u</em></span> is infected and <span class="math"><em>v</em></span> has never been infected.</p>
<p>To describe the algorithm, we introduce some notation and variables. Let <span class="math">$\inc(v)$</span> be the set of incoming edges to <span class="math"><em>v</em></span>, and <span class="math">$\out(v)$</span> be the set of outgoing edges from <span class="math"><em>v</em></span>. Let <span class="math"><em>I</em></span> be the set of infected nodes in the network, <span class="math"><em>R</em></span> be the set of removed nodes, and <span class="math"><em>S</em></span> be the remaining susceptible nodes, and <span class="math"><em>D</em></span> be the set of discordant edges in the network. Let <span class="math"><em>β</em></span> be the total transmission rate over all discordant edges, and <span class="math"><em>γ</em></span> be the total removal rate of all infected nodes, <br /><span class="math"><em>β</em> = ∑ <sub><em>e</em> ∈ <em>D</em></sub><em>β</em><sub><em>e</em></sub>,  <em>γ</em> = ∑ <sub><em>v</em> ∈ <em>I</em></sub><em>γ</em><sub><em>v</em></sub>. </span><br /> The variables <span class="math"><em>S</em></span>, <span class="math"><em>I</em></span>, <span class="math"><em>R</em></span>, <span class="math"><em>D</em></span>, <span class="math"><em>β</em></span>, and <span class="math"><em>γ</em></span> are all updated as the simulation progresses. When a node <span class="math"><em>v</em></span> becomes infected, it is deleted from <span class="math"><em>S</em></span> and added to <span class="math"><em>I</em></span>. Any formerly discordant edges in <span class="math"> ∈ (<em>v</em>)</span> are deleted from <span class="math"><em>D</em></span>, and edges in <span class="math">$\out(v)$</span> to nodes in <span class="math"><em>S</em></span> are added to <span class="math"><em>D</em></span>. If <span class="math"><em>v</em></span> is later removed, it is deleted from <span class="math"><em>I</em></span> and added to <span class="math"><em>R</em></span>, and any discordant edges in <span class="math">$\out(v)$</span> are deleted from <span class="math"><em>D</em></span>. At the time of either infection or removal, the variables <span class="math"><em>β</em></span> and <span class="math"><em>γ</em></span> are updated to reflect the changes in the network. Since these updates are straightforward, we do not write them explicitly in the algorithm.</p>
<p>The Gillespie simulation algorithm is given as Algorithm [alg:nettree]. The transmission tree <span class="math"><em>T</em></span> is simulated along with the epidemic. We keep a map called <span class="math"><em>tip</em></span>, which maps infected nodes in <span class="math"><em>I</em></span> to the tips of <span class="math"><em>T</em></span>. The simulation continues until either there are no discordant edges left in the network, or we reach a user-defined cutoff of time (<span class="math"><em>t</em><sub>max</sub></span>) or number of infections (<span class="math"><em>I</em><sub>max</sub></span>). We use the notation <span class="math">$\Uniform(0, 1)$</span> to indicate a number drawn from a uniform distribution on <span class="math">(0, 1)</span>, and likewise for <span class="math">$\Exponential(\lambda)$</span>. The combined number of internal nodes and tips in <span class="math"><em>T</em></span> is denoted <span class="math">∣<em>T</em>∣</span>.</p>
<p>[alg:nettree]</p>
<p>infect a node <span class="math"><em>v</em></span> at random, updating <span class="math"><em>S</em></span>, <span class="math"><em>I</em></span>, <span class="math"><em>D</em></span>, <span class="math"><em>β</em></span> and <span class="math"><em>γ</em></span> <span class="math"><em>T</em> ← </span> a single node with label <span class="math">1</span> <span class="math"><em>tip</em>[<em>v</em>] ← 1</span> <span class="math"><em>t</em> ← 0</span> <span class="math">$s \gets \min(t_{\max} - t, \Exponential(\beta + \gamma))$</span> <span class="math"><em>t</em> ← <em>t</em> + <em>s</em></span> choose an edge <span class="math"><em>e</em> = (<em>u</em>, <em>v</em>)</span> from <span class="math"><em>D</em></span> with probability <span class="math"><em>β</em><sub><em>e</em></sub> / <em>β</em></span> and infect <span class="math"><em>v</em></span> add tips with labels <span class="math">(∣<em>T</em>∣ + 1)</span> and <span class="math">(∣<em>T</em>∣ + 2)</span> to <span class="math"><em>T</em></span> connect the new nodes to <span class="math"><em>tip</em>[<em>v</em>]</span> in <span class="math"><em>T</em></span>, with branch lengths <span class="math">0</span> <span class="math"><em>tip</em>[<em>v</em>] ← ∣<em>T</em>∣ − 1</span> <span class="math"><em>tip</em>[<em>u</em>] ← ∣<em>T</em>∣</span> choose a node <span class="math"><em>v</em></span> from <span class="math"><em>I</em></span> with probability <span class="math"><em>γ</em><sub><em>v</em></sub> / <em>γ</em></span> and remove <span class="math"><em>v</em></span> delete <span class="math"><em>v</em></span> from <span class="math"><em>tip</em></span> update <span class="math"><em>S</em></span>, <span class="math"><em>I</em></span>, <span class="math"><em>R</em></span>, <span class="math"><em>D</em></span>, <span class="math"><em>β</em></span>, and <span class="math"><em>γ</em></span></p>
<h4 id="phylogenetic-kernel">Phylogenetic kernel</h4>
<p>The tree kernel developed by <span class="citation"></span> provides a comprehensive similarity score between two phylogenetic trees, via the dot-product of the two trees’ feature vectors in the infinite-dimensional space of all possible subset trees with branch lengths (see ). The kernel was implemented using the fast algorithm developed by <span class="citation"></span>. First, the production rule of each node, which is the total number of children and the number of leaf children, is recorded. The nodes of both trees are ordered by production rule, and a list of pairs of nodes sharing the same production rule is created. These are the nodes for which the value of the tree kernel must be computed - all other pairs have a value of zero. The pairs to be compared are then re-ordered so that the child nodes are always evaluated before their parents. Due to its recursive definition, ordering the pairs in this way allows the tree kernel to be computed by dynamic programming. The complexity of this implementation is <span class="math"><em>O</em>(∣<em>T</em><sub>1</sub>∣∣<em>T</em><sub>2</sub>∣)</span>, where <span class="math">∣<em>T</em>∣</span> counts the number of nodes in the tree <span class="math"><em>T</em></span>.</p>
<p>The tree kernel cannot be used directly as a distance measure for , since it is maximized, not minimized, when the two trees being compared are the same. Therefore, we defined the distance between two trees as <br /><span class="math">$$\rho(T_1, T_2) = 1 - \frac{K(T_1, T_2)}{\sqrt{K(T_1, T_1) K(T_2, T_2)}},$$</span><br /> which is a number between 0 and 1 minimized when <span class="math"><em>T</em><sub>1</sub> = <em>T</em><sub>2</sub></span>. This is similar to the normalization used by <span class="citation"></span>.</p>
<h4 id="subsubsec:adaptsmc" class="unnumbered">Adaptive sequential Monte Carlo for Approximate Bayesian computation</h4>
<p>We implemented the adaptive algorithm for developed by <span class="citation"></span>. This algorithm is similar to the reference - algorithm described in , except that the sequence of tolerances <span class="math"><em>ɛ</em><sub><em>i</em></sub></span> is automatically determined rather than specified in advance. The tolerances are chosen such that the of the particle population, which indicates the quality of the Monte Carlo approximation (see ), decays at a controlled rate. A sudden precipitous drop in would indicate that only a small number of particles had non-zero importance weights, which would result in a very poor Monte Carlo approximation to the target distribution. This situation is referred to as the “collapse” of the approximation, and is mitigated by the adaptive approach. A single parameter <span class="math"><em>α</em></span> (not to be confused with the model parameter) controls the decay rate, with <span class="math"><em>ɛ</em><sub><em>i</em></sub></span> being chosen to satisfy <br /><span class="math">$$\ESS(w_i) = \alpha \ESS(w_{i-1}).$$</span><br /> Here, <span class="math"><em>w</em><sub><em>i</em></sub></span> is the vector of weights at the <span class="math"><em>i</em></span>th step. Note that, since <span class="math"><em>w</em><sub><em>i</em></sub></span> depends on <span class="math"><em>ɛ</em><sub><em>i</em></sub></span>, this equation solves for the updated weights and the updated tolerance simultaneously. As pointed out by <span class="citation"></span>, the equation has no analytic solution, but can be solved numerically by bisection. The forward kernels <span class="math"><em>K</em><sub><em>i</em></sub></span> are taken to be kernels with stationary distributions <span class="math"><em>π</em><sub><em>ɛ</em><sub><em>i</em></sub></sub></span> and proposal distributions <br /><span class="math"><em>q</em><sub><em>i</em></sub>(<em>θ</em>, <em>θ</em>ʹ)∏ <sub><em>k</em> = 1</sub><sup><em>M</em></sup>Pr(<em>z</em><sub><em>i</em></sub><sup>(<em>k</em>)ʹ</sup> ∣ <em>θ</em>ʹ), </span><br /> where <span class="math"><em>θ</em></span> is the vector of model parameters and <span class="math"><em>z</em><sub><em>k</em></sub></span> are <span class="math"><em>M</em></span> datasets simulated according to <span class="math"><em>θ</em>ʹ</span>. In our implementation, <span class="math"><em>q</em></span> is either a Gaussian proposal for continuous parameters, or a Poisson proposal for discrete parameters. For the Poisson proposals, the number of steps to move the particle is drawn from a Poisson distribution, and the direction in which to move the particle is chosen uniformly at random. For both proposals, the variance was set equal to twice the empirical variance of the particles, following <span class="citation"></span>. The backwards kernels are <br /><span class="math">$$L_{i-1}(x', x) = \frac{\pi_n(x)K(x, x')}{\pi_n(x')}.$$</span><br /> When substituted into , the forward kernels <span class="math"><em>K</em>(<em>x</em>, <em>x</em>ʹ)</span> and densities <span class="math"><em>π</em><sub><em>n</em></sub>(<em>x</em>ʹ) = <em>π</em><sub><em>ɛ</em><sub><em>n</em></sub></sub>(<em>x</em>ʹ)</span> cancel out, and we are left with the weight update</p>
<p><br /><span class="math">$$\begin{aligned}
  w_i(x) 
    &amp;\propto w_{i-1}(x) \frac{\pi_n(x \mid y)}{\pi_{i-1}(x \mid y)} \\
    &amp;= w_{i-1}(x) \frac{\pi(x) \pi_i(y \mid x)}{\pi(x) \pi_{i-1}(y \mid x)} \\
    &amp;= w_{i-1}(x) \frac{\sum_{k=i}^M \mathbb{I}_{A_{\varepsilon_i, y}}(z_k)}
            {\sum_{k=i}^M \mathbb{I}_{A_{\varepsilon_{i-1}, y}}(z_k)}.\end{aligned}$$</span><br /></p>
<p>In other words, when the distance threshold <span class="math"><em>ɛ</em><sub><em>i</em> − 1</sub></span> is contracted to <span class="math"><em>ɛ</em><sub><em>i</em></sub></span>, the particles’ weights are multiplied by the proportion of simulated datasets which are still inside the new threshold. The algorithm may be stopped when one of two termination conditions is reached. The user may specify a final tolerance <span class="math"><em>ɛ</em></span>, or a final acceptance rate of the kernel. The latter condition stops the algorithm when the particles are not moving around very much, implying little change in the estimated target.</p>
<h3 id="analysis-of-model">Analysis of model</h3>
<p>We investigated four parameters related to the model, denoted , , , . The first three of these are parameters of the model itself, while is related to the simulation of transmission trees over the network. However, we will refer to all four as parameters. denotes the total number of nodes in the network, or equivalently, susceptible individuals in the population. When a node is added to the network, new undirected edges are added incident to it, and are attached to existing nodes of degree <span class="math"><em>k</em></span> with probability proportional to <span class="math"><em>k</em><sup><em>α</em></sup> + 1</span> (). To simulate transmission trees over a network, we allowed an epidemic to spread until nodes were infected, and sampled a transmission tree at that time. We assumed that all contacts had symmetric transmission risk, which was implemented by replacing each undirected edge in the network with two directed edges (one in each direction).</p>
<p>Nodes in our networks followed simple dynamics, meaning that they became infected at a rate proportional to their number of infected neighbours, and never recovered. We did not consider the time scale of the transmission trees in these simulations, only their shape. Therefore, the transmission rate along each edge in the network was set to 1, the removal rate of each node was set to 0, and all transmission trees’ branch lengths were scaled by their mean. For all analyses, the transmission trees’ branch lengths were scaled by dividing by their mean. We used the <em>igraph</em> library’s implementation of the BA model <span class="citation"></span> to generate the graphs. The analyses were run on Westgrid (<a href="https://www.westgrid.ca/">https://www.westgrid.ca/</a>) and a local computer cluster.</p>
<h4 id="subsec:kernel" class="unnumbered">Kernel classifiers</h4>
<p>The experiments presented here involved a large number of variables which were varied combinatorially. For ease of exposition, we will describe a single experiment first, then enumerate the values of all variables for which the experiment was repeated. The parameters of the tree kernel, <span class="math"><em>λ</em></span> and <span class="math"><em>σ</em></span> () will be referred to as <em>meta-parameters</em> to distinguish them from the parameters of the model. With the exception of our own programs, all analyses were done in <em>R</em>, and all packages listed below are <em>R</em> packages.</p>
<p>The attachment power parameter was varied among three values: 0.5, 1.0, and 1.5. For each value, the <em>sample_pa</em> function in the <em>igraph</em> package was used to simulate 100 networks, with the other parameters set to = 5000 and = 2. This step yielded a total of 300 networks. An epidemic was simulated on each network using our <em>nettree</em> binary until = 1000 nodes were infected, at which point 500 of them were sampled to form a transmission tree. A total of 300 transmission trees were thus obtained, comprised of 100 trees for each of the three values of . The trees were “ladderized” so that the subtree descending from the left child of each node was not smaller than that descending from the right child. Summary statistics, such as Sackin’s index and the ratio of internal to terminal branch lengths, were computed for each simulated tree using our <em>treestat</em> binary. The trees were visualized using the <em>ape</em> package <span class="citation"></span>. Our <em>treekernel</em> binary was used to calculate the value of the kernel for each pair of trees, with the meta-parameters set to <span class="math"><em>λ</em> = 0. 3</span> and <span class="math"><em>σ</em> = 4</span>. These values were stored in a symmetric 300 <span class="math"> × </span> 300 kernel matrix. Similarly, we computed the statistic between each pair of trees using our <em>treestat</em> binary, and stored them in a second <span class="math">300 × 300</span> matrix.</p>
<p>To investigate the effect of on tree shape, we constructed classifiers for based on three statistics. First, we used the <em>kernlab</em> package <span class="citation"></span> to create a classifier using the computed kernel matrix. Second, we used the <em>e1071</em> package <span class="citation"></span> to create an ordinary classifier using the pairwise matrix. Finally, we performed an ordinary linear regression of against Sackin’s index. Each of these classifiers was evaluated with 1000 two-fold cross-validations. We also performed a projection of the kernel matrix, and used it to visualize the separation of the different values in the tree kernel’s feature space. A schematic of this experiment is presented in .</p>
<p>Similar experiments were performed with the values shown in . The other three parameters, namely , , and , were each varied while holding the others fixed. The experiments for , , and were repeated with three different values of . All experiments were repeated with trees having three different numbers of tips. Kernel matrices were computed for all pairs of the meta-parameters = {0.2, 0.3, 0.4} and = {18, 14, 12, 1, 2, 4, 8}.</p>
<p>[ht]</p>
<p>[tab:kernelexpt]</p>
<p>[ht]</p>
<p>[tab:gridexpt]</p>
<p><embed src="kernel-expt.pdf" /> [fig:kernelexpt]</p>
<h4 id="grid-search" class="unnumbered">Grid search</h4>
<p>As in the previous section, we will begin by describing a single experiment, and then list the variables for which similar experiments were performed. We varied along a narrowly spaced grid of values: 0, 0.01, …, 2. For each value, fifteen networks were generated with <em>igraph</em>, and transmission trees were simulated over each using <em>nettree</em>. These trees will be referred to as “grid trees”, and their associated values “grid values”. Next, one further test tree was simulated with the test value = 0. Both the grid trees and the test tree had 500 tips, and were simulated with the other parameters set to <span class="math"><em>N</em></span> = 5000, <span class="math"><em>m</em></span> = 2, and <span class="math"><em>I</em></span> = 1000. The test tree was compared to each of the grid trees using the tree kernel, with the meta-parameters set to <span class="math"><em>λ</em> = 0. 3</span> and <span class="math"><em>σ</em> = 4</span>, using the <em>treekernel</em> binary. The median kernel score was calculated for each grid value, and the scores were normalized such that the area under the curve was equal to 1. The grid value with the highest median kernel score was taken as the point estimate for the test value, and a 95% credible interval was obtained using the <em>hpd</em> function in the <em>TeachingDemos</em> package.</p>
<p>Each experiment of the type just described was repeated ten times with the same test value. Similar experiments were performed for each of the four parameters, with several test values and trees of varying sizes. The variables are listed in . A graphical schematic of the grid search experiments is shown in .</p>
<p><img src="gridsearch-expt" alt="image" /> [fig:gridexpt]</p>
<h4 id="approximate-bayesian-computation" class="unnumbered">Approximate Bayesian computation</h4>
<p>To test the full kernel- algorithm, we simulated three trees each under a variety of parameter values, and ran the <em>netabc</em> program to estimate posterior distributions for the parameters. The parameter values and priors used are listed in . The tree kernel meta-parameters were set to <span class="math"><em>λ</em> = 0. 3</span> and <span class="math"><em>σ</em> = 4</span>. The algorithm was run with 1000 particles, five sampled datasets per particle, and the <span class="math"><em>α</em></span> parameter (not to be confused with the preferential attachment parameter, see ) set to 0.95. The algorithm was stopped when the acceptance rate of the kernel dropped below 1.5%, the same criterion used by <span class="citation"></span>. Approximate marginal posterior densities for each parameter were calculated using the <em>density</em> function in <em>R</em> applied to the final weighted population of particles. Credible intervals were obtained for each parameter using the <em>HPDinterval</em> function in the <em>coda</em> package <span class="citation"></span>.</p>
<p>[ht]</p>
<p>[tab:abcexpt]</p>
<p>[tab:gammaexpt]</p>
<p>Two further experiments were performed to address potential sources of error. To evaluate the effect of model misspecification in the case of heterogeneity among nodes, we generated a network where half the nodes were attached with power <span class="math"><em>α</em></span> = 0.5, and the other half with power <span class="math"><em>α</em></span> = 1.5. The other parameters for this network were <span class="math"><em>N</em></span> = 5000, <span class="math"><em>I</em></span> = 1000, and <span class="math"><em>m</em></span> = 2. To investigate the effects of potential sampling bias <span class="citation"></span>, we simulated a transmission tree where the tips were sampled in a peer-driven fashion, rather than at random. That is, the probability to sample a node was twice as high if any of that node’s network peers had already been sampled. The parameters of this network were <span class="math"><em>N</em></span> = 5000, <span class="math"><em>I</em></span> = 2000, <span class="math"><em>m</em></span> = 2, and <span class="math"><em>α</em></span> = 0.5.</p>
<h3 id="application-to-hiv-data">Application to HIV data</h3>
<p>Because the model assumes a single connected contact network, it is most appropriate to apply to groups of individuals who are epidemiologically related. Therefore, we searched for published datasets which originated from existing clusters, either phylogenetically or geographically defined. In addition, we analysed an in-house dataset sampled from -positive individuals in British Columbia, Canada (the “BC data”). The datasets are summarized in .</p>
<p>[ht]</p>
<p>[tab:data]</p>
<p>We downloaded all sequences associated with each published study from GenBank. For the <span class="citation"></span> data, each <em>env</em> sequence was aligned pairwise to the HXB2 reference sequence (GenBank accession number K03455) and the hypervariable regions were clipped out with <em>BioPython</em> version 1.66+ <span class="citation"></span>. Sequences were multiply aligned using <em>MUSCLE</em> version 3.8.31 <span class="citation"></span>, and alignments were manually inspected with <em>Seaview</em> version 4.4.2 <span class="citation"></span>. Phylogenies were constructed from the nucleotide alignments by approximate maximum likelihood using <em>FastTree2</em> version 2.1.7 <span class="citation"></span> with the model <span class="citation"></span>. Transmission trees were estimated by rooting and time-scaling the phylogenies by root-to-tip regression, using a modified version of Path-O-Gen (distributed as part of BEAST <span class="citation"></span>) as described previously <span class="citation"></span>.</p>
<p>Three of the datasets <span class="citation"></span> were initially much larger than the others, containing 1265, 1299, and 7923 sequences respectively. To ensure that the analyses were comparable, we reduced these to a number of sequences similar to the smaller datasets. For the <span class="citation"></span> and BC datasets, we detected clusters of size 280 and 399 respectively using a patristic distance cutoff of 0.02 as described previously <span class="citation"></span>. Only sequences within these clusters were carried forward. For the <span class="citation"></span> data, no large clusters were detected using the same cutoff, so we analysed a subtree of size 180 chosen arbitrarily.</p>
<p>Empirical studies of contact networks often report the exponent <span class="math"><em>γ</em></span> of the power law degree distribution. To compare our results to the literature, we simulated 100 networks each according to the parameter estimates obtained for each investigated dataset. The power-law exponent <span class="math"><em>γ</em></span> was calculated for each network using the <em>fit_power_law</em> function in <em>igraph</em>, with the ‘R.mle’ implementation. The median of the 100 <span class="math"><em>γ</em></span> values was taken as a point estimate for the associated dataset.</p>
<p>For all datasets, we used the priors <span class="math"><em>α</em></span> <span class="math"> ∼ </span> Uniform(0, 2) and <span class="math"><em>N</em></span> and <span class="math"><em>I</em></span> jointly uniform on the region {<span class="math"><em>n</em> ≤ <em>N</em> ≤ 10000</span>, <span class="math"><em>n</em> ≤ <em>I</em> ≤ 10000</span>, <span class="math"><em>I</em> ≤ <em>N</em></span>}, where <span class="math"><em>n</em></span> is the number of tips in the tree (see ). Since the value <span class="math"><em>m</em> = 1</span> produces networks with no cycles, which we considered fairly implausible, we ran one analysis with the prior <span class="math"><em>m</em> ∼ </span> DiscreteUniform(1, 5), and one with the prior <span class="math"><em>m</em> ∼ </span> DiscreteUniform(2, 5). The other parameters to the SMC algorithm were the same as used for the simulation experiments, except that we used 10000 particles instead of 1000 to increase the accuracy of the estimated posterior. This was computationally feasible due to the small number of runs required for this analysis.</p>
<h2 id="results">Results</h2>
<h3 id="analysis-of-model-1">Analysis of model</h3>
<h4 id="classifiers-for-parameters-based-on-tree-shape" class="unnumbered">Classifiers for parameters based on tree shape</h4>
<p>Trees simulated under different values of were visibly quite distinct (). In particular, higher values of produce networks with a small number of highly connected nodes which, once infected, are likely to transmit to many other nodes. This results in a more unbalanced, ladder-like structure in the phylogeny, compared to networks with lower values. None of the other three parameters produced trees which were as easily distinguished from each other (). Sackin’s index, which measures tree imbalance, was significantly correlated with all four parameters (for <span class="math"><em>α</em></span>, <span class="math"><em>I</em></span>, <span class="math"><em>m</em></span>, and <span class="math"><em>N</em></span> respectively: Spearman’s rho = 0.85, <span class="math"> − 0. 12</span>, <span class="math"> − 0. 13</span>, 0.09; <span class="math"><em>p</em></span>-values <span class="math"> &lt; 10<sup> − 5</sup></span>, <span class="math">0. 003</span>, <span class="math"> &lt; 10<sup> − 5</sup></span>, <span class="math"> &lt; 10<sup> − 5</sup></span>) The ratio of internal to terminal branch lengths was negatively correlated with and , and positively corelated with and (Spearman’s rho <span class="math"> − 0. 84</span>, <span class="math"> − 0. 69</span>, 0.1, 0.18; all <span class="math"><em>p</em> &lt; 10<sup> − 5</sup></span>).</p>
<p><embed src="kernel-alpha-tree.pdf" /> [fig:alphatrees]</p>
<p>shows projections of the simulated trees onto the first two principal components of the kernel matrix. The figure shows only the simulations with 500-tip trees and 1000 infected nodes. The three and values considered are well separated from each other in feature space. On the other hand, the three values overlap significantly, and the three values are virtually indistinguishable. Similar observations can be made for other values of and the number of tips (). The values of and separated more clearly with larger numbers of tips, and in the case of , larger epidemic sizes.</p>
<p><embed src="kernel-kpca.pdf" /> [fig:kpca]</p>
<p>Accuracy of the classifiers varied based on the parameter being tested (, left). Classifiers based on two other tree statistics, the and Sackin’s index, generally exhibited worse performance than the tree kernel, although the magnitude of the disparity varied between the parameters (, centre and right). The results were largely robust to variations in the tree kernel meta-parameters <span class="math"><em>λ</em></span> and <span class="math"><em>σ</em></span>, although accuracy varied between different epidemic and sampling scenarios ().</p>
<p>When classifying <span class="math"><em>α</em></span>, the classifier had an average <span class="math"><em>R</em><sup>2</sup></span> of 0.92, compared to 0.56 for the -based SVR, and 0.75 for the linear regression against Sackin’s index. There was little variation about the mean for different tree and epidemic sizes. No classifier could accurately identify the <span class="math"><em>m</em></span> parameter in any epidemic scenario, with average <span class="math"><em>R</em><sup>2</sup></span> values of 0.12 for , 0.01 for the , and 0.06 for Sackin’s index. Again, there was little variation in accuracy between epidemic scenarios, although the accuracy of the was slightly higher on 1000-tip trees (average <span class="math"><em>R</em><sup>2</sup></span> 0.01, 0.11, 0.32 for 100, 500, and 1000 tips respectively).</p>
<p>The accuracy of classifiers <span class="math"><em>I</em></span> varied significantly with the number of tips in the tree. For 100-tip trees, the average <span class="math"><em>R</em><sup>2</sup></span> values were 0.7, 0.55, and 0.02 for the tree kernel, , and Sackin’s index respectively. For 500-tip trees, the values increased to 0.93, 0.83, and 0.07. Finally, the performance of classifiers for <span class="math"><em>N</em></span> depended heavily on the epidemic scenario. The <span class="math"><em>R</em><sup>2</sup></span> of the classifier ranged from 0.08 for the smallest epidemic and smallest sample size, to 0.82 for the largest. Likewise, <span class="math"><em>R</em><sup>2</sup></span> for the -based SVR ranged from 0.01 to 0.54. Sackin’s index did not accurately classify <span class="math"><em>N</em></span> in any scenario, with an average <span class="math"><em>R</em><sup>2</sup></span> of 0.03 and little variation between scenarios.</p>
<p><embed src="kernel-rsquared.pdf" /> [fig:rsquared]</p>
<h4 id="marginal-parameter-estimates-with-grid-search" class="unnumbered">Marginal parameter estimates with grid search</h4>
<p>The accuracy of grid search estimates largely paralleled that of the classifiers. shows point estimates and 95% highest density intervals for each of the parameters, for one replicate experiment with 500-tip trees. Plots showing the point estimates for all replicates can be found in . For all parameters except <span class="math"><em>m</em></span>, the error of point estimates was negatively correlated with the number of sampled tips in the tree (for , , and respectively: Spearman’s <span class="math"><em>ρ</em></span> = <span class="math"> − 0. 22</span>, <span class="math"> − 0. 51</span>, <span class="math"> − 0. 16</span>; <span class="math"><em>p</em></span>-values <span class="math">4  ×  10<sup> − 4</sup></span>, <span class="math"> &lt; 10<sup> − 5</sup></span>, <span class="math">0. 01</span>). The highest density intervals obtained for all parameters were extremely wide, occcupying <span class="math"> &gt; </span>90% of the grid in all cases ().</p>
<p>The parameter was the most accurately estimated, with point estimates having an average deviation of 0.14 from the true value, on a grid from 0 to 2. The error was negatively correlated with the true value of (Spearman’s <span class="math"><em>ρ</em></span> = <span class="math"> − 0. 26</span>, <span class="math"><em>p</em> = 10<sup> − 5</sup></span>), although the relationship was clearly nonlinear (). The accuracy was highest for the test value = 1.25 (mean error 0.02) which exhibited markedly different behaviour than the other values in terms of the distribution of kernel scores along the grid (). In particular, there was a very pronounced peak in scores around the true value, in contrast to most other values where the scores were flat around the true value. The peak was also observed to a lesser degree for = 1. The average absolute error of the point estimates for was 310, on a grid of 500 to 5000, and the errors were not significantly correlated with the true value of . Kernel score distributions for all test values exhibited a similar rounded shape ().</p>
<p>The average error for was 1.31, on a grid from 1 to 6; this error was positively correlated with increasing (Spearman’s <span class="math"><em>ρ</em></span> = 0.25, <span class="math"><em>p</em> = 6  ×  10<sup> − 4</sup></span>). This positive correlation was apparently due to the much lower error for = 1 and 2 than for the other <span class="math"><em>m</em></span> values (mean errors 0.93 for <span class="math"><em>m</em> ≤ 2</span> vs. 1.49 for <span class="math"><em>m</em> &gt; 2</span>, ). The value <span class="math"><em>m</em> = 1</span> causes the network to take on a distinct shape relative to higher values, namely a tree (<em>i.e.</em> there are no cycles, see ). The average error for was 2419, on a grid from 1000 to 15000, and was positively correlated with the true value of (Spearman’s <span class="math"><em>ρ</em></span> = 0.43, <span class="math"><em>p</em>  &lt; 10<sup> − 5</sup></span>). Most of the kernel score distributions were extremely flat, except for the value <span class="math"><em>N</em> = 1000</span> which exhibited a pronounced peak around the true value ().</p>
<p><img src="gridsearch-example" alt="image" /> [fig:gridest]</p>
<h4 id="joint-parameter-estimates-with-kernel-abc" class="unnumbered">Joint parameter estimates with kernel-ABC</h4>
<p>We used <em>netabc</em> to estimate the parameters of the model on simulated trees where the true parameter values were known. Point estimates for each parameter are shown in for the simulations with = 2. The results for the other values of were similar (). The median [IQR] absolute error of estimates of across all simulations was 0.08 [0.05-0.17]. The accuracy of the estimates was not significantly different between values of <span class="math"><em>m</em></span> or <span class="math"><em>I</em></span> (both one-way ANOVA, <span class="math"><em>p</em></span> = 0.05 and 0.07), although the errors when the true value of was zero were significantly greater than the other values (Wilcoxon rank-sum test, <span class="math"><em>p</em> = 8  ×  10<sup> − 3</sup></span>). The error in the estimated value of <span class="math"><em>I</em></span> was 395 [207-683]. Errors were significantly higher for <span class="math"><em>α</em> ≥ 1</span> (Wilcoxon rank-sum test, <span class="math"><em>p</em> = 8  ×  10<sup> − 3</sup></span>) and for <span class="math"><em>I</em></span> = 2000 (<span class="math"><em>p</em> =  &lt; 10<sup> − 5</sup></span>), but not for any values of <span class="math"><em>m</em></span> (one-way ANOVA). The <span class="math"><em>m</em></span> parameter was estimated correctly in only 27 % of simulations, little better than random guessing. The true values of <span class="math"><em>m</em></span> and <span class="math"><em>I</em></span> did not significantly affect the error (one-way ANOVA). Finally, the total number of nodes was consistently over-estimated by about a factor of two (error 5987 [2060 - 7999]). No other parameters influenced the accuracy of the <span class="math"><em>N</em></span> estimates (one-way ANOVA).</p>
<p><img src="abc-point-estimate-m2" alt="image" /> [fig:abcptm2]</p>
<p>shows the approximation to the posterior distribution on the parameters for one simulation. Equivalent plots for one replicate simulation with each combination of parameters can be found in . intervals around and were narrow relative to the region of nonzero prior density, whereas the intervals for <span class="math"><em>m</em></span> and were widely dispersed. shows point estimates and 95% intervals averaged over all simulations.</p>
<p><embed src="{abc-posterior/1.0_1000_2_5000_0}.pdf" /> [fig:abcex]</p>
<p>[tab:abchpd]</p>
<p>To test the effect of model misspecification, we simulated one network where the nodes exhibited heterogeneous preferential attachment power (half 0.5, the other half 1.5), with <span class="math"><em>m</em></span> = 2, <span class="math"><em>N</em></span> = 5000, and <span class="math"><em>I</em></span> = 1000. The MAP [95% HPD] estimates for each parameter were: <span class="math"><em>α</em></span>, 1.1 [0.6 - 1.16]; <span class="math"><em>I</em></span>, 1094 [662 - 4455]; <span class="math"><em>m</em></span>, 3 [1 - 5]; <span class="math"><em>N</em></span>, 12670 [3948 - 14977]. The approximate posterior distributions for this simulation are shown in . To test the effect of sampling bias, we sampled one transmission tree in a peer-driven fashion, where the probability to sample a node was twice as high if one of its peers had already been sampled. The parameters for this experiment were <span class="math"><em>N</em></span> = 5000, <span class="math"><em>m</em></span> = 2, <span class="math"><em>α</em></span> = 0.5, and <span class="math"><em>I</em></span> = 2000. The estimated values were <span class="math"><em>α</em></span>, 0.36 [0.01 - 0.63]; <span class="math"><em>I</em></span>, 2354 [1423 - 3811]; <span class="math"><em>m</em></span>, 3 [2 - 5]; <span class="math"><em>N</em></span>, 9928 [2881 - 14780]. The approximate posterior distributions are shown in . Both of these results were in line with estimates obtained on other simulated datasets (), although the estimate of peer-driven sampling for <span class="math"><em>α</em></span> was somewhat lower than typical.</p>
<h3 id="application-to-hiv-data-1">Application to HIV data</h3>
<p>We applied kernel-ABC to five published HIV datasets (), and found substantial heterogeneity among the parameter estimates (). Plots of the marginal posterior distributions for each dataset are shown in . Two of the datasets (<span class="citation"></span>) had estimated <span class="math"><em>α</em></span> values near unity for the prior allowing <span class="math"><em>m</em> = 1</span> ( estimates [95% ] 1.05 [0.04 - 1.27] and 0.84 [0.01 - 1.02] respectively). The MAP estimates did not change appreciably when <span class="math"><em>m</em> = 1</span> was disallowed by the prior, although the credible interval of the <span class="citation"></span> data was narrower (0.04 - 1.27). When <span class="math"><em>m</em> = 1</span> was permitted, the <span class="citation"></span> both had low estimated <span class="math"><em>α</em></span> values (0.06 [0.01 - 0.73] and 0.19 [0.01 - 0.8]). However, the MAP estimates increased when <span class="math"><em>m</em> = 1</span> was not permitted, although the HPD intervals remained roughly the same (0.78 [0.02 - 0.94] and 0.59 [0.07 - 0.95]). The <span class="citation"></span> data had a fairly low estimated <span class="math"><em>α</em></span> for both priors on <span class="math"><em>m</em></span> (0.32 for <span class="math"><em>m</em> ≥ 1</span>; 0.39 for <span class="math"><em>m</em> ≥ 2</span>). However, the confidence interval was much wider when <span class="math"><em>m</em> = 1</span> was allowed ([0.04 - 1.62] for <span class="math"><em>m</em> ≥ 1</span> vs. [0 - 0.73] for <span class="math"><em>m</em> ≥ 2</span>).</p>
<p>For all the datasets except <span class="citation"></span>, estimated values of <span class="math"><em>I</em></span> were below 2000 when <span class="math"><em>m</em> = 1</span> was allowed, with relatively narrow HPD intervals compared to the nonzero prior density region (<span class="citation"></span>, 482 [293 - 2111]; <span class="citation"></span>, 307 [136 - 2822]; <span class="citation"></span>, 1183 [413 - 2897]; <span class="citation"></span>, 719 [176 - 2114]). The <span class="citation"></span> data was the outlier, with a very high estimated <span class="math"><em>I</em></span>, and HPD interval spanning almost the entire prior region (7409 [187 - 8819]). The <span class="math"><em>I</em></span> estimates and HPD intervals were generally robust to the choice of prior on <span class="math"><em>m</em></span>, with slightly narrower HPD intervals (compare ).</p>
<p>The MAP estimate of <span class="math"><em>m</em></span> was equal to 1 for all but the <span class="citation"></span> data, when this value was allowed. However, the upper bound of the HPD interval was different for each dataset (<span class="citation"></span>, 5; <span class="citation"></span>, 4; <span class="citation"></span>, 1; <span class="citation"></span>, 2). When <span class="math"><em>m</em> = 1</span> was disallowed, the MAP for all datasets was either 2 or 3, with HPD intervals spanning the entire prior region. The estimates for the total number of nodes <span class="math"><em>N</em></span> were largely uninformative for all samples, with almost all MAP estimates greater than 7500 and HPD intervals spanning almost the entire nonzero prior density region. The only exception was the <span class="citation"></span> data, for which the MAP estimate was lower (6973) when <span class="math"><em>m</em> = 1</span> was allowed.</p>
<p>[ht] <img src="realdata-hpd-bc" alt="image" /> [fig:abchpd]</p>
<p>To make our analyses comparable to the existing network literature, we estimated values of the power law exponent <span class="math"><em>γ</em></span> for each of the datasets investigated. The results are summarized in . All of the estimated exponents were in the range <span class="math">2 ≤ <em>γ</em> ≤ 4</span> commonly reported in the literature <span class="citation"></span>. Higher estimated values of <span class="math"><em>α</em></span> translated into higher estimated values of <span class="math"><em>γ</em></span>, which is consistent with the observed relationship between these two quantities ().</p>
<p>[tab:gamma]</p>
<h2 id="discussion">Discussion</h2>
<h3 id="netabc-uses-limitations-and-possible-extensions"><em>Netabc</em>: uses, limitations, and possible extensions</h3>
<p>Contact networks can have a strong influence on epidemic progression, and are potentially useful as a public health tool <span class="citation"></span>. Despite this, few methods exist for investigating contact network parameters in a phylodynamic framework <span class="citation"></span>. Kernel-ABC is a model-agnostic method which can be used to investigate any quantity that affects tree shape <span class="citation"></span>. In this work, we developed <em>netabc</em>, a method based on kernel-ABC to infer the parameters of a contact network model. The method is general, meaning that it can be used to infer parameters of any network model, as long as it allows simulated networks can be easily generated. We have included generators for the model discussed here, as well as the and network models. Instructions for adding additional models are available in the project’s online documentation. We have made <em>netabc</em> publicly available at <a href="github.com/rmcclosk/netabc">github.com/rmcclosk/netabc</a> under a permissive open source license, to encourage other researchers to apply and extend our method.</p>
<p>Several alternative network models and modeling frameworks have been developed which may provide useful future targets for kernel-. Waring models <span class="citation"></span> are a more flexible type of preferential attachment model which permit a subset of attachments to be formed non-prefentially. These models were used by <span class="citation"></span> to characterize the transmission network in the United Kingdom.  <span class="citation"></span> are a flexible and expressive parameterization of contact networks in terms of statistics of network features such as pairs and triads. <span class="citation"></span> evaluated the effect of several different parameterizations on transmission tree shape and effective population size. The author suggested the use of as a general framework for estimation of epidemiological quantities related to transmission. Except for a few special cases, simulating a network according to an generally requires , which would be too computationally intensive to integrate into <em>netabc</em> as it currently stands. To fit with kernel-, one possibility would be to consider the network itself as a parameter to be modified by the kernel. Other network modelling frameworks include the partnership-centric formulation developed by <span class="citation"></span> and the log-linear adjacency matrix parameterization applied by <span class="citation"></span>.</p>
<p>The two-step process of simulating a contact network and subsequently allowing an epidemic to spread over that network carries with it the assumption that the contact network is static over the duration of the epidemic. Clearly this assumption is invalid, as people make and break partnerships on a regular basis. Addressing the impact of this simplifying assumption is outside the scope of this work. However, the same assumption is made by most studies using contact network models in an epidemiological context <span class="citation"></span>. In principle, kernel- could be adapted to dynamic contact networks by using a method such as that developed by <span class="citation"></span> to simulate such a network, while concurrently simulating the spread of an epidemic.</p>
<p>It is important to note that <em>netabc</em> takes a transmission tree as input, rather than a viral phylogeny. In reality, true transmission trees are not available and must be approximated, often by way of a viral phylogeny. Although this has been demonstrated to be a fair approximation <span class="citation"></span>, and is frequently used in practice <span class="citation"></span>, the topologies of a viral phylogeny and transmission tree can differ significantly <span class="citation"></span> due to within-host evolution and the sampling process. We have left the estimation of a transmission tree up to the user. In theory, it is possible to incorporate the process by which a viral phylogeny is generated along with a transmission tree into our method, for example by simulating within-host dynamics. Although this may be an avenue for future extension, we felt that it would obscure the primary purpose of this work, which is to study contact network parameters. In addition, there are a number of different methods available for inferring transmission trees <span class="citation"></span>, some of which incorporate geographic and/or epidemiological data not accommodated by our method. We therefore felt it would be best to allow researchers to use their own preferred method of constructing a transmission tree.</p>
<p>Our implementation of uses a simple multinomial scheme to sample particles from the population according to their weights. Several other sampling strategies have been developed <span class="citation"></span>, and it is possible that the use of a more sophisticated technique might increase the algorithm’s accuracy. Finally, the - algorithm is computationally intensive, taking about a day when run on 20 cores in parallel with the settings described in the methods section. Implementing parallization using MPI, rather than POSIX threads as we have done here, would allow the program to be run over a larger number of cores on multiple CPUs in parallel.</p>
<h3 id="analysis-of-model-2">Analysis of model</h3>
<p>The preferential attachment power had a very strong influence on tree shape in the range of values we considered. Although the tree kernel was the most effective classifier for , a tree balance statistic performed nearly as well. This result was intuitive: high values produce networks with few well-connected “superspreader” nodes which are involved in a large number of transmissions, resulting in a highly unbalanced ladder-like tree structure (). The parameter, representing the prevalence at the time of sampling, was also generally estimable. The dynamics of the model, and the coalescent process <span class="citation"></span>, offer a potential explanation for this result. shows simulated trajectories of the model. In the initial phase of the epidemic, when is small, each new transmission results in potentially many new discordant edges, thus decreasing the waiting time until the next transmission. Hence, there is an early exponential growth phase, producing many short branches near the root of the tree. As the epidemic gets closer to saturating the network, the number of discordant edges decays, causing longer waiting times. The distribution of coalescence times in the tree should therefore be informative about  <span class="citation"></span>. This information is captured by the tree kernel, and also by the statistic, which both performed quite will in classifying ().</p>
<p>The number of nodes in the network, , exhibited the most variation in terms of being estimable. There was almost no difference between trees simulated under different values when the number of infected nodes was very small. There is an intuitive explanation for this result, namely that adding additional nodes does not change the edge density or overall shape of a network. This can be illustrated by imagining that we add a small number of nodes to a network after the epidemic simulation has already been completed. It is possible that none of these new nodes attains a connection to any infected node. Thus, running the simulation again on the new, larger network could produce the exact same transmission tree as before. On the other hand, when is large relative to , the coalescent dynamics discussed above also apply. That is, the waiting times until the next infection increase, resulting in longer coalescence times toward the tips. The relative accuracy of the in these situations coroborates this hypothesis. The parameter, which controls the number of connections added to the network per vertex, did not have a measurable impact on tree shape and was not estimable with kernel-ABC. The exception to this was the value = 1, which produces networks without cycles whose associated trees were more easily distinguished. However, all the analyses presented here did not take the absolute size of the transmission trees into account, as the branch lengths were rescaled by their mean. Because higher values imply higher edge density, an epidemic should spread more quickly for higher than lower with the same per-edge transmission probability. Hence, considering the absolute height of the trees may improve our method’s ability to reconstruct .</p>
<p>In addition to the tree height, many summary statistics have been developed to capture particular details of tree shape. Two of these, Sackin’s index and the ratio of internal to terminal branch lengths, were correlated with every parameter. Classifiers based on Sackin’s index and the similarity measure performed well in some cases, though poorly in others. is often applied using a vector of summary statistics <span class="citation"></span>, rather than a kernel-based similarity score as we have done here. Methods have been developed to select an optimal combination of summary statistics for a given inference task <span class="citation"></span>. Hence, an avenue for future improvement of our method may be the inclusion of additional summary statistics to supplement the tree kernel. In addition, all four parameters were more accurately classified when the number of tips in the transmission trees was larger, underscoring the importance of adequate sampling for accurate phylodynamic inference.</p>
<p>For the more estimable parameters, the credible intervals attained from the marginal target distributions were much narrower than those obtained through grid search, while point estimates were of comparable accuracy. This was likely due to the fact that employs importance sampling to approximate the posterior distribution, while grid search simply calculates a distance metric which may not have any resemblance to the posterior. Admittedly, our method of finding credible intervals from kernel scores along the grid, namely by normalizing the scores to resemble a probability distribution, was somewhat ad hoc, which may also have played a role. Regardless, this result indicates that there is benefit to applying the more sophisticated method, even if values for some of the parameters are known <em>a priori</em>, and especially if credible intervals are desired on the parameters of interest.</p>
<p>As noted by <span class="citation"></span>, uniform priors on model parameters may translate to highly informative priors on quantities of interest. We observed a non-linear relationship between the preferential attachment power <span class="math"><em>α</em></span> and the power law exponent <span class="math"><em>γ</em></span> (). Therefore, placing a uniform prior on <span class="math"><em>α</em></span> between 0 and 2 is equivalent to placing an informative prior that <span class="math"><em>γ</em></span> is close to 2. Therefore, if we were primarily interested in <span class="math"><em>γ</em></span> rather than <span class="math"><em>α</em></span>, a more sensible choice of prior might have a shape informed by and be bounded above by approximately <span class="math"><em>α</em></span> = 1.5. This would uniformly bound <span class="math"><em>γ</em></span> in the region <span class="math">2 ≤ <em>γ</em> ≤ 4</span> commonly reported in the network literature <span class="citation"></span>. We note however that <span class="citation"></span> estimated <span class="math"><em>γ</em></span> values greater than four for some datasets, in one case as high as 17, indicating that a wider range of permitted <span class="math"><em>γ</em></span> values may be warranted.</p>
<h3 id="application-to-hiv-data-2">Application to HIV data</h3>
<p>Our investigation of published HIV datasets indicated heterogeneity in the contact network structures underlying several distinct local epidemics. When interpreting these results, we caution that the BA model is quite simple and most likely misspecified for these data. In particular, the average degree of a node in the network is equal to <span class="math">2<em>m</em></span>, and therefore is constrained to be a multiple of 2. Furthermore, we considered the case <span class="math"><em>m</em> = 1</span>, where the network has no cycles, to be implausible and therefore assigned it zero prior probability in one set of analyses. This forced the average degree to be at least four, which may be unrealistically high for sexual networks. The fact that the estimated values of <span class="math"><em>α</em></span> differed substatially for three datasets depending on whether or not <span class="math"><em>m</em> = 1</span> was allowed by the prior is futher evidence of this potential misspecification. However, we note that for two of the datasets, the estimated values of <span class="math"><em>α</em></span> did not change much between priors, and the estimates of <span class="math"><em>I</em></span> were robust to the choice of prior for all datasets studied. More sophisticated models, for example models incorporating heterogeneity in node behaviour, are likely to provide a better fit to these data.</p>
<p>With respect to the preferential attachment power <span class="math"><em>α</em></span>, the six datasets analysed fell into two categories (). First, we estimated a preferential attachment power close to 1, indicating linear preferential attachment, for the BC data and the outbreaks studied by <span class="citation"></span> and <span class="citation"></span>. These values were robust to specifying different priors for <span class="math"><em>m</em></span>. All three datasets were sampled from populations in which we would expect a high degree of epidemiological relatedness: <span class="citation"></span> studied a recent outbreak among Romanian , <span class="citation"></span> sampled acutely infected MSM in Beijing, China, and the BC data constituted a phylogenetic cluster. These are all contexts in which we would expect some of the assumptions of the BA model, such as a connected network, relatively high mean degree, and preferential attachment dynamics, to hold.</p>
<p>The remaining three datasets (<span class="citation"></span>) had estimated values of <span class="math"><em>α</em></span> below 0.5 when <span class="math"><em>m</em> = 1</span> was included in the prior, but these were not robust to changing the prior to exclude <span class="math"><em>m</em> = 1</span>. For the <span class="citation"></span> data, model misspecification is likely partially responsible. While the authors found that a large proportion of the samples were epidemiologically linked, these were mainly in small local clusters rather than the single large component postulated by the BA model. In addition, the mixed risk groups in the dataset would be unlikely to significantly interact, further weakening any global preferential attachment dynamics. The dataset studied by <span class="citation"></span> originated from a densely sampled population where the predominant risk factor was believed to be heterosexual exposure. Although the MAP estimate of <span class="math"><em>α</em></span> was almost unchanged when the value <span class="math"><em>m</em> = 1</span> was excluded from the prior, the confidence interval shrank substantially. For both priors, the estimated prevalence was extremely high, in fact higher than the estimated HIV prevalence in the sampled region. The authors indicated that the source of the samples was a town in close proximity to the country’s capital city, and suggested that there may have been a high degree of migration and partner interchange between the two locations. It is possible that the contact network underlying the subtree we investigated includes a much larger group based in the capital city, which would explain the high estimate of <span class="math"><em>I</em></span>. There is no clear explanation for the discrepancy between the two priors for the <span class="citation"></span> data, as the subset we analyzed formed a phylogenetic cluster and therefore was a good candidate for the BA model. However, nearly all the posterior density was assigned to <span class="math"><em>m</em> = 1</span> when this value was allowed, indicating that the network was more likely to have an acyclic tree structure.</p>
<p>Our use of the model makes several simplifying assumptions. First, we assume homogeneity across the network with respect to node behaviour and transmission risk. In reality, the attraction to high-degree nodes seems likely to vary among individuals, as does their risk of transmitting or contracting the virus. We have also assumed that all transmission risks are symmetric, which is clearly false for all known modes of transmission, and that infected individuals never recover but remain infectious indefinitely. These assumptions were made for the purpose of keeping the model as simple as possible, since this is the very first attempt to fit a contact network model in a phylodynamic context. However, the Gillespie simulation algorithm built into <em>netabc</em> can handle arbitrary transmission and removal rates which need not be homogeneous across the network. Moreover, it is possible to use kernel- to fit a model which relaxes some or all of these assumptions, which may be a fruitful avenue for future investigation. Despite the possible misspecification, our estimates of the power law exponent <span class="math"><em>γ</em></span> were in line with those reported in the literature ().</p>
<h1 id="conclusion">Conclusion</h1>
<p>Due to the rapid advancement of nucleotide sequencing technology, viral sequence data data have become increasingly feasible to collect on a population level. Through phylodynamic methods, these data offer a window into epidemiological processes which would otherwise be virtually impossible to study on a realistic scale.</p>
<p>This thesis developed <em>netabc</em>, a computer program implementing a statistical inference method for contact network parameters from viral phylogenetic data. <em>Netabc</em> brings together the areas of viral phylodynamics and network epidemiology, which have only intersected in a very limited fashion thus far <span class="citation"></span>. The use of kernel-, a likelihood-free method, makes it possible to fit network models to phylogenies without calculating intractible likelihoods.</p>
<p>Although phylodynamic methods have been developed to fit a wide variety of epidemiological models to phylogenetic data <span class="citation"></span>, our method is the first (to our knowledge) which can fit models that do not assume panmixia. We believe this capability will be of broad interest to the molecular evolution and epidemiology community, as it widens the field of epidemiological parameters which may be investigated through viral sequence data. In addition, the characterization of local contact networks could be valuable from a public health perspective, such as for investigating optimal vaccination strategies <span class="citation"></span>. This information could assist in curtailing current epimemics, as well as preventing future epidemics of different diseases over the same contact network.</p>
<p>The paricular model we have investigated uses a preferential attachment mechanism to generate scale-free networks resembling real-world social and sexual networks <span class="citation"></span>. Of the four parameters we considered, the pereferential attachment power <span class="math"><em>α</em></span> was the most readily estimable. Estimating <span class="math"><em>α</em></span> with traditional epidemiological methods is challenging due to the requirement of sampling the high-degree nodes making up the tail of the power law distribution, although approaches such as respondent-driven sampling <span class="citation"></span> may be effective.</p>
<p>In closing, <em>netabc</em> combines phylodynamics, contact network epidemiology, approximate Bayesian computation, and sequentiial Monte Carlo to provide a source of insight into network structures complementary to traditional epidemiology.</p>
<h1 id="appendix-supplemental-figures" class="unnumbered">Appendix: Supplemental Figures</h1>
<p><embed src="kernel-I-tree.pdf" /> [fig:Itrees]</p>
<p><embed src="kernel-m-tree.pdf" /> [fig:mtrees]</p>
<p><embed src="kernel-N-tree.pdf" /> [fig:Ntrees]</p>
<p><embed src="kernel-alpha-crossv.pdf" /> [fig:alphacrossv]</p>
<p><embed src="kernel-I-crossv.pdf" /> [fig:Icrossv]</p>
<p><embed src="kernel-m-crossv.pdf" /> [fig:mcrossv]</p>
<p><embed src="kernel-N-crossv.pdf" /> [fig:Ncrossv]</p>
<p><embed src="kernel-alpha-kpca.pdf" title="fig:" /> [fig:alphakpca]</p>
<p><embed src="kernel-I-kpca.pdf" title="fig:" /> [fig:Ikpca]</p>
<p><embed src="kernel-m-kpca.pdf" title="fig:" /> [fig:mkpca]</p>
<p><embed src="kernel-N-kpca.pdf" title="fig:" /> [fig:Nkpca]</p>
<p><embed src="gridsearch-alpha-kernel.pdf" title="fig:" /> [fig:gridalpha]</p>
<p><embed src="gridsearch-I-kernel.pdf" title="fig:" /> [fig:gridI]</p>
<p><embed src="gridsearch-m-kernel.pdf" title="fig:" /> [fig:gridm]</p>
<p><embed src="gridsearch-N-kernel.pdf" title="fig:" /> [fig:gridN]</p>
<p><embed src="gridsearch-alpha-point-estimate.pdf" /> [fig:gridptalpha]</p>
<p><embed src="gridsearch-I-point-estimate.pdf" /> [fig:gridptI]</p>
<p><embed src="gridsearch-m-point-estimate.pdf" /> [fig:gridptm]</p>
<p><embed src="gridsearch-N-point-estimate.pdf" /> [fig:gridptN]</p>
<p><embed src="abc-point-estimate-m3.pdf" /> [fig:abcptm3]</p>
<p><embed src="abc-point-estimate-m4.pdf" /> [fig:abcptm4]</p>
<p><embed src="alpha-gamma.pdf" title="fig:" /> [fig:gamma]</p>
<p><embed src="sir-trajectories.pdf" /> [fig:sir]</p>
<p><img src="mixed-posterior" alt="image" /> [fig:mixed]</p>
<p><img src="peerdriven-posterior" alt="image" /> [fig:peerdriven]</p>
<p><img src="realdata-hpd-bc-m2" alt="image" /></p>
<p>[fig:abchpdm2]</p>
<p><img src="bctree-posterior" alt="image" /> [fig:bctree]</p>
<p><img src="cuevas2009-posterior" alt="image" /> [fig:cuevas]</p>
<p><img src="li2015-posterior" alt="image" /> [fig:li]</p>
<p><img src="niculescu2015-posterior" alt="image" /> [fig:niculescu]</p>
<p><img src="novitsky2014-posterior" alt="image" /> [fig:novitsky]</p>
<p><img src="wang2015-posterior" alt="image" /> [fig:wang]</p>
<p><embed src="{abc-posterior/0.0_1000_2_5000_0}.pdf" /> [fig:0.0-1000-2-5000-0]</p>
<p><embed src="{abc-posterior/0.5_1000_2_5000_0}.pdf" /> [fig:0.5-1000-2-5000-0]</p>
<p><embed src="{abc-posterior/1.0_1000_2_5000_0}.pdf" /> [fig:1.0-1000-2-5000-0]</p>
<p><embed src="{abc-posterior/1.5_1000_2_5000_0}.pdf" /> [fig:1.5-1000-2-5000-0]</p>
<p><embed src="{abc-posterior/0.0_2000_2_5000_0}.pdf" /> [fig:0.0-2000-2-5000-0]</p>
<p><embed src="{abc-posterior/0.5_2000_2_5000_0}.pdf" /> [fig:0.5-2000-2-5000-0]</p>
<p><embed src="{abc-posterior/1.0_2000_2_5000_0}.pdf" /> [fig:1.0-2000-2-5000-0]</p>
<p><embed src="{abc-posterior/1.5_2000_2_5000_0}.pdf" /> [fig:1.5-2000-2-5000-0]</p>
<p><embed src="{abc-posterior/0.0_1000_3_5000_0}.pdf" /> [fig:0.0-1000-3-5000-0]</p>
<p><embed src="{abc-posterior/0.5_1000_3_5000_0}.pdf" /> [fig:0.5-1000-3-5000-0]</p>
<p><embed src="{abc-posterior/1.0_1000_3_5000_0}.pdf" /> [fig:1.0-1000-3-5000-0]</p>
<p><embed src="{abc-posterior/1.5_1000_3_5000_0}.pdf" /> [fig:1.5-1000-3-5000-0]</p>
<p><embed src="{abc-posterior/0.0_2000_3_5000_0}.pdf" /> [fig:0.0-2000-3-5000-0]</p>
<p><embed src="{abc-posterior/0.5_2000_3_5000_0}.pdf" /> [fig:0.5-2000-3-5000-0]</p>
<p><embed src="{abc-posterior/1.0_2000_3_5000_0}.pdf" /> [fig:1.0-2000-3-5000-0]</p>
<p><embed src="{abc-posterior/1.5_2000_3_5000_0}.pdf" /> [fig:1.5-2000-3-5000-0]</p>
<p><embed src="{abc-posterior/0.0_1000_4_5000_0}.pdf" /> [fig:0.0-1000-4-5000-0]</p>
<p><embed src="{abc-posterior/0.5_1000_4_5000_0}.pdf" /> [fig:0.5-1000-4-5000-0]</p>
<p><embed src="{abc-posterior/1.0_1000_4_5000_0}.pdf" /> [fig:1.0-1000-4-5000-0]</p>
<p><embed src="{abc-posterior/1.5_1000_4_5000_0}.pdf" /> [fig:1.5-1000-4-5000-0]</p>
<p><embed src="{abc-posterior/0.0_2000_4_5000_0}.pdf" /> [fig:0.0-2000-4-5000-0]</p>
<p><embed src="{abc-posterior/0.5_2000_4_5000_0}.pdf" /> [fig:0.5-2000-4-5000-0]</p>
<p><embed src="{abc-posterior/1.0_2000_4_5000_0}.pdf" /> [fig:1.0-2000-4-5000-0]</p>
<p><embed src="{abc-posterior/1.5_2000_4_5000_0}.pdf" /> [fig:1.5-2000-4-5000-0]</p>
