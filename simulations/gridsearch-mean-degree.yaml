Name: gridsearch-mean-degree
Description: Grid search for preferential attachment mean degree
Processes: 20
Steps:
    train-network:
        Extension: gml
        Parameters:
            nnode: 5000
            m: "range(2, 11)"
            m: 1
            replicate: "range(100)"
        Interpreter: R --vanilla --silent
        Startup: suppressPackageStartupMessages(library(netabc))
        Rule: | 
            set.seed({seed})
            g <- sample_pa({nnode}, power={m}, m={m}, directed=FALSE)
            graph_attr(g, "comment") <- "{yaml}"
            write.graph(SI.net(g), "{train-network}", format="gml")

    train-tree:
        Extension: nwk
        Parameters:
            m: "range(2, 11)"
            replicate: "range(100)"
            nsimnode: 1000
            ntip: 1000
        Depends: train-network
        Interpreter: bash
        Rule: |
            echo "#{yaml}" > {train-tree}                                                 
            nettree --sim-nodes {nsimnode} --tree-tips {ntip} --seed {seed} {train-network} >> {train-tree}

    test-network:
        Extension: gml
        Parameters:
            nnode: 5000
            true_m: [2, 3, 5, 8]
            m: 1
            test_replicate: "range(10)"
        Interpreter: R --vanilla --silent
        Startup: suppressPackageStartupMessages(library(netabc))
        Rule: | 
            set.seed({seed})
            g <- sample_pa({nnode}, power={m}, m={true_m}, directed=FALSE)
            graph_attr(g, "comment") <- "{yaml}"
            write.graph(SI.net(g), "{test-network}", format="gml")

    test-tree:
        Extension: nwk
        Parameters:
            true_m: [2, 3, 5, 8]
            test_replicate: "range(10)"
            nsimnode: 1000
            ntip: 1000
        Depends: test-network
        Interpreter: bash
        Rule: |
            echo "#{yaml}" > {test-tree}                                                 
            nettree --sim-nodes {nsimnode} --tree-tips {ntip} --seed {seed} {test-network} >> {test-tree}

    kernel:
        Extension: tsv
        Parameters:
            true_m: [2, 3, 5, 8]
            test_replicate: "range(10)"
            decay_factor: 0.3
            rbf_variance: 20
        Depends: train-tree test-tree
        Interpreter: bash
        Rule: |
            echo "#{yaml}" > {kernel}
            for T in {train-tree}; do
                echo -n "$T"$'\t' >> {kernel}
                treekernel --ladderize --normalize --scale-branches mean \
                           --decay-factor {decay_factor} \
                           --gauss-factor {rbf_variance} \
                           {test-tree} $T >> {kernel}
            done

    kernel-plot:
        Extension: pdf
        Parameters:
            true_m: [2, 3, 5, 8]
            test_replicate: "range(10)"
        Depends: kernel train-tree
        Interpreter: R --quiet --vanilla
        Startup: suppressPackageStartupMessages(library(netabc))
        Rule: |
            k <- read.table("{kernel}", row.names=1, col.names=c("", "score"))
            k <- merge(k, collect.metadata(strsplit("{train-tree}", " ")[[1]]), by=0)
            pt.est <- with(k, as.numeric(names(which.max(tapply(score, m, mean)))))
            lines <- data.frame(value=c(pt.est, {true_m}),
                                m=c("estimated", "true"))
            ggplot(k, aes(x=m, y=score)) + geom_point() +
                geom_vline(data=lines, aes(xintercept=value, linetype=m, color=m), show_guide = TRUE) +
                theme_bw()
            ggsave("{kernel-plot}")

    error-plot:
        Extension: pdf
        Parameters:
            placeholder: 0
        Depends: kernel test-tree train-tree
        Interpreter: R --quiet --vanilla
        Startup: |
            suppressPackageStartupMessages(library(netabc))
            suppressPackageStartupMessages(library(data.table))
            suppressPackageStartupMessages(library(reshape2))
        Rule: |
            # collect all kernel scores
            files <- strsplit("{kernel}", " ")[[1]]
            d <- collect.data(files, header=FALSE, col.names=c("tree", "score"))

            # combine with metadata from train and test trees
            mtrain <- collect.metadata(strsplit("{train-tree}", " ")[[1]])
            d <- merge(d, mtrain, by.x="tree", by.y=0)
            mtest <- collect.metadata(strsplit("{test-tree}", " ")[[1]])
            test.cols <- c("test_replicate", "true_m")
            d <- setDT(merge(d, mtest, by=test.cols))

            # choose point estimate with highest median kernel score
            d[,median := median(score), by=c(test.cols, "m")]

            # compute absolute and relative errors
            plot.data <- d[,abs(true_m - m[which.max(median)]), by=test.cols]
            plot.data <- setNames(plot.data, c(test.cols, "absolute"))
            plot.data <- plot.data[,relative := absolute / true_m]
            plot.data <- melt(plot.data, id.vars=test.cols, variable.name="error.type", value.name="error")
            plot.data <- plot.data[,true_m := as.factor(true_m)]

            # plot it out
            ggplot(plot.data, aes(x=true_m, y=error, group=true_m)) + 
                geom_boxplot() + 
                facet_wrap(~error.type, scales="free") + theme_bw()
            ggsave("{error-plot}")
